[
    {
        "text": "The purpose of this document is to capture frequently asked technical questions\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\nSubscribe to course public Google Calendar (it works from Desktop only).\nRegister before the course starts using this link.\nJoin the course Telegram channel with announcements.\nDon’t forget to register in DataTalks.Club's Slack and join the channel.",
        "question": "What date and time does the course start?",
        "section": "General course-related questions",
        "document_id": 1,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The purpose of this document is to capture frequently asked technical questions\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\nSubscribe to course public Google Calendar (it works from Desktop only).\nRegister before the course starts using this link.\nJoin the course Telegram channel with announcements.\nDon’t forget to register in DataTalks.Club's Slack and join the channel.",
        "question": "Where can I subscribe to the course public Google Calendar?",
        "section": "General course-related questions",
        "document_id": 1,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The purpose of this document is to capture frequently asked technical questions\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\nSubscribe to course public Google Calendar (it works from Desktop only).\nRegister before the course starts using this link.\nJoin the course Telegram channel with announcements.\nDon’t forget to register in DataTalks.Club's Slack and join the channel.",
        "question": "What is the importance of registering before the course starts?",
        "section": "General course-related questions",
        "document_id": 1,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The purpose of this document is to capture frequently asked technical questions\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\nSubscribe to course public Google Calendar (it works from Desktop only).\nRegister before the course starts using this link.\nJoin the course Telegram channel with announcements.\nDon’t forget to register in DataTalks.Club's Slack and join the channel.",
        "question": "How can I join the course Telegram channel?",
        "section": "General course-related questions",
        "document_id": 1,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The purpose of this document is to capture frequently asked technical questions\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\nSubscribe to course public Google Calendar (it works from Desktop only).\nRegister before the course starts using this link.\nJoin the course Telegram channel with announcements.\nDon’t forget to register in DataTalks.Club's Slack and join the channel.",
        "question": "Which platform should I use to register in DataTalks.Club's Slack?",
        "section": "General course-related questions",
        "document_id": 1,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites",
        "question": "What prerequisites are mentioned for the Data Engineering Zoomcamp?",
        "section": "General course-related questions",
        "document_id": 2,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites",
        "question": "Which GitHub repository is associated with the DataTalksClub?",
        "section": "General course-related questions",
        "document_id": 2,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites",
        "question": "What skills should participants have before joining the Zoomcamp?",
        "section": "General course-related questions",
        "document_id": 2,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites",
        "question": "Are there any specific tools or technologies recommended for the course?",
        "section": "General course-related questions",
        "document_id": 2,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites",
        "question": "How can individuals prepare themselves for the Data Engineering Zoomcamp?",
        "section": "General course-related questions",
        "document_id": 2,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, even if you don't register, you're still eligible to submit the homeworks.\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.",
        "question": "Can you submit homework without registering?",
        "section": "General course-related questions",
        "document_id": 3,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, even if you don't register, you're still eligible to submit the homeworks.\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.",
        "question": "What is the importance of deadlines in submitting final projects?",
        "section": "General course-related questions",
        "document_id": 3,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, even if you don't register, you're still eligible to submit the homeworks.\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.",
        "question": "What might happen if you wait until the last minute to submit your final project?",
        "section": "General course-related questions",
        "document_id": 3,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, even if you don't register, you're still eligible to submit the homeworks.\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.",
        "question": "Are there any restrictions on submitting homework for non-registered students?",
        "section": "General course-related questions",
        "document_id": 3,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, even if you don't register, you're still eligible to submit the homeworks.\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.",
        "question": "What should students keep in mind regarding final project submissions?",
        "section": "General course-related questions",
        "document_id": 3,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.",
        "question": "What is the purpose of registration mentioned in the text?",
        "section": "General course-related questions",
        "document_id": 4,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.",
        "question": "Can you start learning and submitting homework without registering?",
        "section": "General course-related questions",
        "document_id": 4,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.",
        "question": "Is the homework checked against a registered list?",
        "section": "General course-related questions",
        "document_id": 4,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.",
        "question": "What does the text imply about the necessity of registration?",
        "section": "General course-related questions",
        "document_id": 4,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.",
        "question": "How does the course handle interest gauging before the start date?",
        "section": "General course-related questions",
        "document_id": 4,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can start by installing and setting up all the dependencies and requirements:\nGoogle cloud account\nGoogle Cloud SDK\nPython 3 (installed with Anaconda)\nTerraform\nGit\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.",
        "question": "What is the first step in the setup process according to the text?",
        "section": "General course-related questions",
        "document_id": 5,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can start by installing and setting up all the dependencies and requirements:\nGoogle cloud account\nGoogle Cloud SDK\nPython 3 (installed with Anaconda)\nTerraform\nGit\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.",
        "question": "Which cloud service provider is mentioned in the setup requirements?",
        "section": "General course-related questions",
        "document_id": 5,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can start by installing and setting up all the dependencies and requirements:\nGoogle cloud account\nGoogle Cloud SDK\nPython 3 (installed with Anaconda)\nTerraform\nGit\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.",
        "question": "What programming language should be installed with Anaconda?",
        "section": "General course-related questions",
        "document_id": 5,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can start by installing and setting up all the dependencies and requirements:\nGoogle cloud account\nGoogle Cloud SDK\nPython 3 (installed with Anaconda)\nTerraform\nGit\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.",
        "question": "What tool is recommended for infrastructure as code in this setup?",
        "section": "General course-related questions",
        "document_id": 5,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can start by installing and setting up all the dependencies and requirements:\nGoogle cloud account\nGoogle Cloud SDK\nPython 3 (installed with Anaconda)\nTerraform\nGit\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.",
        "question": "Why is it important to review the prerequisites and syllabus before starting?",
        "section": "General course-related questions",
        "document_id": 5,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\nData-Engineering (Jan - Apr)\nMLOps (May - Aug)\nMachine Learning (Sep - Jan)\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.",
        "question": "How many Zoom Camps are offered in a year as of 2024?",
        "section": "General course-related questions",
        "document_id": 6,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\nData-Engineering (Jan - Apr)\nMLOps (May - Aug)\nMachine Learning (Sep - Jan)\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.",
        "question": "What are the specific timeframes for each Zoom Camp course?",
        "section": "General course-related questions",
        "document_id": 6,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\nData-Engineering (Jan - Apr)\nMLOps (May - Aug)\nMachine Learning (Sep - Jan)\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.",
        "question": "Is there more than one 'live' cohort for the Data-Engineering Zoom Camp per year?",
        "section": "General course-related questions",
        "document_id": 6,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\nData-Engineering (Jan - Apr)\nMLOps (May - Aug)\nMachine Learning (Sep - Jan)\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.",
        "question": "Can participants take Zoom Camps at their own pace, and if so, how does this differ from attending a 'live' cohort?",
        "section": "General course-related questions",
        "document_id": 6,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\nData-Engineering (Jan - Apr)\nMLOps (May - Aug)\nMachine Learning (Sep - Jan)\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.",
        "question": "What is the primary difference between taking a Zoom Camp for certification and taking it without the intention of certification?",
        "section": "General course-related questions",
        "document_id": 6,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes. For the 2024 edition we are using Mage AI instead of Prefect and re-recorded the terraform videos, For 2023, we used Prefect instead of Airflow..",
        "question": "What tool are we using for the 2024 edition?",
        "section": "General course-related questions",
        "document_id": 7,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes. For the 2024 edition we are using Mage AI instead of Prefect and re-recorded the terraform videos, For 2023, we used Prefect instead of Airflow..",
        "question": "Which tool was used in the 2023 edition instead of Airflow?",
        "section": "General course-related questions",
        "document_id": 7,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes. For the 2024 edition we are using Mage AI instead of Prefect and re-recorded the terraform videos, For 2023, we used Prefect instead of Airflow..",
        "question": "What type of videos were re-recorded for the 2024 edition?",
        "section": "General course-related questions",
        "document_id": 7,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes. For the 2024 edition we are using Mage AI instead of Prefect and re-recorded the terraform videos, For 2023, we used Prefect instead of Airflow..",
        "question": "Why was Prefect replaced with Mage AI for the 2024 edition?",
        "section": "General course-related questions",
        "document_id": 7,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes. For the 2024 edition we are using Mage AI instead of Prefect and re-recorded the terraform videos, For 2023, we used Prefect instead of Airflow..",
        "question": "Which tool was used in the 2023 edition?",
        "section": "General course-related questions",
        "document_id": 7,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.",
        "question": "Will the materials be available after the course finishes?",
        "section": "General course-related questions",
        "document_id": 8,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.",
        "question": "Can participants continue to work on homeworks after the course?",
        "section": "General course-related questions",
        "document_id": 8,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.",
        "question": "Is it possible to prepare for the next cohort after completing the course?",
        "section": "General course-related questions",
        "document_id": 8,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.",
        "question": "What can participants work on after the course ends?",
        "section": "General course-related questions",
        "document_id": 8,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.",
        "question": "Are there any projects that can be started following the completion of the course?",
        "section": "General course-related questions",
        "document_id": 8,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\nYou can also tag the bot @ZoomcampQABot to help you conduct the search, but don’t rely on its answers 100%, it is pretty good though.",
        "question": "Is the Slack channel still open for asking questions?",
        "section": "General course-related questions",
        "document_id": 9,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\nYou can also tag the bot @ZoomcampQABot to help you conduct the search, but don’t rely on its answers 100%, it is pretty good though.",
        "question": "What should I do before asking a question in the Slack channel?",
        "section": "General course-related questions",
        "document_id": 9,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\nYou can also tag the bot @ZoomcampQABot to help you conduct the search, but don’t rely on its answers 100%, it is pretty good though.",
        "question": "How can I search for answers to my questions?",
        "section": "General course-related questions",
        "document_id": 9,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\nYou can also tag the bot @ZoomcampQABot to help you conduct the search, but don’t rely on its answers 100%, it is pretty good though.",
        "question": "Who should I tag for assistance in the Slack channel?",
        "section": "General course-related questions",
        "document_id": 9,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\nYou can also tag the bot @ZoomcampQABot to help you conduct the search, but don’t rely on its answers 100%, it is pretty good though.",
        "question": "Can I rely completely on the bot's answers in the Slack channel?",
        "section": "General course-related questions",
        "document_id": 9,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "All the main videos are stored in the Main “DATA ENGINEERING” playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\nBelow is the MAIN PLAYLIST’. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\nh\nttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-",
        "question": "Where are all the main videos stored?",
        "section": "General course-related questions",
        "document_id": 10,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "All the main videos are stored in the Main “DATA ENGINEERING” playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\nBelow is the MAIN PLAYLIST’. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\nh\nttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-",
        "question": "How can one access the GitHub repository related to the videos?",
        "section": "General course-related questions",
        "document_id": 10,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "All the main videos are stored in the Main “DATA ENGINEERING” playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\nBelow is the MAIN PLAYLIST’. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\nh\nttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-",
        "question": "What type of additional videos can be found in the year-specific playlist?",
        "section": "General course-related questions",
        "document_id": 10,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "All the main videos are stored in the Main “DATA ENGINEERING” playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\nBelow is the MAIN PLAYLIST’. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\nh\nttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-",
        "question": "Where can the main playlist be found?",
        "section": "General course-related questions",
        "document_id": 10,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "All the main videos are stored in the Main “DATA ENGINEERING” playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\nBelow is the MAIN PLAYLIST’. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\nh\nttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-",
        "question": "Is the playlist linked to any communication channels?",
        "section": "General course-related questions",
        "document_id": 10,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week. [source1] [source2]\nYou can also calculate it yourself using this data and then update this answer.",
        "question": "What factors influence the time commitment required for the modules?",
        "section": "General course-related questions",
        "document_id": 11,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week. [source1] [source2]\nYou can also calculate it yourself using this data and then update this answer.",
        "question": "How many hours per week is a participant expected to dedicate to the modules?",
        "section": "General course-related questions",
        "document_id": 11,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week. [source1] [source2]\nYou can also calculate it yourself using this data and then update this answer.",
        "question": "Is prior experience relevant to the time required for the modules?",
        "section": "General course-related questions",
        "document_id": 11,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week. [source1] [source2]\nYou can also calculate it yourself using this data and then update this answer.",
        "question": "How can one estimate their own time commitment using the provided data?",
        "section": "General course-related questions",
        "document_id": 11,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week. [source1] [source2]\nYou can also calculate it yourself using this data and then update this answer.",
        "question": "What is the minimum and maximum expected hours per week for the modules?",
        "section": "General course-related questions",
        "document_id": 11,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.",
        "question": "Can you receive a certificate for completing a self-paced course?",
        "section": "General course-related questions",
        "document_id": 12,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.",
        "question": "Why is peer-reviewing capstone projects necessary for obtaining a certificate?",
        "section": "General course-related questions",
        "document_id": 12,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.",
        "question": "What are the requirements to earn a certificate in this course?",
        "section": "General course-related questions",
        "document_id": 12,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.",
        "question": "When can students peer-review projects in this course?",
        "section": "General course-related questions",
        "document_id": 12,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.",
        "question": "What happens if a student completes the course without participating in a live cohort?",
        "section": "General course-related questions",
        "document_id": 12,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The zoom link is only published to instructors/presenters/TAs.\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.",
        "question": "Who can access the Zoom link for the session?",
        "section": "General course-related questions",
        "document_id": 13,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The zoom link is only published to instructors/presenters/TAs.\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.",
        "question": "How can students participate in the live session?",
        "section": "General course-related questions",
        "document_id": 13,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The zoom link is only published to instructors/presenters/TAs.\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.",
        "question": "Where will the video URL be posted before the session starts?",
        "section": "General course-related questions",
        "document_id": 13,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The zoom link is only published to instructors/presenters/TAs.\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.",
        "question": "What should students use to submit their questions during the live event?",
        "section": "General course-related questions",
        "document_id": 13,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The zoom link is only published to instructors/presenters/TAs.\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.",
        "question": "Why should students avoid posting questions in the chat during the session?",
        "section": "General course-related questions",
        "document_id": 13,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes! Every “Office Hours” will be recorded and available a few minutes after the live session is over; so you can view (or rewatch) whenever you want.",
        "question": "Will the Office Hours sessions be recorded?",
        "section": "General course-related questions",
        "document_id": 14,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes! Every “Office Hours” will be recorded and available a few minutes after the live session is over; so you can view (or rewatch) whenever you want.",
        "question": "How soon after the live session can I access the recordings?",
        "section": "General course-related questions",
        "document_id": 14,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes! Every “Office Hours” will be recorded and available a few minutes after the live session is over; so you can view (or rewatch) whenever you want.",
        "question": "Can I rewatch the Office Hours sessions?",
        "section": "General course-related questions",
        "document_id": 14,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes! Every “Office Hours” will be recorded and available a few minutes after the live session is over; so you can view (or rewatch) whenever you want.",
        "question": "Are the recordings of Office Hours available for everyone?",
        "section": "General course-related questions",
        "document_id": 14,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes! Every “Office Hours” will be recorded and available a few minutes after the live session is over; so you can view (or rewatch) whenever you want.",
        "question": "Is there a limit to how many times I can view the recorded sessions?",
        "section": "General course-related questions",
        "document_id": 14,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can find the latest and up-to-date deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\nAlso, take note of Announcements from @Au-Tomator for any extensions or other news. Or, the form may also show the updated deadline, if Instructor(s) has updated it.",
        "question": "Where can I find the latest deadlines for the course?",
        "section": "General course-related questions",
        "document_id": 15,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can find the latest and up-to-date deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\nAlso, take note of Announcements from @Au-Tomator for any extensions or other news. Or, the form may also show the updated deadline, if Instructor(s) has updated it.",
        "question": "Who is responsible for updating the announcements regarding the deadlines?",
        "section": "General course-related questions",
        "document_id": 15,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can find the latest and up-to-date deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\nAlso, take note of Announcements from @Au-Tomator for any extensions or other news. Or, the form may also show the updated deadline, if Instructor(s) has updated it.",
        "question": "How can I access the updated deadlines if the instructor makes changes?",
        "section": "General course-related questions",
        "document_id": 15,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can find the latest and up-to-date deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\nAlso, take note of Announcements from @Au-Tomator for any extensions or other news. Or, the form may also show the updated deadline, if Instructor(s) has updated it.",
        "question": "What source should I check for any extensions or news related to deadlines?",
        "section": "General course-related questions",
        "document_id": 15,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can find the latest and up-to-date deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\nAlso, take note of Announcements from @Au-Tomator for any extensions or other news. Or, the form may also show the updated deadline, if Instructor(s) has updated it.",
        "question": "Is there a specific form mentioned where the updated deadlines might be displayed?",
        "section": "General course-related questions",
        "document_id": 15,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\nOlder news:[source1] [source2]",
        "question": "Are late submissions allowed?",
        "section": "General course-related questions",
        "document_id": 16,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\nOlder news:[source1] [source2]",
        "question": "What should you do if the form is still open after the due date?",
        "section": "General course-related questions",
        "document_id": 16,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\nOlder news:[source1] [source2]",
        "question": "How can you confirm your submission?",
        "section": "General course-related questions",
        "document_id": 16,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\nOlder news:[source1] [source2]",
        "question": "Where can you find the date-timestamp for your submission?",
        "section": "General course-related questions",
        "document_id": 16,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\nOlder news:[source1] [source2]",
        "question": "What happens if the form is closed after the due date?",
        "section": "General course-related questions",
        "document_id": 16,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Answer: In short, it’s your repository on github, gitlab, bitbucket, etc\nIn long, your repository or any other location you have your code where a reasonable person would look at it and think yes, you went through the week and exercises.",
        "question": "What is a repository in the context of version control systems?",
        "section": "General course-related questions",
        "document_id": 17,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Answer: In short, it’s your repository on github, gitlab, bitbucket, etc\nIn long, your repository or any other location you have your code where a reasonable person would look at it and think yes, you went through the week and exercises.",
        "question": "Which platforms are mentioned for hosting repositories?",
        "section": "General course-related questions",
        "document_id": 17,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Answer: In short, it’s your repository on github, gitlab, bitbucket, etc\nIn long, your repository or any other location you have your code where a reasonable person would look at it and think yes, you went through the week and exercises.",
        "question": "What criteria should a location for code meet according to the text?",
        "section": "General course-related questions",
        "document_id": 17,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Answer: In short, it’s your repository on github, gitlab, bitbucket, etc\nIn long, your repository or any other location you have your code where a reasonable person would look at it and think yes, you went through the week and exercises.",
        "question": "How does the text describe a repository in short?",
        "section": "General course-related questions",
        "document_id": 17,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Answer: In short, it’s your repository on github, gitlab, bitbucket, etc\nIn long, your repository or any other location you have your code where a reasonable person would look at it and think yes, you went through the week and exercises.",
        "question": "What is implied about the purpose of having a repository?",
        "section": "General course-related questions",
        "document_id": 17,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you’ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\n(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)",
        "question": "How is homework graded?",
        "section": "General course-related questions",
        "document_id": 18,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you’ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\n(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)",
        "question": "Where can you see the number of points earned for homework?",
        "section": "General course-related questions",
        "document_id": 18,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you’ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\n(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)",
        "question": "What types of activities contribute to the points on the leaderboard?",
        "section": "General course-related questions",
        "document_id": 18,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you’ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\n(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)",
        "question": "How many points do you earn for submitting something to FAQ?",
        "section": "General course-related questions",
        "document_id": 18,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you’ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\n(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)",
        "question": "What is the point value for sharing a learning in public link?",
        "section": "General course-related questions",
        "document_id": 18,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. If you want to see what your Display name is.\nGo to the Homework submission link →  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on ‘Data Engineering Zoom Camp 2024’ > click on ‘Edit Course Profile’ - your display name is here, you can also change it should you wish:",
        "question": "What is assigned to you when you set up your account?",
        "section": "General course-related questions",
        "document_id": 19,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. If you want to see what your Display name is.\nGo to the Homework submission link →  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on ‘Data Engineering Zoom Camp 2024’ > click on ‘Edit Course Profile’ - your display name is here, you can also change it should you wish:",
        "question": "How can you find out what your Display name is?",
        "section": "General course-related questions",
        "document_id": 19,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. If you want to see what your Display name is.\nGo to the Homework submission link →  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on ‘Data Engineering Zoom Camp 2024’ > click on ‘Edit Course Profile’ - your display name is here, you can also change it should you wish:",
        "question": "What is the URL to the Homework submission link?",
        "section": "General course-related questions",
        "document_id": 19,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. If you want to see what your Display name is.\nGo to the Homework submission link →  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on ‘Data Engineering Zoom Camp 2024’ > click on ‘Edit Course Profile’ - your display name is here, you can also change it should you wish:",
        "question": "What steps do you need to follow to edit your Course Profile?",
        "section": "General course-related questions",
        "document_id": 19,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. If you want to see what your Display name is.\nGo to the Homework submission link →  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on ‘Data Engineering Zoom Camp 2024’ > click on ‘Edit Course Profile’ - your display name is here, you can also change it should you wish:",
        "question": "Can you change your Display name after it has been assigned?",
        "section": "General course-related questions",
        "document_id": 19,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, for simplicity (of troubleshooting against the recorded videos) and stability. [source]\nBut Python 3.10 and 3.11 should work fine.",
        "question": "What versions of Python are mentioned as functioning well?",
        "section": "General course-related questions",
        "document_id": 20,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, for simplicity (of troubleshooting against the recorded videos) and stability. [source]\nBut Python 3.10 and 3.11 should work fine.",
        "question": "Why is simplicity important when troubleshooting?",
        "section": "General course-related questions",
        "document_id": 20,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, for simplicity (of troubleshooting against the recorded videos) and stability. [source]\nBut Python 3.10 and 3.11 should work fine.",
        "question": "What is the purpose of recorded videos in this context?",
        "section": "General course-related questions",
        "document_id": 20,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, for simplicity (of troubleshooting against the recorded videos) and stability. [source]\nBut Python 3.10 and 3.11 should work fine.",
        "question": "Is there any indication that earlier versions of Python may not work?",
        "section": "General course-related questions",
        "document_id": 20,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, for simplicity (of troubleshooting against the recorded videos) and stability. [source]\nBut Python 3.10 and 3.11 should work fine.",
        "question": "What factors contribute to the stability mentioned in the text?",
        "section": "General course-related questions",
        "document_id": 20,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can set it up on your laptop or PC if you prefer to work locally from your laptop or PC.\nYou might face some challenges, especially for Windows users. If you face cnd2\nIf you prefer to work on the local machine, you may start with the week 1 Introduction to Docker and follow through.\nHowever, if you prefer to set up a virtual machine, you may start with these first:\nUsing GitHub Codespaces\nSetting up the environment on a cloudV Mcodespace\nI decided to work on a virtual machine because I have different laptops & PCs for my home & office, so I can work on this boot camp virtually anywhere.",
        "question": "What are the two options for setting up the working environment mentioned in the text?",
        "section": "General course-related questions",
        "document_id": 21,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can set it up on your laptop or PC if you prefer to work locally from your laptop or PC.\nYou might face some challenges, especially for Windows users. If you face cnd2\nIf you prefer to work on the local machine, you may start with the week 1 Introduction to Docker and follow through.\nHowever, if you prefer to set up a virtual machine, you may start with these first:\nUsing GitHub Codespaces\nSetting up the environment on a cloudV Mcodespace\nI decided to work on a virtual machine because I have different laptops & PCs for my home & office, so I can work on this boot camp virtually anywhere.",
        "question": "What challenges might Windows users face?",
        "section": "General course-related questions",
        "document_id": 21,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can set it up on your laptop or PC if you prefer to work locally from your laptop or PC.\nYou might face some challenges, especially for Windows users. If you face cnd2\nIf you prefer to work on the local machine, you may start with the week 1 Introduction to Docker and follow through.\nHowever, if you prefer to set up a virtual machine, you may start with these first:\nUsing GitHub Codespaces\nSetting up the environment on a cloudV Mcodespace\nI decided to work on a virtual machine because I have different laptops & PCs for my home & office, so I can work on this boot camp virtually anywhere.",
        "question": "What is the recommended starting point for those who prefer to work locally?",
        "section": "General course-related questions",
        "document_id": 21,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can set it up on your laptop or PC if you prefer to work locally from your laptop or PC.\nYou might face some challenges, especially for Windows users. If you face cnd2\nIf you prefer to work on the local machine, you may start with the week 1 Introduction to Docker and follow through.\nHowever, if you prefer to set up a virtual machine, you may start with these first:\nUsing GitHub Codespaces\nSetting up the environment on a cloudV Mcodespace\nI decided to work on a virtual machine because I have different laptops & PCs for my home & office, so I can work on this boot camp virtually anywhere.",
        "question": "Why did the author choose to work on a virtual machine?",
        "section": "General course-related questions",
        "document_id": 21,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can set it up on your laptop or PC if you prefer to work locally from your laptop or PC.\nYou might face some challenges, especially for Windows users. If you face cnd2\nIf you prefer to work on the local machine, you may start with the week 1 Introduction to Docker and follow through.\nHowever, if you prefer to set up a virtual machine, you may start with these first:\nUsing GitHub Codespaces\nSetting up the environment on a cloudV Mcodespace\nI decided to work on a virtual machine because I have different laptops & PCs for my home & office, so I can work on this boot camp virtually anywhere.",
        "question": "What are the first steps mentioned for setting up a virtual machine?",
        "section": "General course-related questions",
        "document_id": 21,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\nYou can also open any GitHub repository in a GitHub Codespace.",
        "question": "What resources does GitHub Codespaces provide?",
        "section": "General course-related questions",
        "document_id": 22,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\nYou can also open any GitHub repository in a GitHub Codespace.",
        "question": "Which tools are pre-installed in GitHub Codespaces?",
        "section": "General course-related questions",
        "document_id": 22,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\nYou can also open any GitHub repository in a GitHub Codespace.",
        "question": "Can you open any GitHub repository in a GitHub Codespace?",
        "section": "General course-related questions",
        "document_id": 22,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\nYou can also open any GitHub repository in a GitHub Codespace.",
        "question": "What is the operating system used in GitHub Codespaces?",
        "section": "General course-related questions",
        "document_id": 22,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\nYou can also open any GitHub repository in a GitHub Codespace.",
        "question": "What development technologies are mentioned in relation to GitHub Codespaces?",
        "section": "General course-related questions",
        "document_id": 22,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It's up to you which platform and environment you use for the course.\nGithub codespaces or GCP VM are just possible options, but you can do the entire course from your laptop.",
        "question": "What platforms are recommended for the course?",
        "section": "General course-related questions",
        "document_id": 23,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It's up to you which platform and environment you use for the course.\nGithub codespaces or GCP VM are just possible options, but you can do the entire course from your laptop.",
        "question": "Can I complete the course using my own laptop?",
        "section": "General course-related questions",
        "document_id": 23,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It's up to you which platform and environment you use for the course.\nGithub codespaces or GCP VM are just possible options, but you can do the entire course from your laptop.",
        "question": "What is the significance of using Github codespaces or GCP VM?",
        "section": "General course-related questions",
        "document_id": 23,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It's up to you which platform and environment you use for the course.\nGithub codespaces or GCP VM are just possible options, but you can do the entire course from your laptop.",
        "question": "Are there any restrictions on the platform I choose for the course?",
        "section": "General course-related questions",
        "document_id": 23,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It's up to you which platform and environment you use for the course.\nGithub codespaces or GCP VM are just possible options, but you can do the entire course from your laptop.",
        "question": "Is it required to use Github codespaces or GCP VM for the course?",
        "section": "General course-related questions",
        "document_id": 23,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Choose the approach that aligns the most with your idea for the end project\nOne of those should suffice. However, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Or you can set up a local environment for most of this course.",
        "question": "What approach should be chosen for the end project?",
        "section": "General course-related questions",
        "document_id": 24,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Choose the approach that aligns the most with your idea for the end project\nOne of those should suffice. However, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Or you can set up a local environment for most of this course.",
        "question": "Why is learning BigQuery emphasized in the text?",
        "section": "General course-related questions",
        "document_id": 24,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Choose the approach that aligns the most with your idea for the end project\nOne of those should suffice. However, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Or you can set up a local environment for most of this course.",
        "question": "What is BigQuery a part of?",
        "section": "General course-related questions",
        "document_id": 24,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Choose the approach that aligns the most with your idea for the end project\nOne of those should suffice. However, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Or you can set up a local environment for most of this course.",
        "question": "What alternative to using BigQuery is mentioned?",
        "section": "General course-related questions",
        "document_id": 24,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Choose the approach that aligns the most with your idea for the end project\nOne of those should suffice. However, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Or you can set up a local environment for most of this course.",
        "question": "What is the suggested environment for the course?",
        "section": "General course-related questions",
        "document_id": 24,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "1. To open Run command window, you can either:\n(1-1) Use the shortcut keys: 'Windows + R', or\n(1-2) Right Click \"Start\", and click \"Run\" to open.\n2. Registry Values Located in Registry Editor, to open it: Type 'regedit' in the Run command window, and then press Enter.' 3. Now you can change the registry values \"Autorun\" in \"HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor\" from \"if exists\" to a blank.\nAlternatively, You can simplify the solution by deleting the fingerprint saved within the known_hosts file. In Windows, this file is placed at  C:\\Users\\<your_user_name>\\.ssh\\known_host",
        "question": "What are the two methods to open the Run command window?",
        "section": "General course-related questions",
        "document_id": 25,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "1. To open Run command window, you can either:\n(1-1) Use the shortcut keys: 'Windows + R', or\n(1-2) Right Click \"Start\", and click \"Run\" to open.\n2. Registry Values Located in Registry Editor, to open it: Type 'regedit' in the Run command window, and then press Enter.' 3. Now you can change the registry values \"Autorun\" in \"HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor\" from \"if exists\" to a blank.\nAlternatively, You can simplify the solution by deleting the fingerprint saved within the known_hosts file. In Windows, this file is placed at  C:\\Users\\<your_user_name>\\.ssh\\known_host",
        "question": "How can you access the Registry Editor?",
        "section": "General course-related questions",
        "document_id": 25,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "1. To open Run command window, you can either:\n(1-1) Use the shortcut keys: 'Windows + R', or\n(1-2) Right Click \"Start\", and click \"Run\" to open.\n2. Registry Values Located in Registry Editor, to open it: Type 'regedit' in the Run command window, and then press Enter.' 3. Now you can change the registry values \"Autorun\" in \"HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor\" from \"if exists\" to a blank.\nAlternatively, You can simplify the solution by deleting the fingerprint saved within the known_hosts file. In Windows, this file is placed at  C:\\Users\\<your_user_name>\\.ssh\\known_host",
        "question": "What specific registry value can be changed in the Command Processor?",
        "section": "General course-related questions",
        "document_id": 25,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "1. To open Run command window, you can either:\n(1-1) Use the shortcut keys: 'Windows + R', or\n(1-2) Right Click \"Start\", and click \"Run\" to open.\n2. Registry Values Located in Registry Editor, to open it: Type 'regedit' in the Run command window, and then press Enter.' 3. Now you can change the registry values \"Autorun\" in \"HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor\" from \"if exists\" to a blank.\nAlternatively, You can simplify the solution by deleting the fingerprint saved within the known_hosts file. In Windows, this file is placed at  C:\\Users\\<your_user_name>\\.ssh\\known_host",
        "question": "Where is the known_hosts file located in Windows?",
        "section": "General course-related questions",
        "document_id": 25,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "1. To open Run command window, you can either:\n(1-1) Use the shortcut keys: 'Windows + R', or\n(1-2) Right Click \"Start\", and click \"Run\" to open.\n2. Registry Values Located in Registry Editor, to open it: Type 'regedit' in the Run command window, and then press Enter.' 3. Now you can change the registry values \"Autorun\" in \"HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor\" from \"if exists\" to a blank.\nAlternatively, You can simplify the solution by deleting the fingerprint saved within the known_hosts file. In Windows, this file is placed at  C:\\Users\\<your_user_name>\\.ssh\\known_host",
        "question": "What alternative method is suggested for simplifying the registry change process?",
        "section": "General course-related questions",
        "document_id": 25,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For uniformity at least, but you’re not restricted to GCP, you can use other cloud platforms like AWS if you’re comfortable with other cloud platforms, since you get every service that’s been provided by GCP in Azure and AWS or others..\nBecause everyone has a google account, GCP has a free trial period and gives $300 in credits  to new users. Also, we are working with BigQuery, which is a part of GCP.\nNote that to sign up for a free GCP account, you must have a valid credit card.",
        "question": "What is the main benefit of using GCP for new users?",
        "section": "General course-related questions",
        "document_id": 26,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For uniformity at least, but you’re not restricted to GCP, you can use other cloud platforms like AWS if you’re comfortable with other cloud platforms, since you get every service that’s been provided by GCP in Azure and AWS or others..\nBecause everyone has a google account, GCP has a free trial period and gives $300 in credits  to new users. Also, we are working with BigQuery, which is a part of GCP.\nNote that to sign up for a free GCP account, you must have a valid credit card.",
        "question": "Can users sign up for GCP without a credit card?",
        "section": "General course-related questions",
        "document_id": 26,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For uniformity at least, but you’re not restricted to GCP, you can use other cloud platforms like AWS if you’re comfortable with other cloud platforms, since you get every service that’s been provided by GCP in Azure and AWS or others..\nBecause everyone has a google account, GCP has a free trial period and gives $300 in credits  to new users. Also, we are working with BigQuery, which is a part of GCP.\nNote that to sign up for a free GCP account, you must have a valid credit card.",
        "question": "Which cloud platforms are mentioned as alternatives to GCP?",
        "section": "General course-related questions",
        "document_id": 26,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For uniformity at least, but you’re not restricted to GCP, you can use other cloud platforms like AWS if you’re comfortable with other cloud platforms, since you get every service that’s been provided by GCP in Azure and AWS or others..\nBecause everyone has a google account, GCP has a free trial period and gives $300 in credits  to new users. Also, we are working with BigQuery, which is a part of GCP.\nNote that to sign up for a free GCP account, you must have a valid credit card.",
        "question": "What financial incentive does GCP offer to new users during the trial period?",
        "section": "General course-related questions",
        "document_id": 26,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For uniformity at least, but you’re not restricted to GCP, you can use other cloud platforms like AWS if you’re comfortable with other cloud platforms, since you get every service that’s been provided by GCP in Azure and AWS or others..\nBecause everyone has a google account, GCP has a free trial period and gives $300 in credits  to new users. Also, we are working with BigQuery, which is a part of GCP.\nNote that to sign up for a free GCP account, you must have a valid credit card.",
        "question": "What specific service from GCP is mentioned in the text?",
        "section": "General course-related questions",
        "document_id": 26,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, if you use GCP and take advantage of their free trial.",
        "question": "What is GCP?",
        "section": "General course-related questions",
        "document_id": 27,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, if you use GCP and take advantage of their free trial.",
        "question": "What does GCP stand for?",
        "section": "General course-related questions",
        "document_id": 27,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, if you use GCP and take advantage of their free trial.",
        "question": "What is the benefit of using GCP's free trial?",
        "section": "General course-related questions",
        "document_id": 27,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, if you use GCP and take advantage of their free trial.",
        "question": "Are there any costs associated with using GCP's free trial?",
        "section": "General course-related questions",
        "document_id": 27,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, if you use GCP and take advantage of their free trial.",
        "question": "Can anyone use GCP's free trial?",
        "section": "General course-related questions",
        "document_id": 27,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won’t be able to provide guidelines for some things, but most of the materials are runnable without GCP.\nFor everything in the course, there’s a local alternative. You could even do the whole course locally.",
        "question": "What is the significance of GCP in the course?",
        "section": "General course-related questions",
        "document_id": 28,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won’t be able to provide guidelines for some things, but most of the materials are runnable without GCP.\nFor everything in the course, there’s a local alternative. You could even do the whole course locally.",
        "question": "Can the entire course be completed without using cloud services?",
        "section": "General course-related questions",
        "document_id": 28,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won’t be able to provide guidelines for some things, but most of the materials are runnable without GCP.\nFor everything in the course, there’s a local alternative. You could even do the whole course locally.",
        "question": "Which tool mentioned cannot be run locally?",
        "section": "General course-related questions",
        "document_id": 28,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won’t be able to provide guidelines for some things, but most of the materials are runnable without GCP.\nFor everything in the course, there’s a local alternative. You could even do the whole course locally.",
        "question": "Are there guidelines provided for running materials locally?",
        "section": "General course-related questions",
        "document_id": 28,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won’t be able to provide guidelines for some things, but most of the materials are runnable without GCP.\nFor everything in the course, there’s a local alternative. You could even do the whole course locally.",
        "question": "What does the course suggest about local alternatives?",
        "section": "General course-related questions",
        "document_id": 28,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, you can. Just remember to adapt all the information on the videos to AWS. Besides, the final capstone will be evaluated based on the task: Create a data pipeline! Develop a visualisation!\nThe problem would be when you need help. You’d need to rely on  fellow coursemates who also use AWS (or have experience using it before), which might be in smaller numbers than those learning the course with GCP.\nAlso see Is it possible to use x tool instead of the one tool you use?",
        "question": "What adaptation is required for the videos mentioned in the text?",
        "section": "General course-related questions",
        "document_id": 29,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, you can. Just remember to adapt all the information on the videos to AWS. Besides, the final capstone will be evaluated based on the task: Create a data pipeline! Develop a visualisation!\nThe problem would be when you need help. You’d need to rely on  fellow coursemates who also use AWS (or have experience using it before), which might be in smaller numbers than those learning the course with GCP.\nAlso see Is it possible to use x tool instead of the one tool you use?",
        "question": "What is the main task for the final capstone project?",
        "section": "General course-related questions",
        "document_id": 29,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, you can. Just remember to adapt all the information on the videos to AWS. Besides, the final capstone will be evaluated based on the task: Create a data pipeline! Develop a visualisation!\nThe problem would be when you need help. You’d need to rely on  fellow coursemates who also use AWS (or have experience using it before), which might be in smaller numbers than those learning the course with GCP.\nAlso see Is it possible to use x tool instead of the one tool you use?",
        "question": "What challenges might arise when seeking help in the course?",
        "section": "General course-related questions",
        "document_id": 29,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, you can. Just remember to adapt all the information on the videos to AWS. Besides, the final capstone will be evaluated based on the task: Create a data pipeline! Develop a visualisation!\nThe problem would be when you need help. You’d need to rely on  fellow coursemates who also use AWS (or have experience using it before), which might be in smaller numbers than those learning the course with GCP.\nAlso see Is it possible to use x tool instead of the one tool you use?",
        "question": "How does the experience of coursemates with AWS compare to those using GCP?",
        "section": "General course-related questions",
        "document_id": 29,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, you can. Just remember to adapt all the information on the videos to AWS. Besides, the final capstone will be evaluated based on the task: Create a data pipeline! Develop a visualisation!\nThe problem would be when you need help. You’d need to rely on  fellow coursemates who also use AWS (or have experience using it before), which might be in smaller numbers than those learning the course with GCP.\nAlso see Is it possible to use x tool instead of the one tool you use?",
        "question": "What is the significance of visualisation in the capstone project?",
        "section": "General course-related questions",
        "document_id": 29,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "We will probably have some calls during the Capstone period to clear some questions but it will be announced in advance if that happens.",
        "question": "When will the calls during the Capstone period be announced?",
        "section": "General course-related questions",
        "document_id": 30,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "We will probably have some calls during the Capstone period to clear some questions but it will be announced in advance if that happens.",
        "question": "What is the purpose of the calls during the Capstone period?",
        "section": "General course-related questions",
        "document_id": 30,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "We will probably have some calls during the Capstone period to clear some questions but it will be announced in advance if that happens.",
        "question": "How will participants be notified about the calls?",
        "section": "General course-related questions",
        "document_id": 30,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "We will probably have some calls during the Capstone period to clear some questions but it will be announced in advance if that happens.",
        "question": "Are the calls during the Capstone period mandatory?",
        "section": "General course-related questions",
        "document_id": 30,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "We will probably have some calls during the Capstone period to clear some questions but it will be announced in advance if that happens.",
        "question": "What type of questions might be discussed during the calls?",
        "section": "General course-related questions",
        "document_id": 30,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "We will use the same data, as the project will essentially remain the same as last year’s. The data is available here",
        "question": "What is the source of the data mentioned?",
        "section": "General course-related questions",
        "document_id": 31,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "We will use the same data, as the project will essentially remain the same as last year’s. The data is available here",
        "question": "Will the project change from last year?",
        "section": "General course-related questions",
        "document_id": 31,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "We will use the same data, as the project will essentially remain the same as last year’s. The data is available here",
        "question": "How is this year's project related to last year's?",
        "section": "General course-related questions",
        "document_id": 31,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "We will use the same data, as the project will essentially remain the same as last year’s. The data is available here",
        "question": "Where can one access the data used for the project?",
        "section": "General course-related questions",
        "document_id": 31,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "We will use the same data, as the project will essentially remain the same as last year’s. The data is available here",
        "question": "Is the project expected to have any new features this year?",
        "section": "General course-related questions",
        "document_id": 31,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, but we moved the 2022 stuff here",
        "question": "What does the text imply about the 2022 content?",
        "section": "General course-related questions",
        "document_id": 32,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, but we moved the 2022 stuff here",
        "question": "Where was the 2022 content moved to?",
        "section": "General course-related questions",
        "document_id": 32,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, but we moved the 2022 stuff here",
        "question": "Is there an indication that the 2022 content is complete?",
        "section": "General course-related questions",
        "document_id": 32,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, but we moved the 2022 stuff here",
        "question": "What might 'here' refer to in the context of the text?",
        "section": "General course-related questions",
        "document_id": 32,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No, but we moved the 2022 stuff here",
        "question": "Why is there a need to move the 2022 material?",
        "section": "General course-related questions",
        "document_id": 32,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, you can use any tool you want for your project.",
        "question": "What types of tools can be used for the project?",
        "section": "General course-related questions",
        "document_id": 33,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, you can use any tool you want for your project.",
        "question": "Are there any restrictions on the tools used for the project?",
        "section": "General course-related questions",
        "document_id": 33,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, you can use any tool you want for your project.",
        "question": "Can I use software tools for my project?",
        "section": "General course-related questions",
        "document_id": 33,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, you can use any tool you want for your project.",
        "question": "Is it permissible to select any tool for the project?",
        "section": "General course-related questions",
        "document_id": 33,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, you can use any tool you want for your project.",
        "question": "What criteria should be considered when choosing a tool for the project?",
        "section": "General course-related questions",
        "document_id": 33,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\nThe course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\nShould you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.",
        "question": "What are the two alternative data stacks covered in the course?",
        "section": "General course-related questions",
        "document_id": 34,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\nThe course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\nShould you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.",
        "question": "Can students choose tools other than the ones suggested in the course?",
        "section": "General course-related questions",
        "document_id": 34,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\nThe course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\nShould you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.",
        "question": "What tools can be substituted for Mage, GCP products, and Metabase?",
        "section": "General course-related questions",
        "document_id": 34,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\nThe course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\nShould you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.",
        "question": "What requirement must be fulfilled for the peer review of the capstone project?",
        "section": "General course-related questions",
        "document_id": 34,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\nThe course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\nShould you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.",
        "question": "What is one limitation mentioned regarding using a different stack in the course?",
        "section": "General course-related questions",
        "document_id": 34,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Star the repo! Share it with friends if you find it useful ❣️\nCreate a PR if you see you can improve the text or the structure of the repository.",
        "question": "What is the first action suggested for users in the text?",
        "section": "General course-related questions",
        "document_id": 35,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Star the repo! Share it with friends if you find it useful ❣️\nCreate a PR if you see you can improve the text or the structure of the repository.",
        "question": "How can users contribute to improving the repository?",
        "section": "General course-related questions",
        "document_id": 35,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Star the repo! Share it with friends if you find it useful ❣️\nCreate a PR if you see you can improve the text or the structure of the repository.",
        "question": "What symbol is used to express appreciation for the repository?",
        "section": "General course-related questions",
        "document_id": 35,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Star the repo! Share it with friends if you find it useful ❣️\nCreate a PR if you see you can improve the text or the structure of the repository.",
        "question": "What is the purpose of creating a PR according to the text?",
        "section": "General course-related questions",
        "document_id": 35,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Star the repo! Share it with friends if you find it useful ❣️\nCreate a PR if you see you can improve the text or the structure of the repository.",
        "question": "Who should users share the repository with if they find it useful?",
        "section": "General course-related questions",
        "document_id": 35,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes! Linux is ideal but technically it should not matter. Students last year used all 3 OSes successfully",
        "question": "What operating systems were used by students last year?",
        "section": "General course-related questions",
        "document_id": 36,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes! Linux is ideal but technically it should not matter. Students last year used all 3 OSes successfully",
        "question": "Why is Linux considered ideal according to the text?",
        "section": "General course-related questions",
        "document_id": 36,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes! Linux is ideal but technically it should not matter. Students last year used all 3 OSes successfully",
        "question": "Did students have success using different operating systems?",
        "section": "General course-related questions",
        "document_id": 36,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes! Linux is ideal but technically it should not matter. Students last year used all 3 OSes successfully",
        "question": "Is there a specific operating system that is recommended?",
        "section": "General course-related questions",
        "document_id": 36,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes! Linux is ideal but technically it should not matter. Students last year used all 3 OSes successfully",
        "question": "How many operating systems are mentioned in the text?",
        "section": "General course-related questions",
        "document_id": 36,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Have no idea how past cohorts got past this as I haven't read old slack messages, and no FAQ entries that I can find.\nLater modules (module-05 & RisingWave workshop) use shell scripts in *.sh files and most Windows users not using WSL would hit a wall and cannot continue, even in git bash or MINGW64. This is why WSL environment setup is recommended from the start.",
        "question": "What challenges do Windows users face when using shell scripts in the course modules?",
        "section": "General course-related questions",
        "document_id": 37,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Have no idea how past cohorts got past this as I haven't read old slack messages, and no FAQ entries that I can find.\nLater modules (module-05 & RisingWave workshop) use shell scripts in *.sh files and most Windows users not using WSL would hit a wall and cannot continue, even in git bash or MINGW64. This is why WSL environment setup is recommended from the start.",
        "question": "Why is WSL environment setup recommended for the course participants?",
        "section": "General course-related questions",
        "document_id": 37,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Have no idea how past cohorts got past this as I haven't read old slack messages, and no FAQ entries that I can find.\nLater modules (module-05 & RisingWave workshop) use shell scripts in *.sh files and most Windows users not using WSL would hit a wall and cannot continue, even in git bash or MINGW64. This is why WSL environment setup is recommended from the start.",
        "question": "What are the names of the modules that involve the use of shell scripts?",
        "section": "General course-related questions",
        "document_id": 37,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Have no idea how past cohorts got past this as I haven't read old slack messages, and no FAQ entries that I can find.\nLater modules (module-05 & RisingWave workshop) use shell scripts in *.sh files and most Windows users not using WSL would hit a wall and cannot continue, even in git bash or MINGW64. This is why WSL environment setup is recommended from the start.",
        "question": "What tools do Windows users typically use that may not be compatible with the course requirements?",
        "section": "General course-related questions",
        "document_id": 37,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Have no idea how past cohorts got past this as I haven't read old slack messages, and no FAQ entries that I can find.\nLater modules (module-05 & RisingWave workshop) use shell scripts in *.sh files and most Windows users not using WSL would hit a wall and cannot continue, even in git bash or MINGW64. This is why WSL environment setup is recommended from the start.",
        "question": "What resources did the author check to find solutions related to past cohorts' experiences?",
        "section": "General course-related questions",
        "document_id": 37,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md",
        "question": "What is the main topic of the document mentioned in the text?",
        "section": "General course-related questions",
        "document_id": 38,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md",
        "question": "Where can one find the document referenced in the text?",
        "section": "General course-related questions",
        "document_id": 38,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md",
        "question": "What does the document linked in the text pertain to?",
        "section": "General course-related questions",
        "document_id": 38,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md",
        "question": "Is the statement in the text affirming something specific?",
        "section": "General course-related questions",
        "document_id": 38,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md",
        "question": "What organization is associated with the document in the link?",
        "section": "General course-related questions",
        "document_id": 38,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You will have two attempts for a project. If the first project deadline is over and you’re late or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt.",
        "question": "How many attempts do you have for the project?",
        "section": "General course-related questions",
        "document_id": 39,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You will have two attempts for a project. If the first project deadline is over and you’re late or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt.",
        "question": "What happens if you are late for the first project deadline?",
        "section": "General course-related questions",
        "document_id": 39,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You will have two attempts for a project. If the first project deadline is over and you’re late or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt.",
        "question": "Can you submit the project again if you fail the first attempt?",
        "section": "General course-related questions",
        "document_id": 39,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You will have two attempts for a project. If the first project deadline is over and you’re late or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt.",
        "question": "What is the consequence of failing the first attempt on the project?",
        "section": "General course-related questions",
        "document_id": 39,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You will have two attempts for a project. If the first project deadline is over and you’re late or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt.",
        "question": "Is there a possibility to submit the project after the first deadline?",
        "section": "General course-related questions",
        "document_id": 39,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The first step is to try to solve the issue on your own. Get used to solving problems and reading documentation. This will be a real life skill you need when employed. [ctrl+f] is your friend, use it! It is a universal shortcut and works in all apps/browsers.\nWhat does the error say? There will often be a description of the error or instructions on what is needed or even how to fix it. I have even seen a link to the solution. Does it reference a specific line of your code?\nRestart app or server/pc.\nGoogle it, use ChatGPT, Bing AI etc.\nIt is going to be rare that you are the first to have the problem, someone out there has posted the fly issue and likely the solution.\nSearch using: <technology> <problem statement>. Example: pgcli error column c.relhasoids does not exist.\nThere are often different solutions for the same problem due to variation in environments.\nCheck the tech’s documentation. Use its search if available or use the browsers search function.\nTry uninstall (this may remove the bad actor) and reinstall of application or reimplementation of action. Remember to restart the server/pc for reinstalls.\nSometimes reinstalling fails to resolve the issue but works if you uninstall first.\nPost your question to Stackoverflow. Read the Stackoverflow guide on posting good questions.\nhttps://stackoverflow.com/help/how-to-ask\nThis will be your real life. Ask an expert in the future (in addition to coworkers).\nAsk in Slack\nBefore asking a question,\nCheck Pins (where the shortcut to the repo and this FAQ is located)\nUse the slack app’s search function\nUse the bot @ZoomcampQABot to do the search for you\ncheck the FAQ (this document), use search [ctrl+f]\nWhen asking a question, include as much information as possible:\nWhat are you coding on? What OS?\nWhat command did you run, which video did you follow? Etc etc\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.\nDO NOT use screenshots, especially don’t take pictures from a phone.\nDO NOT tag instructors, it may discourage others from helping you. Copy and paste errors; if it’s long, just post it in a reply to your thread.\nUse ``` for formatting your code.\nUse the same thread for the conversation (that means reply to your own thread).\nDO NOT create multiple posts to discuss the issue.\nlearYou may create a new post if the issue reemerges down the road. Describe what has changed in the environment.\nProvide additional information in the same thread of the steps you have taken for resolution.\nTake a break and come back later. You will be amazed at how often you figure out the solution after letting your brain rest. Get some fresh air, workout, play a video game, watch a tv show, whatever allows your brain to not think about it for a little while or even until the next day.\nRemember technology issues in real life sometimes take days or even weeks to resolve.\nIf somebody helped you with your problem and it's not in the FAQ, please add it there. It will help other students.",
        "question": "What are the first steps one should take to solve a technical issue on their own?",
        "section": "General course-related questions",
        "document_id": 40,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The first step is to try to solve the issue on your own. Get used to solving problems and reading documentation. This will be a real life skill you need when employed. [ctrl+f] is your friend, use it! It is a universal shortcut and works in all apps/browsers.\nWhat does the error say? There will often be a description of the error or instructions on what is needed or even how to fix it. I have even seen a link to the solution. Does it reference a specific line of your code?\nRestart app or server/pc.\nGoogle it, use ChatGPT, Bing AI etc.\nIt is going to be rare that you are the first to have the problem, someone out there has posted the fly issue and likely the solution.\nSearch using: <technology> <problem statement>. Example: pgcli error column c.relhasoids does not exist.\nThere are often different solutions for the same problem due to variation in environments.\nCheck the tech’s documentation. Use its search if available or use the browsers search function.\nTry uninstall (this may remove the bad actor) and reinstall of application or reimplementation of action. Remember to restart the server/pc for reinstalls.\nSometimes reinstalling fails to resolve the issue but works if you uninstall first.\nPost your question to Stackoverflow. Read the Stackoverflow guide on posting good questions.\nhttps://stackoverflow.com/help/how-to-ask\nThis will be your real life. Ask an expert in the future (in addition to coworkers).\nAsk in Slack\nBefore asking a question,\nCheck Pins (where the shortcut to the repo and this FAQ is located)\nUse the slack app’s search function\nUse the bot @ZoomcampQABot to do the search for you\ncheck the FAQ (this document), use search [ctrl+f]\nWhen asking a question, include as much information as possible:\nWhat are you coding on? What OS?\nWhat command did you run, which video did you follow? Etc etc\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.\nDO NOT use screenshots, especially don’t take pictures from a phone.\nDO NOT tag instructors, it may discourage others from helping you. Copy and paste errors; if it’s long, just post it in a reply to your thread.\nUse ``` for formatting your code.\nUse the same thread for the conversation (that means reply to your own thread).\nDO NOT create multiple posts to discuss the issue.\nlearYou may create a new post if the issue reemerges down the road. Describe what has changed in the environment.\nProvide additional information in the same thread of the steps you have taken for resolution.\nTake a break and come back later. You will be amazed at how often you figure out the solution after letting your brain rest. Get some fresh air, workout, play a video game, watch a tv show, whatever allows your brain to not think about it for a little while or even until the next day.\nRemember technology issues in real life sometimes take days or even weeks to resolve.\nIf somebody helped you with your problem and it's not in the FAQ, please add it there. It will help other students.",
        "question": "Why is it important to include as much information as possible when asking for help?",
        "section": "General course-related questions",
        "document_id": 40,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The first step is to try to solve the issue on your own. Get used to solving problems and reading documentation. This will be a real life skill you need when employed. [ctrl+f] is your friend, use it! It is a universal shortcut and works in all apps/browsers.\nWhat does the error say? There will often be a description of the error or instructions on what is needed or even how to fix it. I have even seen a link to the solution. Does it reference a specific line of your code?\nRestart app or server/pc.\nGoogle it, use ChatGPT, Bing AI etc.\nIt is going to be rare that you are the first to have the problem, someone out there has posted the fly issue and likely the solution.\nSearch using: <technology> <problem statement>. Example: pgcli error column c.relhasoids does not exist.\nThere are often different solutions for the same problem due to variation in environments.\nCheck the tech’s documentation. Use its search if available or use the browsers search function.\nTry uninstall (this may remove the bad actor) and reinstall of application or reimplementation of action. Remember to restart the server/pc for reinstalls.\nSometimes reinstalling fails to resolve the issue but works if you uninstall first.\nPost your question to Stackoverflow. Read the Stackoverflow guide on posting good questions.\nhttps://stackoverflow.com/help/how-to-ask\nThis will be your real life. Ask an expert in the future (in addition to coworkers).\nAsk in Slack\nBefore asking a question,\nCheck Pins (where the shortcut to the repo and this FAQ is located)\nUse the slack app’s search function\nUse the bot @ZoomcampQABot to do the search for you\ncheck the FAQ (this document), use search [ctrl+f]\nWhen asking a question, include as much information as possible:\nWhat are you coding on? What OS?\nWhat command did you run, which video did you follow? Etc etc\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.\nDO NOT use screenshots, especially don’t take pictures from a phone.\nDO NOT tag instructors, it may discourage others from helping you. Copy and paste errors; if it’s long, just post it in a reply to your thread.\nUse ``` for formatting your code.\nUse the same thread for the conversation (that means reply to your own thread).\nDO NOT create multiple posts to discuss the issue.\nlearYou may create a new post if the issue reemerges down the road. Describe what has changed in the environment.\nProvide additional information in the same thread of the steps you have taken for resolution.\nTake a break and come back later. You will be amazed at how often you figure out the solution after letting your brain rest. Get some fresh air, workout, play a video game, watch a tv show, whatever allows your brain to not think about it for a little while or even until the next day.\nRemember technology issues in real life sometimes take days or even weeks to resolve.\nIf somebody helped you with your problem and it's not in the FAQ, please add it there. It will help other students.",
        "question": "What should you do if reinstalling an application fails to resolve the issue?",
        "section": "General course-related questions",
        "document_id": 40,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The first step is to try to solve the issue on your own. Get used to solving problems and reading documentation. This will be a real life skill you need when employed. [ctrl+f] is your friend, use it! It is a universal shortcut and works in all apps/browsers.\nWhat does the error say? There will often be a description of the error or instructions on what is needed or even how to fix it. I have even seen a link to the solution. Does it reference a specific line of your code?\nRestart app or server/pc.\nGoogle it, use ChatGPT, Bing AI etc.\nIt is going to be rare that you are the first to have the problem, someone out there has posted the fly issue and likely the solution.\nSearch using: <technology> <problem statement>. Example: pgcli error column c.relhasoids does not exist.\nThere are often different solutions for the same problem due to variation in environments.\nCheck the tech’s documentation. Use its search if available or use the browsers search function.\nTry uninstall (this may remove the bad actor) and reinstall of application or reimplementation of action. Remember to restart the server/pc for reinstalls.\nSometimes reinstalling fails to resolve the issue but works if you uninstall first.\nPost your question to Stackoverflow. Read the Stackoverflow guide on posting good questions.\nhttps://stackoverflow.com/help/how-to-ask\nThis will be your real life. Ask an expert in the future (in addition to coworkers).\nAsk in Slack\nBefore asking a question,\nCheck Pins (where the shortcut to the repo and this FAQ is located)\nUse the slack app’s search function\nUse the bot @ZoomcampQABot to do the search for you\ncheck the FAQ (this document), use search [ctrl+f]\nWhen asking a question, include as much information as possible:\nWhat are you coding on? What OS?\nWhat command did you run, which video did you follow? Etc etc\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.\nDO NOT use screenshots, especially don’t take pictures from a phone.\nDO NOT tag instructors, it may discourage others from helping you. Copy and paste errors; if it’s long, just post it in a reply to your thread.\nUse ``` for formatting your code.\nUse the same thread for the conversation (that means reply to your own thread).\nDO NOT create multiple posts to discuss the issue.\nlearYou may create a new post if the issue reemerges down the road. Describe what has changed in the environment.\nProvide additional information in the same thread of the steps you have taken for resolution.\nTake a break and come back later. You will be amazed at how often you figure out the solution after letting your brain rest. Get some fresh air, workout, play a video game, watch a tv show, whatever allows your brain to not think about it for a little while or even until the next day.\nRemember technology issues in real life sometimes take days or even weeks to resolve.\nIf somebody helped you with your problem and it's not in the FAQ, please add it there. It will help other students.",
        "question": "How can you effectively use the Slack app when seeking help for technical problems?",
        "section": "General course-related questions",
        "document_id": 40,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The first step is to try to solve the issue on your own. Get used to solving problems and reading documentation. This will be a real life skill you need when employed. [ctrl+f] is your friend, use it! It is a universal shortcut and works in all apps/browsers.\nWhat does the error say? There will often be a description of the error or instructions on what is needed or even how to fix it. I have even seen a link to the solution. Does it reference a specific line of your code?\nRestart app or server/pc.\nGoogle it, use ChatGPT, Bing AI etc.\nIt is going to be rare that you are the first to have the problem, someone out there has posted the fly issue and likely the solution.\nSearch using: <technology> <problem statement>. Example: pgcli error column c.relhasoids does not exist.\nThere are often different solutions for the same problem due to variation in environments.\nCheck the tech’s documentation. Use its search if available or use the browsers search function.\nTry uninstall (this may remove the bad actor) and reinstall of application or reimplementation of action. Remember to restart the server/pc for reinstalls.\nSometimes reinstalling fails to resolve the issue but works if you uninstall first.\nPost your question to Stackoverflow. Read the Stackoverflow guide on posting good questions.\nhttps://stackoverflow.com/help/how-to-ask\nThis will be your real life. Ask an expert in the future (in addition to coworkers).\nAsk in Slack\nBefore asking a question,\nCheck Pins (where the shortcut to the repo and this FAQ is located)\nUse the slack app’s search function\nUse the bot @ZoomcampQABot to do the search for you\ncheck the FAQ (this document), use search [ctrl+f]\nWhen asking a question, include as much information as possible:\nWhat are you coding on? What OS?\nWhat command did you run, which video did you follow? Etc etc\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.\nDO NOT use screenshots, especially don’t take pictures from a phone.\nDO NOT tag instructors, it may discourage others from helping you. Copy and paste errors; if it’s long, just post it in a reply to your thread.\nUse ``` for formatting your code.\nUse the same thread for the conversation (that means reply to your own thread).\nDO NOT create multiple posts to discuss the issue.\nlearYou may create a new post if the issue reemerges down the road. Describe what has changed in the environment.\nProvide additional information in the same thread of the steps you have taken for resolution.\nTake a break and come back later. You will be amazed at how often you figure out the solution after letting your brain rest. Get some fresh air, workout, play a video game, watch a tv show, whatever allows your brain to not think about it for a little while or even until the next day.\nRemember technology issues in real life sometimes take days or even weeks to resolve.\nIf somebody helped you with your problem and it's not in the FAQ, please add it there. It will help other students.",
        "question": "What is the significance of taking a break when dealing with technology issues?",
        "section": "General course-related questions",
        "document_id": 40,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When the troubleshooting guide above does not help resolve it and you need another pair of eyeballs to spot mistakes. When asking a question, include as much information as possible:\nWhat are you coding on? What OS?\nWhat command did you run, which video did you follow? Etc etc\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.",
        "question": "What information should you include when asking for help with coding issues?",
        "section": "General course-related questions",
        "document_id": 41,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When the troubleshooting guide above does not help resolve it and you need another pair of eyeballs to spot mistakes. When asking a question, include as much information as possible:\nWhat are you coding on? What OS?\nWhat command did you run, which video did you follow? Etc etc\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.",
        "question": "Why is it important to mention the operating system you are coding on?",
        "section": "General course-related questions",
        "document_id": 41,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When the troubleshooting guide above does not help resolve it and you need another pair of eyeballs to spot mistakes. When asking a question, include as much information as possible:\nWhat are you coding on? What OS?\nWhat command did you run, which video did you follow? Etc etc\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.",
        "question": "What types of questions should you answer regarding the actions you have taken?",
        "section": "General course-related questions",
        "document_id": 41,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When the troubleshooting guide above does not help resolve it and you need another pair of eyeballs to spot mistakes. When asking a question, include as much information as possible:\nWhat are you coding on? What OS?\nWhat command did you run, which video did you follow? Etc etc\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.",
        "question": "How can specifying the command you ran help in troubleshooting?",
        "section": "General course-related questions",
        "document_id": 41,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When the troubleshooting guide above does not help resolve it and you need another pair of eyeballs to spot mistakes. When asking a question, include as much information as possible:\nWhat are you coding on? What OS?\nWhat command did you run, which video did you follow? Etc etc\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.",
        "question": "Why is it crucial to mention what you have already tried that did not work?",
        "section": "General course-related questions",
        "document_id": 41,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\nThis is also a great resource: https://dangitgit.com/",
        "question": "What is the first step to take after creating a GitHub account?",
        "section": "General course-related questions",
        "document_id": 42,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\nThis is also a great resource: https://dangitgit.com/",
        "question": "Why is it important to have a local repository on your computer?",
        "section": "General course-related questions",
        "document_id": 42,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\nThis is also a great resource: https://dangitgit.com/",
        "question": "What should you remember to ignore when creating your own repositories?",
        "section": "General course-related questions",
        "document_id": 42,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\nThis is also a great resource: https://dangitgit.com/",
        "question": "Where can you find a tutorial on how to set up a repository?",
        "section": "General course-related questions",
        "document_id": 42,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\nThis is also a great resource: https://dangitgit.com/",
        "question": "What should you never store in a git repository, even if it's private?",
        "section": "General course-related questions",
        "document_id": 42,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: Makefile:2: *** missing separator.  Stop.\nSolution: Tabs in document should be converted to Tab instead of spaces. Follow this stack.",
        "question": "What is the error stated in the text?",
        "section": "General course-related questions",
        "document_id": 43,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: Makefile:2: *** missing separator.  Stop.\nSolution: Tabs in document should be converted to Tab instead of spaces. Follow this stack.",
        "question": "Where is the error located in the Makefile?",
        "section": "General course-related questions",
        "document_id": 43,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: Makefile:2: *** missing separator.  Stop.\nSolution: Tabs in document should be converted to Tab instead of spaces. Follow this stack.",
        "question": "What is the solution to the missing separator error?",
        "section": "General course-related questions",
        "document_id": 43,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: Makefile:2: *** missing separator.  Stop.\nSolution: Tabs in document should be converted to Tab instead of spaces. Follow this stack.",
        "question": "What should be used in the document instead of spaces?",
        "section": "General course-related questions",
        "document_id": 43,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: Makefile:2: *** missing separator.  Stop.\nSolution: Tabs in document should be converted to Tab instead of spaces. Follow this stack.",
        "question": "What does the text suggest to follow for troubleshooting?",
        "section": "General course-related questions",
        "document_id": 43,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re running Linux on Windows Subsystem for Linux (WSL) 2, you can open HTML files from the guest (Linux) with whatever Internet Browser you have installed on the host (Windows). Just install wslu and open the page with wslview <file>, for example:\nwslview index.html\nYou can customise which browser to use by setting the BROWSER environment variable first. For example:\nexport BROWSER='/mnt/c/Program Files/Firefox/firefox.exe'",
        "question": "What is the main function of wslview in WSL 2?",
        "section": "General course-related questions",
        "document_id": 44,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re running Linux on Windows Subsystem for Linux (WSL) 2, you can open HTML files from the guest (Linux) with whatever Internet Browser you have installed on the host (Windows). Just install wslu and open the page with wslview <file>, for example:\nwslview index.html\nYou can customise which browser to use by setting the BROWSER environment variable first. For example:\nexport BROWSER='/mnt/c/Program Files/Firefox/firefox.exe'",
        "question": "How can you set a custom browser for opening HTML files in WSL 2?",
        "section": "General course-related questions",
        "document_id": 44,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re running Linux on Windows Subsystem for Linux (WSL) 2, you can open HTML files from the guest (Linux) with whatever Internet Browser you have installed on the host (Windows). Just install wslu and open the page with wslview <file>, for example:\nwslview index.html\nYou can customise which browser to use by setting the BROWSER environment variable first. For example:\nexport BROWSER='/mnt/c/Program Files/Firefox/firefox.exe'",
        "question": "What command is used to open an HTML file with wslview?",
        "section": "General course-related questions",
        "document_id": 44,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re running Linux on Windows Subsystem for Linux (WSL) 2, you can open HTML files from the guest (Linux) with whatever Internet Browser you have installed on the host (Windows). Just install wslu and open the page with wslview <file>, for example:\nwslview index.html\nYou can customise which browser to use by setting the BROWSER environment variable first. For example:\nexport BROWSER='/mnt/c/Program Files/Firefox/firefox.exe'",
        "question": "Where should the browser path be set for the BROWSER environment variable?",
        "section": "General course-related questions",
        "document_id": 44,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re running Linux on Windows Subsystem for Linux (WSL) 2, you can open HTML files from the guest (Linux) with whatever Internet Browser you have installed on the host (Windows). Just install wslu and open the page with wslview <file>, for example:\nwslview index.html\nYou can customise which browser to use by setting the BROWSER environment variable first. For example:\nexport BROWSER='/mnt/c/Program Files/Firefox/firefox.exe'",
        "question": "What system does WSL 2 allow you to run Linux on?",
        "section": "General course-related questions",
        "document_id": 44,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This tutorial shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface.\nTaxi Data - Yellow Taxi Trip Records downloading error, Error no or XML error webpage\nWhen you try to download the 2021 data from TLC website, you get this error:\nIf you click on the link, and ERROR 403: Forbidden on the terminal.\nWe have a backup, so use it instead: https://github.com/DataTalksClub/nyc-tlc-data\nSo the link should be https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\nNote: Make sure to unzip the “gz” file (no, the “unzip” command won’t work for this.)\n“gzip -d file.gz”g",
        "question": "What is the purpose of the Chrome Remote Desktop service?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 45,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This tutorial shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface.\nTaxi Data - Yellow Taxi Trip Records downloading error, Error no or XML error webpage\nWhen you try to download the 2021 data from TLC website, you get this error:\nIf you click on the link, and ERROR 403: Forbidden on the terminal.\nWe have a backup, so use it instead: https://github.com/DataTalksClub/nyc-tlc-data\nSo the link should be https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\nNote: Make sure to unzip the “gz” file (no, the “unzip” command won’t work for this.)\n“gzip -d file.gz”g",
        "question": "How do you access the Yellow Taxi Trip Records for 2021?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 45,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This tutorial shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface.\nTaxi Data - Yellow Taxi Trip Records downloading error, Error no or XML error webpage\nWhen you try to download the 2021 data from TLC website, you get this error:\nIf you click on the link, and ERROR 403: Forbidden on the terminal.\nWe have a backup, so use it instead: https://github.com/DataTalksClub/nyc-tlc-data\nSo the link should be https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\nNote: Make sure to unzip the “gz” file (no, the “unzip” command won’t work for this.)\n“gzip -d file.gz”g",
        "question": "What error occurs when trying to download data directly from the TLC website?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 45,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This tutorial shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface.\nTaxi Data - Yellow Taxi Trip Records downloading error, Error no or XML error webpage\nWhen you try to download the 2021 data from TLC website, you get this error:\nIf you click on the link, and ERROR 403: Forbidden on the terminal.\nWe have a backup, so use it instead: https://github.com/DataTalksClub/nyc-tlc-data\nSo the link should be https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\nNote: Make sure to unzip the “gz” file (no, the “unzip” command won’t work for this.)\n“gzip -d file.gz”g",
        "question": "Where can you find a backup of the Yellow Taxi Trip Records data?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 45,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This tutorial shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface.\nTaxi Data - Yellow Taxi Trip Records downloading error, Error no or XML error webpage\nWhen you try to download the 2021 data from TLC website, you get this error:\nIf you click on the link, and ERROR 403: Forbidden on the terminal.\nWe have a backup, so use it instead: https://github.com/DataTalksClub/nyc-tlc-data\nSo the link should be https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\nNote: Make sure to unzip the “gz” file (no, the “unzip” command won’t work for this.)\n“gzip -d file.gz”g",
        "question": "What command should you use to unzip a '.gz' file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 45,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In this video, we store the data file as “output.csv”. The data file won’t store correctly if the file extension is csv.gz instead of csv. One alternative is to replace csv_name = “output.cs -v” with the file name given at the end of the URL. Notice that the URL for the yellow taxi data is: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz where the highlighted part is the name of the file. We can parse this file name from the URL and use it as csv_name. That is, we can replace csv_name = “output.csv” with\ncsv_name = url.split(“/”)[-1] . Then when we use csv_name to using pd.read_csv, there won’t be an issue even though the file name really has the extension csv.gz instead of csv since the pandas read_csv function can read csv.gz files directly.",
        "question": "What file extension is required for the data file to store correctly?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 46,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In this video, we store the data file as “output.csv”. The data file won’t store correctly if the file extension is csv.gz instead of csv. One alternative is to replace csv_name = “output.cs -v” with the file name given at the end of the URL. Notice that the URL for the yellow taxi data is: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz where the highlighted part is the name of the file. We can parse this file name from the URL and use it as csv_name. That is, we can replace csv_name = “output.csv” with\ncsv_name = url.split(“/”)[-1] . Then when we use csv_name to using pd.read_csv, there won’t be an issue even though the file name really has the extension csv.gz instead of csv since the pandas read_csv function can read csv.gz files directly.",
        "question": "How can we modify the variable csv_name to use the file name from a URL?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 46,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In this video, we store the data file as “output.csv”. The data file won’t store correctly if the file extension is csv.gz instead of csv. One alternative is to replace csv_name = “output.cs -v” with the file name given at the end of the URL. Notice that the URL for the yellow taxi data is: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz where the highlighted part is the name of the file. We can parse this file name from the URL and use it as csv_name. That is, we can replace csv_name = “output.csv” with\ncsv_name = url.split(“/”)[-1] . Then when we use csv_name to using pd.read_csv, there won’t be an issue even though the file name really has the extension csv.gz instead of csv since the pandas read_csv function can read csv.gz files directly.",
        "question": "What is the URL provided for the yellow taxi data?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 46,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In this video, we store the data file as “output.csv”. The data file won’t store correctly if the file extension is csv.gz instead of csv. One alternative is to replace csv_name = “output.cs -v” with the file name given at the end of the URL. Notice that the URL for the yellow taxi data is: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz where the highlighted part is the name of the file. We can parse this file name from the URL and use it as csv_name. That is, we can replace csv_name = “output.csv” with\ncsv_name = url.split(“/”)[-1] . Then when we use csv_name to using pd.read_csv, there won’t be an issue even though the file name really has the extension csv.gz instead of csv since the pandas read_csv function can read csv.gz files directly.",
        "question": "What function can be used to read .csv.gz files directly in pandas?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 46,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In this video, we store the data file as “output.csv”. The data file won’t store correctly if the file extension is csv.gz instead of csv. One alternative is to replace csv_name = “output.cs -v” with the file name given at the end of the URL. Notice that the URL for the yellow taxi data is: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz where the highlighted part is the name of the file. We can parse this file name from the URL and use it as csv_name. That is, we can replace csv_name = “output.csv” with\ncsv_name = url.split(“/”)[-1] . Then when we use csv_name to using pd.read_csv, there won’t be an issue even though the file name really has the extension csv.gz instead of csv since the pandas read_csv function can read csv.gz files directly.",
        "question": "What is the importance of the file extension in determining how the data file is stored?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 46,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yellow Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\nGreen Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf",
        "question": "What information can be found in the Yellow Trips data dictionary?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 47,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yellow Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\nGreen Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf",
        "question": "What type of trips does the Green Trips data dictionary cover?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 47,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yellow Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\nGreen Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf",
        "question": "Where can one access the Yellow Trips data dictionary?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 47,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yellow Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\nGreen Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf",
        "question": "Are the Yellow and Green Trips data dictionaries accessible online?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 47,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Yellow Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\nGreen Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf",
        "question": "What is the purpose of the data dictionaries for Yellow and Green Trips?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 47,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can unzip this downloaded parquet file, in the command line. The result is a csv file which can be imported with pandas using the pd.read_csv() shown in the videos.\n‘’’gunzip green_tripdata_2019-09.csv.gz’’’\nSOLUTION TO USING PARQUET FILES DIRECTLY IN PYTHON SCRIPT ingest_data.py\nIn the def main(params) add this line\nparquet_name= 'output.parquet'\nThen edit the code which downloads the files\nos.system(f\"wget {url} -O {parquet_name}\")\nConvert the download .parquet file to csv and rename as csv_name to keep it relevant to the rest of the code\ndf = pd.read_parquet(parquet_name)\ndf.to_csv(csv_name, index=False)",
        "question": "How can you unzip a downloaded parquet file using the command line?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 48,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can unzip this downloaded parquet file, in the command line. The result is a csv file which can be imported with pandas using the pd.read_csv() shown in the videos.\n‘’’gunzip green_tripdata_2019-09.csv.gz’’’\nSOLUTION TO USING PARQUET FILES DIRECTLY IN PYTHON SCRIPT ingest_data.py\nIn the def main(params) add this line\nparquet_name= 'output.parquet'\nThen edit the code which downloads the files\nos.system(f\"wget {url} -O {parquet_name}\")\nConvert the download .parquet file to csv and rename as csv_name to keep it relevant to the rest of the code\ndf = pd.read_parquet(parquet_name)\ndf.to_csv(csv_name, index=False)",
        "question": "What command is used to download a parquet file in the provided Python script?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 48,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can unzip this downloaded parquet file, in the command line. The result is a csv file which can be imported with pandas using the pd.read_csv() shown in the videos.\n‘’’gunzip green_tripdata_2019-09.csv.gz’’’\nSOLUTION TO USING PARQUET FILES DIRECTLY IN PYTHON SCRIPT ingest_data.py\nIn the def main(params) add this line\nparquet_name= 'output.parquet'\nThen edit the code which downloads the files\nos.system(f\"wget {url} -O {parquet_name}\")\nConvert the download .parquet file to csv and rename as csv_name to keep it relevant to the rest of the code\ndf = pd.read_parquet(parquet_name)\ndf.to_csv(csv_name, index=False)",
        "question": "How do you convert a parquet file to a csv file using pandas?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 48,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can unzip this downloaded parquet file, in the command line. The result is a csv file which can be imported with pandas using the pd.read_csv() shown in the videos.\n‘’’gunzip green_tripdata_2019-09.csv.gz’’’\nSOLUTION TO USING PARQUET FILES DIRECTLY IN PYTHON SCRIPT ingest_data.py\nIn the def main(params) add this line\nparquet_name= 'output.parquet'\nThen edit the code which downloads the files\nos.system(f\"wget {url} -O {parquet_name}\")\nConvert the download .parquet file to csv and rename as csv_name to keep it relevant to the rest of the code\ndf = pd.read_parquet(parquet_name)\ndf.to_csv(csv_name, index=False)",
        "question": "What is the purpose of the 'csv_name' variable in the code?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 48,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can unzip this downloaded parquet file, in the command line. The result is a csv file which can be imported with pandas using the pd.read_csv() shown in the videos.\n‘’’gunzip green_tripdata_2019-09.csv.gz’’’\nSOLUTION TO USING PARQUET FILES DIRECTLY IN PYTHON SCRIPT ingest_data.py\nIn the def main(params) add this line\nparquet_name= 'output.parquet'\nThen edit the code which downloads the files\nos.system(f\"wget {url} -O {parquet_name}\")\nConvert the download .parquet file to csv and rename as csv_name to keep it relevant to the rest of the code\ndf = pd.read_parquet(parquet_name)\ndf.to_csv(csv_name, index=False)",
        "question": "What function is used to read a parquet file in the provided code?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 48,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "“wget is not recognized as an internal or external command”, you need to install it.\nOn Ubuntu, run:\n$ sudo apt-get install wget\nOn MacOS, the easiest way to install wget is to use Brew:\n$ brew install wget\nOn Windows, the easiest way to install wget is to use Chocolatey:\n$ choco install wget\nOr you can download a binary (https://gnuwin32.sourceforge.net/packages/wget.htm) and put it to any location in your PATH (e.g. C:/tools/)\nAlso, you can following this step to install Wget on MS Windows\n* Download the latest wget binary for windows from [eternallybored] (https://eternallybored.org/misc/wget/) (they are available as a zip with documentation, or just an exe)\n* If you downloaded the zip, extract all (if windows built in zip utility gives an error, use [7-zip] (https://7-zip.org/)).\n* Rename the file `wget64.exe` to `wget.exe` if necessary.\n* Move wget.exe to your `Git\\mingw64\\bin\\`.\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need to use\npython -m wget\nYou need to install it with pip first:\npip install wget\nAlternatively, you can just paste the file URL into your web browser and download the file normally that way. You’ll want to move the resulting file into your working directory.\nAlso recommended a look at the python library requests for the loading gz file  https://pypi.org/project/requests",
        "question": "What command do you use to install wget on Ubuntu?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 49,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "“wget is not recognized as an internal or external command”, you need to install it.\nOn Ubuntu, run:\n$ sudo apt-get install wget\nOn MacOS, the easiest way to install wget is to use Brew:\n$ brew install wget\nOn Windows, the easiest way to install wget is to use Chocolatey:\n$ choco install wget\nOr you can download a binary (https://gnuwin32.sourceforge.net/packages/wget.htm) and put it to any location in your PATH (e.g. C:/tools/)\nAlso, you can following this step to install Wget on MS Windows\n* Download the latest wget binary for windows from [eternallybored] (https://eternallybored.org/misc/wget/) (they are available as a zip with documentation, or just an exe)\n* If you downloaded the zip, extract all (if windows built in zip utility gives an error, use [7-zip] (https://7-zip.org/)).\n* Rename the file `wget64.exe` to `wget.exe` if necessary.\n* Move wget.exe to your `Git\\mingw64\\bin\\`.\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need to use\npython -m wget\nYou need to install it with pip first:\npip install wget\nAlternatively, you can just paste the file URL into your web browser and download the file normally that way. You’ll want to move the resulting file into your working directory.\nAlso recommended a look at the python library requests for the loading gz file  https://pypi.org/project/requests",
        "question": "How can you install wget on MacOS using Brew?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 49,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "“wget is not recognized as an internal or external command”, you need to install it.\nOn Ubuntu, run:\n$ sudo apt-get install wget\nOn MacOS, the easiest way to install wget is to use Brew:\n$ brew install wget\nOn Windows, the easiest way to install wget is to use Chocolatey:\n$ choco install wget\nOr you can download a binary (https://gnuwin32.sourceforge.net/packages/wget.htm) and put it to any location in your PATH (e.g. C:/tools/)\nAlso, you can following this step to install Wget on MS Windows\n* Download the latest wget binary for windows from [eternallybored] (https://eternallybored.org/misc/wget/) (they are available as a zip with documentation, or just an exe)\n* If you downloaded the zip, extract all (if windows built in zip utility gives an error, use [7-zip] (https://7-zip.org/)).\n* Rename the file `wget64.exe` to `wget.exe` if necessary.\n* Move wget.exe to your `Git\\mingw64\\bin\\`.\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need to use\npython -m wget\nYou need to install it with pip first:\npip install wget\nAlternatively, you can just paste the file URL into your web browser and download the file normally that way. You’ll want to move the resulting file into your working directory.\nAlso recommended a look at the python library requests for the loading gz file  https://pypi.org/project/requests",
        "question": "What is the command to install wget on Windows using Chocolatey?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 49,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "“wget is not recognized as an internal or external command”, you need to install it.\nOn Ubuntu, run:\n$ sudo apt-get install wget\nOn MacOS, the easiest way to install wget is to use Brew:\n$ brew install wget\nOn Windows, the easiest way to install wget is to use Chocolatey:\n$ choco install wget\nOr you can download a binary (https://gnuwin32.sourceforge.net/packages/wget.htm) and put it to any location in your PATH (e.g. C:/tools/)\nAlso, you can following this step to install Wget on MS Windows\n* Download the latest wget binary for windows from [eternallybored] (https://eternallybored.org/misc/wget/) (they are available as a zip with documentation, or just an exe)\n* If you downloaded the zip, extract all (if windows built in zip utility gives an error, use [7-zip] (https://7-zip.org/)).\n* Rename the file `wget64.exe` to `wget.exe` if necessary.\n* Move wget.exe to your `Git\\mingw64\\bin\\`.\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need to use\npython -m wget\nYou need to install it with pip first:\npip install wget\nAlternatively, you can just paste the file URL into your web browser and download the file normally that way. You’ll want to move the resulting file into your working directory.\nAlso recommended a look at the python library requests for the loading gz file  https://pypi.org/project/requests",
        "question": "What steps should be followed to install wget on Windows manually?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 49,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "“wget is not recognized as an internal or external command”, you need to install it.\nOn Ubuntu, run:\n$ sudo apt-get install wget\nOn MacOS, the easiest way to install wget is to use Brew:\n$ brew install wget\nOn Windows, the easiest way to install wget is to use Chocolatey:\n$ choco install wget\nOr you can download a binary (https://gnuwin32.sourceforge.net/packages/wget.htm) and put it to any location in your PATH (e.g. C:/tools/)\nAlso, you can following this step to install Wget on MS Windows\n* Download the latest wget binary for windows from [eternallybored] (https://eternallybored.org/misc/wget/) (they are available as a zip with documentation, or just an exe)\n* If you downloaded the zip, extract all (if windows built in zip utility gives an error, use [7-zip] (https://7-zip.org/)).\n* Rename the file `wget64.exe` to `wget.exe` if necessary.\n* Move wget.exe to your `Git\\mingw64\\bin\\`.\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need to use\npython -m wget\nYou need to install it with pip first:\npip install wget\nAlternatively, you can just paste the file URL into your web browser and download the file normally that way. You’ll want to move the resulting file into your working directory.\nAlso recommended a look at the python library requests for the loading gz file  https://pypi.org/project/requests",
        "question": "What alternative method is suggested for downloading files if wget is not available?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 49,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Firstly, make sure that you add “!” before wget if you’re running your command in a Jupyter Notebook or CLI. Then, you can check one of this 2 things (from CLI):\nUsing the Python library wget you installed with pip, try python -m wget <url>\nWrite the usual command and add --no-check-certificate at the end. So it should be:\n!wget <website_url> --no-check-certificate",
        "question": "What should you add before the wget command when using a Jupyter Notebook or CLI?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 50,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Firstly, make sure that you add “!” before wget if you’re running your command in a Jupyter Notebook or CLI. Then, you can check one of this 2 things (from CLI):\nUsing the Python library wget you installed with pip, try python -m wget <url>\nWrite the usual command and add --no-check-certificate at the end. So it should be:\n!wget <website_url> --no-check-certificate",
        "question": "How can you check the functionality of the wget library in Python?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 50,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Firstly, make sure that you add “!” before wget if you’re running your command in a Jupyter Notebook or CLI. Then, you can check one of this 2 things (from CLI):\nUsing the Python library wget you installed with pip, try python -m wget <url>\nWrite the usual command and add --no-check-certificate at the end. So it should be:\n!wget <website_url> --no-check-certificate",
        "question": "What command should you run if you want to disable certificate checks with wget?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 50,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Firstly, make sure that you add “!” before wget if you’re running your command in a Jupyter Notebook or CLI. Then, you can check one of this 2 things (from CLI):\nUsing the Python library wget you installed with pip, try python -m wget <url>\nWrite the usual command and add --no-check-certificate at the end. So it should be:\n!wget <website_url> --no-check-certificate",
        "question": "What is the purpose of the --no-check-certificate option in the wget command?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 50,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Firstly, make sure that you add “!” before wget if you’re running your command in a Jupyter Notebook or CLI. Then, you can check one of this 2 things (from CLI):\nUsing the Python library wget you installed with pip, try python -m wget <url>\nWrite the usual command and add --no-check-certificate at the end. So it should be:\n!wget <website_url> --no-check-certificate",
        "question": "What is the correct syntax for using wget in a Jupyter Notebook?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 50,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For those who wish to use the backslash as an escape character in Git Bash for Windows (as Alexey normally does), type in the terminal: bash.escapeChar=\\ (no need to include in .bashrc)",
        "question": "What command should be typed in the terminal to use the backslash as an escape character in Git Bash for Windows?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 51,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For those who wish to use the backslash as an escape character in Git Bash for Windows (as Alexey normally does), type in the terminal: bash.escapeChar=\\ (no need to include in .bashrc)",
        "question": "Is it necessary to include the escape character command in the .bashrc file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 51,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For those who wish to use the backslash as an escape character in Git Bash for Windows (as Alexey normally does), type in the terminal: bash.escapeChar=\\ (no need to include in .bashrc)",
        "question": "Who is mentioned as normally using the backslash as an escape character?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 51,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For those who wish to use the backslash as an escape character in Git Bash for Windows (as Alexey normally does), type in the terminal: bash.escapeChar=\\ (no need to include in .bashrc)",
        "question": "What operating system is referenced in the text regarding Git Bash?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 51,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For those who wish to use the backslash as an escape character in Git Bash for Windows (as Alexey normally does), type in the terminal: bash.escapeChar=\\ (no need to include in .bashrc)",
        "question": "What is the syntax to set the escape character in Git Bash according to the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 51,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Instruction on how to store secrets that will be avialable in GitHub  Codespaces.\nManaging your account-specific secrets for GitHub Codespaces - GitHub Docs",
        "question": "What are account-specific secrets in GitHub Codespaces?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 52,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Instruction on how to store secrets that will be avialable in GitHub  Codespaces.\nManaging your account-specific secrets for GitHub Codespaces - GitHub Docs",
        "question": "How can I store secrets in GitHub Codespaces?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 52,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Instruction on how to store secrets that will be avialable in GitHub  Codespaces.\nManaging your account-specific secrets for GitHub Codespaces - GitHub Docs",
        "question": "Where can I find instructions for managing secrets in GitHub Codespaces?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 52,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Instruction on how to store secrets that will be avialable in GitHub  Codespaces.\nManaging your account-specific secrets for GitHub Codespaces - GitHub Docs",
        "question": "What is the purpose of storing secrets in Codespaces?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 52,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Instruction on how to store secrets that will be avialable in GitHub  Codespaces.\nManaging your account-specific secrets for GitHub Codespaces - GitHub Docs",
        "question": "Are there any best practices for managing secrets in GitHub?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 52,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure you're able to start the Docker daemon, and check the issue immediately down below:\nAnd don’t forget to update the wsl in powershell the  command is wsl –update",
        "question": "What is the first step mentioned in the text regarding Docker?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 53,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure you're able to start the Docker daemon, and check the issue immediately down below:\nAnd don’t forget to update the wsl in powershell the  command is wsl –update",
        "question": "How can one check the issue after starting the Docker daemon?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 53,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure you're able to start the Docker daemon, and check the issue immediately down below:\nAnd don’t forget to update the wsl in powershell the  command is wsl –update",
        "question": "What command is used to update WSL in PowerShell?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 53,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure you're able to start the Docker daemon, and check the issue immediately down below:\nAnd don’t forget to update the wsl in powershell the  command is wsl –update",
        "question": "Is there a specific tool mentioned that needs to have its daemon started?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 53,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure you're able to start the Docker daemon, and check the issue immediately down below:\nAnd don’t forget to update the wsl in powershell the  command is wsl –update",
        "question": "What platform is mentioned in relation to updating WSL?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 53,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "As the official Docker for Windows documentation says, the Docker engine can either use the\nHyper-V or WSL2 as its backend. However, a few constraints might apply\nWindows 10 Pro / 11 Pro Users: \nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\nWindows 10 Home / 11 Home Users: \nOn the other hand, Users of the 'Home' version do NOT have the option Hyper-V option enabled, which means, you can only get Docker up and running using the WSL2 credentials(Windows Subsystem for Linux). Url\nYou can find the detailed instructions to do so here: rt ghttps://pureinfotech.com/install-wsl-windows-11/\nIn case, you run into another issue while trying to install WSL2 (WslRegisterDistribution failed with error: 0x800701bc), Make sure you update the WSL2 Linux Kernel, following the guidelines here: \n\nhttps://github.com/microsoft/WSL/issues/5393",
        "question": "What are the two back-end options for the Docker engine on Windows?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 54,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "As the official Docker for Windows documentation says, the Docker engine can either use the\nHyper-V or WSL2 as its backend. However, a few constraints might apply\nWindows 10 Pro / 11 Pro Users: \nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\nWindows 10 Home / 11 Home Users: \nOn the other hand, Users of the 'Home' version do NOT have the option Hyper-V option enabled, which means, you can only get Docker up and running using the WSL2 credentials(Windows Subsystem for Linux). Url\nYou can find the detailed instructions to do so here: rt ghttps://pureinfotech.com/install-wsl-windows-11/\nIn case, you run into another issue while trying to install WSL2 (WslRegisterDistribution failed with error: 0x800701bc), Make sure you update the WSL2 Linux Kernel, following the guidelines here: \n\nhttps://github.com/microsoft/WSL/issues/5393",
        "question": "What is required for Windows 10 Pro / 11 Pro users to use Hyper-V?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 54,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "As the official Docker for Windows documentation says, the Docker engine can either use the\nHyper-V or WSL2 as its backend. However, a few constraints might apply\nWindows 10 Pro / 11 Pro Users: \nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\nWindows 10 Home / 11 Home Users: \nOn the other hand, Users of the 'Home' version do NOT have the option Hyper-V option enabled, which means, you can only get Docker up and running using the WSL2 credentials(Windows Subsystem for Linux). Url\nYou can find the detailed instructions to do so here: rt ghttps://pureinfotech.com/install-wsl-windows-11/\nIn case, you run into another issue while trying to install WSL2 (WslRegisterDistribution failed with error: 0x800701bc), Make sure you update the WSL2 Linux Kernel, following the guidelines here: \n\nhttps://github.com/microsoft/WSL/issues/5393",
        "question": "Can Windows 10 Home / 11 Home users use Hyper-V with Docker?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 54,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "As the official Docker for Windows documentation says, the Docker engine can either use the\nHyper-V or WSL2 as its backend. However, a few constraints might apply\nWindows 10 Pro / 11 Pro Users: \nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\nWindows 10 Home / 11 Home Users: \nOn the other hand, Users of the 'Home' version do NOT have the option Hyper-V option enabled, which means, you can only get Docker up and running using the WSL2 credentials(Windows Subsystem for Linux). Url\nYou can find the detailed instructions to do so here: rt ghttps://pureinfotech.com/install-wsl-windows-11/\nIn case, you run into another issue while trying to install WSL2 (WslRegisterDistribution failed with error: 0x800701bc), Make sure you update the WSL2 Linux Kernel, following the guidelines here: \n\nhttps://github.com/microsoft/WSL/issues/5393",
        "question": "Where can users find instructions to install WSL2 on Windows?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 54,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "As the official Docker for Windows documentation says, the Docker engine can either use the\nHyper-V or WSL2 as its backend. However, a few constraints might apply\nWindows 10 Pro / 11 Pro Users: \nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\nWindows 10 Home / 11 Home Users: \nOn the other hand, Users of the 'Home' version do NOT have the option Hyper-V option enabled, which means, you can only get Docker up and running using the WSL2 credentials(Windows Subsystem for Linux). Url\nYou can find the detailed instructions to do so here: rt ghttps://pureinfotech.com/install-wsl-windows-11/\nIn case, you run into another issue while trying to install WSL2 (WslRegisterDistribution failed with error: 0x800701bc), Make sure you update the WSL2 Linux Kernel, following the guidelines here: \n\nhttps://github.com/microsoft/WSL/issues/5393",
        "question": "What should you do if you encounter the WslRegisterDistribution error while installing WSL2?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 54,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage).\nIF the repository is public, the fetch and download happens without any issue whatsoever.\nFor instance:\ndocker pull postgres:13\ndocker pull dpage/pgadmin4\nBE ADVISED:\n\nThe Docker Images we'll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).\n\nMeaning: you are NOT required to perform a docker login to fetch them. \n\nSo if you get the message above saying \"docker login': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:\n\nFor instance:\n$ docker pull dbpage/pgadmin4\nWill throw that exception telling you \"repository does not exist or may require 'docker login'\nError response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or \nmay require 'docker login': denied: requested access to the resource is denied\nBut that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4\nHow to fix it:\n$ docker pull dpage/pgadmin4\nEXTRA NOTES:\nIn the real world, occasionally, when you're working for a company or closed organisation, the Docker image you're trying to fetch might be under a private repo that your DockerHub Username was granted access to.\nFor which cases, you must first execute:\n$ docker login\nFill in the details of your username and password.\nAnd only then perform the `docker pull` against that private repository\nWhy am I encountering a \"permission denied\" error when creating a PostgreSQL Docker container for the New York Taxi Database with a mounted volume on macOS M1?\nIssue Description:\nWhen attempting to run a Docker command similar to the one below:\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\mount\npostgres:13\nYou encounter the error message:\ndocker: Error response from daemon: error while creating mount source path '/path/to/ny_taxi_postgres_data': chown /path/to/ny_taxi_postgres_data: permission denied.\nSolution:\n1- Stop Rancher Desktop:\nIf you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\n2- Install Docker Desktop:\nInstall Docker Desktop, ensuring that it is properly configured and has the required permissions.\n2-Retry Docker Command:\nRun the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\nNote: The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.",
        "question": "What happens when a docker pull command is executed?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 55,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage).\nIF the repository is public, the fetch and download happens without any issue whatsoever.\nFor instance:\ndocker pull postgres:13\ndocker pull dpage/pgadmin4\nBE ADVISED:\n\nThe Docker Images we'll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).\n\nMeaning: you are NOT required to perform a docker login to fetch them. \n\nSo if you get the message above saying \"docker login': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:\n\nFor instance:\n$ docker pull dbpage/pgadmin4\nWill throw that exception telling you \"repository does not exist or may require 'docker login'\nError response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or \nmay require 'docker login': denied: requested access to the resource is denied\nBut that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4\nHow to fix it:\n$ docker pull dpage/pgadmin4\nEXTRA NOTES:\nIn the real world, occasionally, when you're working for a company or closed organisation, the Docker image you're trying to fetch might be under a private repo that your DockerHub Username was granted access to.\nFor which cases, you must first execute:\n$ docker login\nFill in the details of your username and password.\nAnd only then perform the `docker pull` against that private repository\nWhy am I encountering a \"permission denied\" error when creating a PostgreSQL Docker container for the New York Taxi Database with a mounted volume on macOS M1?\nIssue Description:\nWhen attempting to run a Docker command similar to the one below:\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\mount\npostgres:13\nYou encounter the error message:\ndocker: Error response from daemon: error while creating mount source path '/path/to/ny_taxi_postgres_data': chown /path/to/ny_taxi_postgres_data: permission denied.\nSolution:\n1- Stop Rancher Desktop:\nIf you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\n2- Install Docker Desktop:\nInstall Docker Desktop, ensuring that it is properly configured and has the required permissions.\n2-Retry Docker Command:\nRun the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\nNote: The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.",
        "question": "What should you do if you receive a 'permission denied' error while creating a PostgreSQL Docker container?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 55,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage).\nIF the repository is public, the fetch and download happens without any issue whatsoever.\nFor instance:\ndocker pull postgres:13\ndocker pull dpage/pgadmin4\nBE ADVISED:\n\nThe Docker Images we'll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).\n\nMeaning: you are NOT required to perform a docker login to fetch them. \n\nSo if you get the message above saying \"docker login': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:\n\nFor instance:\n$ docker pull dbpage/pgadmin4\nWill throw that exception telling you \"repository does not exist or may require 'docker login'\nError response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or \nmay require 'docker login': denied: requested access to the resource is denied\nBut that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4\nHow to fix it:\n$ docker pull dpage/pgadmin4\nEXTRA NOTES:\nIn the real world, occasionally, when you're working for a company or closed organisation, the Docker image you're trying to fetch might be under a private repo that your DockerHub Username was granted access to.\nFor which cases, you must first execute:\n$ docker login\nFill in the details of your username and password.\nAnd only then perform the `docker pull` against that private repository\nWhy am I encountering a \"permission denied\" error when creating a PostgreSQL Docker container for the New York Taxi Database with a mounted volume on macOS M1?\nIssue Description:\nWhen attempting to run a Docker command similar to the one below:\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\mount\npostgres:13\nYou encounter the error message:\ndocker: Error response from daemon: error while creating mount source path '/path/to/ny_taxi_postgres_data': chown /path/to/ny_taxi_postgres_data: permission denied.\nSolution:\n1- Stop Rancher Desktop:\nIf you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\n2- Install Docker Desktop:\nInstall Docker Desktop, ensuring that it is properly configured and has the required permissions.\n2-Retry Docker Command:\nRun the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\nNote: The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.",
        "question": "How can you fix a typo in the image name when performing a docker pull?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 55,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage).\nIF the repository is public, the fetch and download happens without any issue whatsoever.\nFor instance:\ndocker pull postgres:13\ndocker pull dpage/pgadmin4\nBE ADVISED:\n\nThe Docker Images we'll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).\n\nMeaning: you are NOT required to perform a docker login to fetch them. \n\nSo if you get the message above saying \"docker login': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:\n\nFor instance:\n$ docker pull dbpage/pgadmin4\nWill throw that exception telling you \"repository does not exist or may require 'docker login'\nError response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or \nmay require 'docker login': denied: requested access to the resource is denied\nBut that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4\nHow to fix it:\n$ docker pull dpage/pgadmin4\nEXTRA NOTES:\nIn the real world, occasionally, when you're working for a company or closed organisation, the Docker image you're trying to fetch might be under a private repo that your DockerHub Username was granted access to.\nFor which cases, you must first execute:\n$ docker login\nFill in the details of your username and password.\nAnd only then perform the `docker pull` against that private repository\nWhy am I encountering a \"permission denied\" error when creating a PostgreSQL Docker container for the New York Taxi Database with a mounted volume on macOS M1?\nIssue Description:\nWhen attempting to run a Docker command similar to the one below:\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\mount\npostgres:13\nYou encounter the error message:\ndocker: Error response from daemon: error while creating mount source path '/path/to/ny_taxi_postgres_data': chown /path/to/ny_taxi_postgres_data: permission denied.\nSolution:\n1- Stop Rancher Desktop:\nIf you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\n2- Install Docker Desktop:\nInstall Docker Desktop, ensuring that it is properly configured and has the required permissions.\n2-Retry Docker Command:\nRun the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\nNote: The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.",
        "question": "What is the significance of a Docker image being public in the context of the Data Engineering Zoomcamp?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 55,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage).\nIF the repository is public, the fetch and download happens without any issue whatsoever.\nFor instance:\ndocker pull postgres:13\ndocker pull dpage/pgadmin4\nBE ADVISED:\n\nThe Docker Images we'll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).\n\nMeaning: you are NOT required to perform a docker login to fetch them. \n\nSo if you get the message above saying \"docker login': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:\n\nFor instance:\n$ docker pull dbpage/pgadmin4\nWill throw that exception telling you \"repository does not exist or may require 'docker login'\nError response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or \nmay require 'docker login': denied: requested access to the resource is denied\nBut that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4\nHow to fix it:\n$ docker pull dpage/pgadmin4\nEXTRA NOTES:\nIn the real world, occasionally, when you're working for a company or closed organisation, the Docker image you're trying to fetch might be under a private repo that your DockerHub Username was granted access to.\nFor which cases, you must first execute:\n$ docker login\nFill in the details of your username and password.\nAnd only then perform the `docker pull` against that private repository\nWhy am I encountering a \"permission denied\" error when creating a PostgreSQL Docker container for the New York Taxi Database with a mounted volume on macOS M1?\nIssue Description:\nWhen attempting to run a Docker command similar to the one below:\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\mount\npostgres:13\nYou encounter the error message:\ndocker: Error response from daemon: error while creating mount source path '/path/to/ny_taxi_postgres_data': chown /path/to/ny_taxi_postgres_data: permission denied.\nSolution:\n1- Stop Rancher Desktop:\nIf you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\n2- Install Docker Desktop:\nInstall Docker Desktop, ensuring that it is properly configured and has the required permissions.\n2-Retry Docker Command:\nRun the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\nNote: The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.",
        "question": "Why is it recommended to stop Rancher Desktop before running Docker commands related to the New York Taxi Database?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 55,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When I runned command to create postgre in docker container it created folder on my local machine to mount it to volume inside container. It has write and read protection and owned by user 999, so I could not delete it by simply drag to trash.  My obsidian could not started due to access error, so I had to change placement of this folder and delete old folder by this command:\nsudo rm -r -f docker_test/\n- where `rm` - remove, `-r` - recursively, `-f` - force, `docker_test/` - folder.",
        "question": "What command was run to create PostgreSQL in the Docker container?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 56,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When I runned command to create postgre in docker container it created folder on my local machine to mount it to volume inside container. It has write and read protection and owned by user 999, so I could not delete it by simply drag to trash.  My obsidian could not started due to access error, so I had to change placement of this folder and delete old folder by this command:\nsudo rm -r -f docker_test/\n- where `rm` - remove, `-r` - recursively, `-f` - force, `docker_test/` - folder.",
        "question": "What is the ownership of the folder created on the local machine?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 56,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When I runned command to create postgre in docker container it created folder on my local machine to mount it to volume inside container. It has write and read protection and owned by user 999, so I could not delete it by simply drag to trash.  My obsidian could not started due to access error, so I had to change placement of this folder and delete old folder by this command:\nsudo rm -r -f docker_test/\n- where `rm` - remove, `-r` - recursively, `-f` - force, `docker_test/` - folder.",
        "question": "Why could the user not delete the folder by dragging it to the trash?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 56,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When I runned command to create postgre in docker container it created folder on my local machine to mount it to volume inside container. It has write and read protection and owned by user 999, so I could not delete it by simply drag to trash.  My obsidian could not started due to access error, so I had to change placement of this folder and delete old folder by this command:\nsudo rm -r -f docker_test/\n- where `rm` - remove, `-r` - recursively, `-f` - force, `docker_test/` - folder.",
        "question": "What access issue did the user face with Obsidian?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 56,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When I runned command to create postgre in docker container it created folder on my local machine to mount it to volume inside container. It has write and read protection and owned by user 999, so I could not delete it by simply drag to trash.  My obsidian could not started due to access error, so I had to change placement of this folder and delete old folder by this command:\nsudo rm -r -f docker_test/\n- where `rm` - remove, `-r` - recursively, `-f` - force, `docker_test/` - folder.",
        "question": "What command was used to delete the old folder, and what do the options in the command mean?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 56,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "First off, make sure you're running the latest version of Docker for Windows, which you can download from here. Sometimes using the menu to \"Upgrade\" doesn't work (which is another clear indicator for you to uninstall, and reinstall with the latest version)\nIf docker is stuck on starting, first try to switch containers by right clicking the docker symbol from the running programs and switch the containers from windows to linux or vice versa\n[Windows 10 / 11 Pro Edition] The Pro Edition of Windows can run Docker either by using Hyper-V or WSL2 as its backend (Docker Engine)\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\nIf you opt-in for WSL2, you can follow the same steps as detailed in the tutorial here",
        "question": "What should you do if Docker for Windows is stuck on starting?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 57,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "First off, make sure you're running the latest version of Docker for Windows, which you can download from here. Sometimes using the menu to \"Upgrade\" doesn't work (which is another clear indicator for you to uninstall, and reinstall with the latest version)\nIf docker is stuck on starting, first try to switch containers by right clicking the docker symbol from the running programs and switch the containers from windows to linux or vice versa\n[Windows 10 / 11 Pro Edition] The Pro Edition of Windows can run Docker either by using Hyper-V or WSL2 as its backend (Docker Engine)\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\nIf you opt-in for WSL2, you can follow the same steps as detailed in the tutorial here",
        "question": "How can you switch between containers in Docker on Windows?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 57,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "First off, make sure you're running the latest version of Docker for Windows, which you can download from here. Sometimes using the menu to \"Upgrade\" doesn't work (which is another clear indicator for you to uninstall, and reinstall with the latest version)\nIf docker is stuck on starting, first try to switch containers by right clicking the docker symbol from the running programs and switch the containers from windows to linux or vice versa\n[Windows 10 / 11 Pro Edition] The Pro Edition of Windows can run Docker either by using Hyper-V or WSL2 as its backend (Docker Engine)\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\nIf you opt-in for WSL2, you can follow the same steps as detailed in the tutorial here",
        "question": "What are the two back-end options available for running Docker on Windows 10/11 Pro Edition?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 57,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "First off, make sure you're running the latest version of Docker for Windows, which you can download from here. Sometimes using the menu to \"Upgrade\" doesn't work (which is another clear indicator for you to uninstall, and reinstall with the latest version)\nIf docker is stuck on starting, first try to switch containers by right clicking the docker symbol from the running programs and switch the containers from windows to linux or vice versa\n[Windows 10 / 11 Pro Edition] The Pro Edition of Windows can run Docker either by using Hyper-V or WSL2 as its backend (Docker Engine)\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\nIf you opt-in for WSL2, you can follow the same steps as detailed in the tutorial here",
        "question": "What is the prerequisite for using Hyper-V as a backend for Docker on Windows?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 57,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "First off, make sure you're running the latest version of Docker for Windows, which you can download from here. Sometimes using the menu to \"Upgrade\" doesn't work (which is another clear indicator for you to uninstall, and reinstall with the latest version)\nIf docker is stuck on starting, first try to switch containers by right clicking the docker symbol from the running programs and switch the containers from windows to linux or vice versa\n[Windows 10 / 11 Pro Edition] The Pro Edition of Windows can run Docker either by using Hyper-V or WSL2 as its backend (Docker Engine)\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\nIf you opt-in for WSL2, you can follow the same steps as detailed in the tutorial here",
        "question": "Where can you find the tutorial to enable Hyper-V on Windows 10/11?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 57,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is recommended by the Docker do\n[Windows 10 / 11 Home Edition] If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the tutorial here\nIf even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the option to Reset to Factory Defaults or do a fresh install.",
        "question": "What is recommended for users running Docker on Windows 10 or 11 Home Edition?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 58,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is recommended by the Docker do\n[Windows 10 / 11 Home Edition] If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the tutorial here\nIf even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the option to Reset to Factory Defaults or do a fresh install.",
        "question": "How can users make Docker work on Windows Home Edition?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 58,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is recommended by the Docker do\n[Windows 10 / 11 Home Edition] If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the tutorial here\nIf even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the option to Reset to Factory Defaults or do a fresh install.",
        "question": "What should you do if Docker remains stuck despite proper WSL2 setup?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 58,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is recommended by the Docker do\n[Windows 10 / 11 Home Edition] If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the tutorial here\nIf even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the option to Reset to Factory Defaults or do a fresh install.",
        "question": "What options are available if Docker is not functioning correctly on Windows?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 58,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is recommended by the Docker do\n[Windows 10 / 11 Home Edition] If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the tutorial here\nIf even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the option to Reset to Factory Defaults or do a fresh install.",
        "question": "What can you do to troubleshoot Docker installation issues on Windows?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 58,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "More info in the Docker Docs on Best Practises",
        "question": "What can you find in the Docker Docs?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 59,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "More info in the Docker Docs on Best Practises",
        "question": "What are the Best Practices mentioned in the Docker Docs?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 59,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "More info in the Docker Docs on Best Practises",
        "question": "Where can I access information about Docker Best Practices?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 59,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "More info in the Docker Docs on Best Practises",
        "question": "Why are Docker Best Practices important?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 59,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "More info in the Docker Docs on Best Practises",
        "question": "How can following Docker Best Practices benefit my projects?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 59,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You may have this error:\n$ docker run -it ubuntu bash\nthe input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty'\nerror:\nSolution:\nUse winpty before docker command (source)\n$ winpty docker run -it ubuntu bash\nYou also can make an alias:\necho \"alias docker='winpty docker'\" >> ~/.bashrc\nOR\necho \"alias docker='winpty docker'\" >> ~/.bash_profile",
        "question": "What error message might you encounter when running the Docker command with Ubuntu?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 60,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You may have this error:\n$ docker run -it ubuntu bash\nthe input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty'\nerror:\nSolution:\nUse winpty before docker command (source)\n$ winpty docker run -it ubuntu bash\nYou also can make an alias:\necho \"alias docker='winpty docker'\" >> ~/.bashrc\nOR\necho \"alias docker='winpty docker'\" >> ~/.bash_profile",
        "question": "What solution is suggested if you receive the input device is not a TTY error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 60,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You may have this error:\n$ docker run -it ubuntu bash\nthe input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty'\nerror:\nSolution:\nUse winpty before docker command (source)\n$ winpty docker run -it ubuntu bash\nYou also can make an alias:\necho \"alias docker='winpty docker'\" >> ~/.bashrc\nOR\necho \"alias docker='winpty docker'\" >> ~/.bash_profile",
        "question": "What command should you use to prefix the Docker command to fix the TTY error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 60,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You may have this error:\n$ docker run -it ubuntu bash\nthe input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty'\nerror:\nSolution:\nUse winpty before docker command (source)\n$ winpty docker run -it ubuntu bash\nYou also can make an alias:\necho \"alias docker='winpty docker'\" >> ~/.bashrc\nOR\necho \"alias docker='winpty docker'\" >> ~/.bash_profile",
        "question": "How can you create an alias for the Docker command using winpty?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 60,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You may have this error:\n$ docker run -it ubuntu bash\nthe input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty'\nerror:\nSolution:\nUse winpty before docker command (source)\n$ winpty docker run -it ubuntu bash\nYou also can make an alias:\necho \"alias docker='winpty docker'\" >> ~/.bashrc\nOR\necho \"alias docker='winpty docker'\" >> ~/.bash_profile",
        "question": "Which file do you need to edit to create a permanent alias for the Docker command?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 60,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You may have this error:\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\nrllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\n/simple/pandas/\nPossible solution might be:\n$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9",
        "question": "What error might occur when trying to establish a connection?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 61,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You may have this error:\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\nrllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\n/simple/pandas/\nPossible solution might be:\n$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9",
        "question": "What does the error message indicate about the connection attempt?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 61,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You may have this error:\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\nrllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\n/simple/pandas/\nPossible solution might be:\n$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9",
        "question": "What is one potential solution provided for the connection issue?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 61,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You may have this error:\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\nrllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\n/simple/pandas/\nPossible solution might be:\n$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9",
        "question": "Which DNS server is suggested as part of the solution?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 61,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You may have this error:\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\nrllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\n/simple/pandas/\nPossible solution might be:\n$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9",
        "question": "What command is recommended to run in Docker to resolve the connection problem?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 61,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Even after properly running the docker script the folder is empty in the vs code  then try this (For Windows)\nwinpty docker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v \"C:\\Users\\abhin\\dataengg\\DE_Project_git_connected\\DE_OLD\\week1_set_up\\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\" \\\n-p 5432:5432 \\\npostgres:13\nHere quoting the absolute path in  the -v parameter is solving the issue and all the files are visible in the Vs-code ny_taxi folder as shown in the video",
        "question": "What issue is encountered when running the docker script in VS Code?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 62,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Even after properly running the docker script the folder is empty in the vs code  then try this (For Windows)\nwinpty docker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v \"C:\\Users\\abhin\\dataengg\\DE_Project_git_connected\\DE_OLD\\week1_set_up\\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\" \\\n-p 5432:5432 \\\npostgres:13\nHere quoting the absolute path in  the -v parameter is solving the issue and all the files are visible in the Vs-code ny_taxi folder as shown in the video",
        "question": "What environment variables are set in the docker run command for the PostgreSQL configuration?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 62,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Even after properly running the docker script the folder is empty in the vs code  then try this (For Windows)\nwinpty docker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v \"C:\\Users\\abhin\\dataengg\\DE_Project_git_connected\\DE_OLD\\week1_set_up\\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\" \\\n-p 5432:5432 \\\npostgres:13\nHere quoting the absolute path in  the -v parameter is solving the issue and all the files are visible in the Vs-code ny_taxi folder as shown in the video",
        "question": "What is the purpose of the -v parameter in the docker run command?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 62,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Even after properly running the docker script the folder is empty in the vs code  then try this (For Windows)\nwinpty docker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v \"C:\\Users\\abhin\\dataengg\\DE_Project_git_connected\\DE_OLD\\week1_set_up\\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\" \\\n-p 5432:5432 \\\npostgres:13\nHere quoting the absolute path in  the -v parameter is solving the issue and all the files are visible in the Vs-code ny_taxi folder as shown in the video",
        "question": "Which version of PostgreSQL is being used in this docker command?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 62,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Even after properly running the docker script the folder is empty in the vs code  then try this (For Windows)\nwinpty docker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v \"C:\\Users\\abhin\\dataengg\\DE_Project_git_connected\\DE_OLD\\week1_set_up\\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\" \\\n-p 5432:5432 \\\npostgres:13\nHere quoting the absolute path in  the -v parameter is solving the issue and all the files are visible in the Vs-code ny_taxi folder as shown in the video",
        "question": "How does quoting the absolute path in the -v parameter affect the visibility of files in VS Code?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 62,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check this article for details - Setting up docker in macOS\nFrom researching it seems this method might be out of date, it seems that since docker changed their licensing model, the above is a bit hit and miss. What worked for me was to just go to the docker website and download their dmg. Haven’t had an issue with that method.",
        "question": "What method is suggested for setting up Docker in macOS?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 63,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check this article for details - Setting up docker in macOS\nFrom researching it seems this method might be out of date, it seems that since docker changed their licensing model, the above is a bit hit and miss. What worked for me was to just go to the docker website and download their dmg. Haven’t had an issue with that method.",
        "question": "What potential issues are mentioned regarding the previous setup method?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 63,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check this article for details - Setting up docker in macOS\nFrom researching it seems this method might be out of date, it seems that since docker changed their licensing model, the above is a bit hit and miss. What worked for me was to just go to the docker website and download their dmg. Haven’t had an issue with that method.",
        "question": "Where can one find the most reliable method to install Docker?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 63,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check this article for details - Setting up docker in macOS\nFrom researching it seems this method might be out of date, it seems that since docker changed their licensing model, the above is a bit hit and miss. What worked for me was to just go to the docker website and download their dmg. Haven’t had an issue with that method.",
        "question": "What change in Docker's licensing model is referenced?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 63,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check this article for details - Setting up docker in macOS\nFrom researching it seems this method might be out of date, it seems that since docker changed their licensing model, the above is a bit hit and miss. What worked for me was to just go to the docker website and download their dmg. Haven’t had an issue with that method.",
        "question": "What was the author's personal experience with downloading Docker from the website?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 63,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change the mounting path. Replace it with one of following:\n-v /e/zoomcamp/...:/var/lib/postgresql/data\n-v /c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\\ (leading slash in front of c:)",
        "question": "What is the purpose of changing the mounting path?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 66,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change the mounting path. Replace it with one of following:\n-v /e/zoomcamp/...:/var/lib/postgresql/data\n-v /c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\\ (leading slash in front of c:)",
        "question": "What are the two suggested mounting paths for Postgres data?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 66,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change the mounting path. Replace it with one of following:\n-v /e/zoomcamp/...:/var/lib/postgresql/data\n-v /c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\\ (leading slash in front of c:)",
        "question": "Why is there a leading slash in front of the 'c:' path?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 66,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change the mounting path. Replace it with one of following:\n-v /e/zoomcamp/...:/var/lib/postgresql/data\n-v /c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\\ (leading slash in front of c:)",
        "question": "How does the mounting path affect the Postgres data storage?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 66,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change the mounting path. Replace it with one of following:\n-v /e/zoomcamp/...:/var/lib/postgresql/data\n-v /c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\\ (leading slash in front of c:)",
        "question": "Can the mounting path be customized beyond the provided examples?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 66,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you run this command second time\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v <your path>:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\nThe error message above could happen. That means you should not mount on the second run. This command helped me:\nWhen you run this command second time\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-p 5432:5432 \\\npostgres:13",
        "question": "What does the command 'docker run' do in the provided example?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 67,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you run this command second time\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v <your path>:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\nThe error message above could happen. That means you should not mount on the second run. This command helped me:\nWhen you run this command second time\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-p 5432:5432 \\\npostgres:13",
        "question": "What environment variables are specified in the Docker command?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 67,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you run this command second time\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v <your path>:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\nThe error message above could happen. That means you should not mount on the second run. This command helped me:\nWhen you run this command second time\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-p 5432:5432 \\\npostgres:13",
        "question": "What error might occur when running the command a second time?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 67,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you run this command second time\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v <your path>:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\nThe error message above could happen. That means you should not mount on the second run. This command helped me:\nWhen you run this command second time\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-p 5432:5432 \\\npostgres:13",
        "question": "Why should you avoid mounting a volume on the second run of the command?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 67,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you run this command second time\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v <your path>:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\nThe error message above could happen. That means you should not mount on the second run. This command helped me:\nWhen you run this command second time\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-p 5432:5432 \\\npostgres:13",
        "question": "What is the purpose of the '-p 5432:5432' option in the Docker command?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 67,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error appeared when running the command: docker build -t taxi_ingest:v001 .\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\nsudo chown -R $USER dir_path\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \n\n\t\t\t\t\t\t\t\t\t\t\tAdded by\n\t\t\t\t\t\t\t\t\t\t\tKenan Arslanbay",
        "question": "What command was used that resulted in an error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 68,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error appeared when running the command: docker build -t taxi_ingest:v001 .\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\nsudo chown -R $USER dir_path\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \n\n\t\t\t\t\t\t\t\t\t\t\tAdded by\n\t\t\t\t\t\t\t\t\t\t\tKenan Arslanbay",
        "question": "What directory had its user id changed, causing a permission issue?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 68,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error appeared when running the command: docker build -t taxi_ingest:v001 .\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\nsudo chown -R $USER dir_path\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \n\n\t\t\t\t\t\t\t\t\t\t\tAdded by\n\t\t\t\t\t\t\t\t\t\t\tKenan Arslanbay",
        "question": "What two files are necessary to fix the error when running the docker build command?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 68,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error appeared when running the command: docker build -t taxi_ingest:v001 .\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\nsudo chown -R $USER dir_path\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \n\n\t\t\t\t\t\t\t\t\t\t\tAdded by\n\t\t\t\t\t\t\t\t\t\t\tKenan Arslanbay",
        "question": "What command can be used to change the ownership of a directory on Ubuntu?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 68,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error appeared when running the command: docker build -t taxi_ingest:v001 .\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\nsudo chown -R $USER dir_path\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \n\n\t\t\t\t\t\t\t\t\t\t\tAdded by\n\t\t\t\t\t\t\t\t\t\t\tKenan Arslanbay",
        "question": "Where can one find a more complete explanation of the error encountered?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 68,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You might have installed docker via snap. Run “sudo snap status docker” to verify.\nIf you have “error: unknown command \"status\", see 'snap help'.” as a response than deinstall docker and install via the official website\nBind for 0.0.0.0:5432 failed: port is a",
        "question": "What command should you run to verify the installation of docker via snap?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 69,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You might have installed docker via snap. Run “sudo snap status docker” to verify.\nIf you have “error: unknown command \"status\", see 'snap help'.” as a response than deinstall docker and install via the official website\nBind for 0.0.0.0:5432 failed: port is a",
        "question": "What should you do if you receive an error about an unknown command when checking the status of docker?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 69,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You might have installed docker via snap. Run “sudo snap status docker” to verify.\nIf you have “error: unknown command \"status\", see 'snap help'.” as a response than deinstall docker and install via the official website\nBind for 0.0.0.0:5432 failed: port is a",
        "question": "Where should you go to install docker if it needs to be reinstalled?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 69,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You might have installed docker via snap. Run “sudo snap status docker” to verify.\nIf you have “error: unknown command \"status\", see 'snap help'.” as a response than deinstall docker and install via the official website\nBind for 0.0.0.0:5432 failed: port is a",
        "question": "What does the error 'Bind for 0.0.0.0:5432 failed: port is a' indicate?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 69,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You might have installed docker via snap. Run “sudo snap status docker” to verify.\nIf you have “error: unknown command \"status\", see 'snap help'.” as a response than deinstall docker and install via the official website\nBind for 0.0.0.0:5432 failed: port is a",
        "question": "Is there a specific port mentioned in the error message, and if so, what is it?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 69,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Found the issue in the PopOS linux. It happened because our user didn’t have authorization rights to the host folder ( which also caused folder seems empty, but it didn’t!).\n✅Solution:\nJust add permission for everyone to the corresponding folder\nsudo chmod -R 777 <path_to_folder>\nExample:\nsudo chmod -R 777 ny_taxi_postgres_data/",
        "question": "What was the issue found in PopOS Linux?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 70,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Found the issue in the PopOS linux. It happened because our user didn’t have authorization rights to the host folder ( which also caused folder seems empty, but it didn’t!).\n✅Solution:\nJust add permission for everyone to the corresponding folder\nsudo chmod -R 777 <path_to_folder>\nExample:\nsudo chmod -R 777 ny_taxi_postgres_data/",
        "question": "Why did the folder appear empty to the user?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 70,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Found the issue in the PopOS linux. It happened because our user didn’t have authorization rights to the host folder ( which also caused folder seems empty, but it didn’t!).\n✅Solution:\nJust add permission for everyone to the corresponding folder\nsudo chmod -R 777 <path_to_folder>\nExample:\nsudo chmod -R 777 ny_taxi_postgres_data/",
        "question": "What command is used to change permissions for a folder in Linux?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 70,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Found the issue in the PopOS linux. It happened because our user didn’t have authorization rights to the host folder ( which also caused folder seems empty, but it didn’t!).\n✅Solution:\nJust add permission for everyone to the corresponding folder\nsudo chmod -R 777 <path_to_folder>\nExample:\nsudo chmod -R 777 ny_taxi_postgres_data/",
        "question": "What does the command 'sudo chmod -R 777' do?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 70,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Found the issue in the PopOS linux. It happened because our user didn’t have authorization rights to the host folder ( which also caused folder seems empty, but it didn’t!).\n✅Solution:\nJust add permission for everyone to the corresponding folder\nsudo chmod -R 777 <path_to_folder>\nExample:\nsudo chmod -R 777 ny_taxi_postgres_data/",
        "question": "Provide an example of a folder path where permissions were changed.",
        "section": "Module 1: Docker and Terraform",
        "document_id": 70,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again.\n$ docker build -t taxi_ingest:v001 .\nA folder is created to host the Docker files. When the build command is executed again to rebuild the pipeline or create a new one the error is raised as there are no permissions on this new folder. Grant permissions by running this comtionmand;\n$ sudo chmod -R 755 ny_taxi_postgres_data\nOr use 777 if you still see problems. 755 grants write access to only the owner.",
        "question": "What command is used to build the Docker container?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 71,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again.\n$ docker build -t taxi_ingest:v001 .\nA folder is created to host the Docker files. When the build command is executed again to rebuild the pipeline or create a new one the error is raised as there are no permissions on this new folder. Grant permissions by running this comtionmand;\n$ sudo chmod -R 755 ny_taxi_postgres_data\nOr use 777 if you still see problems. 755 grants write access to only the owner.",
        "question": "What issue arises when trying to rebuild the Docker pipeline on Ubuntu/Linux systems?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 71,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again.\n$ docker build -t taxi_ingest:v001 .\nA folder is created to host the Docker files. When the build command is executed again to rebuild the pipeline or create a new one the error is raised as there are no permissions on this new folder. Grant permissions by running this comtionmand;\n$ sudo chmod -R 755 ny_taxi_postgres_data\nOr use 777 if you still see problems. 755 grants write access to only the owner.",
        "question": "What command is suggested to grant permissions on the new folder?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 71,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again.\n$ docker build -t taxi_ingest:v001 .\nA folder is created to host the Docker files. When the build command is executed again to rebuild the pipeline or create a new one the error is raised as there are no permissions on this new folder. Grant permissions by running this comtionmand;\n$ sudo chmod -R 755 ny_taxi_postgres_data\nOr use 777 if you still see problems. 755 grants write access to only the owner.",
        "question": "What does the permission setting '755' allow in terms of access?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 71,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again.\n$ docker build -t taxi_ingest:v001 .\nA folder is created to host the Docker files. When the build command is executed again to rebuild the pipeline or create a new one the error is raised as there are no permissions on this new folder. Grant permissions by running this comtionmand;\n$ sudo chmod -R 755 ny_taxi_postgres_data\nOr use 777 if you still see problems. 755 grants write access to only the owner.",
        "question": "When might you consider using permission '777' instead of '755'?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 71,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Get the network name via: $ docker network ls.",
        "question": "What command is used to list Docker networks?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 72,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Get the network name via: $ docker network ls.",
        "question": "How can you retrieve the network name in Docker?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 72,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Get the network name via: $ docker network ls.",
        "question": "What is the purpose of the command $ docker network ls?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 72,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Get the network name via: $ docker network ls.",
        "question": "What does the command $ docker network ls output?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 72,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Get the network name via: $ docker network ls.",
        "question": "Can you explain the steps to access Docker network information?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 72,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Sometimes, when you try to restart a docker image configured with a network name, the above message appears. In this case, use the following command with the appropriate container name:\n>>> If the container is running state, use docker stop <container_name>\n>>> then, docker rm pg-database\nOr use docker start instead of docker run in order to restart the docker image without removing it.",
        "question": "What message might appear when trying to restart a docker image configured with a network name?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 73,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Sometimes, when you try to restart a docker image configured with a network name, the above message appears. In this case, use the following command with the appropriate container name:\n>>> If the container is running state, use docker stop <container_name>\n>>> then, docker rm pg-database\nOr use docker start instead of docker run in order to restart the docker image without removing it.",
        "question": "What command should be used to stop a running Docker container?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 73,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Sometimes, when you try to restart a docker image configured with a network name, the above message appears. In this case, use the following command with the appropriate container name:\n>>> If the container is running state, use docker stop <container_name>\n>>> then, docker rm pg-database\nOr use docker start instead of docker run in order to restart the docker image without removing it.",
        "question": "How do you remove a Docker container after stopping it?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 73,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Sometimes, when you try to restart a docker image configured with a network name, the above message appears. In this case, use the following command with the appropriate container name:\n>>> If the container is running state, use docker stop <container_name>\n>>> then, docker rm pg-database\nOr use docker start instead of docker run in order to restart the docker image without removing it.",
        "question": "What is an alternative command to restarting a Docker image without removing it?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 73,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Sometimes, when you try to restart a docker image configured with a network name, the above message appears. In this case, use the following command with the appropriate container name:\n>>> If the container is running state, use docker stop <container_name>\n>>> then, docker rm pg-database\nOr use docker start instead of docker run in order to restart the docker image without removing it.",
        "question": "What is the correct syntax to stop a container using docker commands?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 73,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Typical error: sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"pgdatabase\" to address: Name or service not known\nWhen running docker-compose up -d see which network is created and use this for the ingestions script instead of pg-network and see the name of the database to use instead of pgdatabase\nE.g.:\npg-network becomes 2docker_default\nPgdatabase becomes 2docker-pgdatabase-1",
        "question": "What error is indicated in the text relating to sqlalchemy?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 74,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Typical error: sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"pgdatabase\" to address: Name or service not known\nWhen running docker-compose up -d see which network is created and use this for the ingestions script instead of pg-network and see the name of the database to use instead of pgdatabase\nE.g.:\npg-network becomes 2docker_default\nPgdatabase becomes 2docker-pgdatabase-1",
        "question": "What action should be taken when encountering the OperationalError with the host name?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 74,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Typical error: sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"pgdatabase\" to address: Name or service not known\nWhen running docker-compose up -d see which network is created and use this for the ingestions script instead of pg-network and see the name of the database to use instead of pgdatabase\nE.g.:\npg-network becomes 2docker_default\nPgdatabase becomes 2docker-pgdatabase-1",
        "question": "What network name should be used instead of pg-network?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 74,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Typical error: sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"pgdatabase\" to address: Name or service not known\nWhen running docker-compose up -d see which network is created and use this for the ingestions script instead of pg-network and see the name of the database to use instead of pgdatabase\nE.g.:\npg-network becomes 2docker_default\nPgdatabase becomes 2docker-pgdatabase-1",
        "question": "How is the name of the database supposed to change from pgdatabase?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 74,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Typical error: sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"pgdatabase\" to address: Name or service not known\nWhen running docker-compose up -d see which network is created and use this for the ingestions script instead of pg-network and see the name of the database to use instead of pgdatabase\nE.g.:\npg-network becomes 2docker_default\nPgdatabase becomes 2docker-pgdatabase-1",
        "question": "What command is suggested to run before troubleshooting the database connection?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 74,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "terraformRun this command before starting your VM:\nOn Intel CPU:\nmodprobe -r kvm_intel\nmodprobe kvm_intel nested=1\nOn AMD CPU:\nmodprobe -r kvm_amd\nmodprobe kvm_amd nested=1",
        "question": "What command should be run before starting a VM?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 75,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "terraformRun this command before starting your VM:\nOn Intel CPU:\nmodprobe -r kvm_intel\nmodprobe kvm_intel nested=1\nOn AMD CPU:\nmodprobe -r kvm_amd\nmodprobe kvm_amd nested=1",
        "question": "What is the command to configure Intel CPUs for nested virtualization?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 75,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "terraformRun this command before starting your VM:\nOn Intel CPU:\nmodprobe -r kvm_intel\nmodprobe kvm_intel nested=1\nOn AMD CPU:\nmodprobe -r kvm_amd\nmodprobe kvm_amd nested=1",
        "question": "How do you prepare AMD CPUs for nested virtualization?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 75,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "terraformRun this command before starting your VM:\nOn Intel CPU:\nmodprobe -r kvm_intel\nmodprobe kvm_intel nested=1\nOn AMD CPU:\nmodprobe -r kvm_amd\nmodprobe kvm_amd nested=1",
        "question": "What does the 'modprobe' command do in the context of virtual machines?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 75,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "terraformRun this command before starting your VM:\nOn Intel CPU:\nmodprobe -r kvm_intel\nmodprobe kvm_intel nested=1\nOn AMD CPU:\nmodprobe -r kvm_amd\nmodprobe kvm_amd nested=1",
        "question": "Why is it necessary to set the 'nested' option to 1?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 75,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It’s very easy to manage your docker container, images, network and compose projects from VS Code.\nJust install the official extension and launch it from the left side icon.\nIt will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux.\nDocker - How to stop a container?\nUse the following command:\n$ docker stop <container_id>",
        "question": "What is required to manage Docker from VS Code?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 76,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It’s very easy to manage your docker container, images, network and compose projects from VS Code.\nJust install the official extension and launch it from the left side icon.\nIt will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux.\nDocker - How to stop a container?\nUse the following command:\n$ docker stop <container_id>",
        "question": "How do you launch the Docker extension in VS Code?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 76,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It’s very easy to manage your docker container, images, network and compose projects from VS Code.\nJust install the official extension and launch it from the left side icon.\nIt will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux.\nDocker - How to stop a container?\nUse the following command:\n$ docker stop <container_id>",
        "question": "Can VS Code connect to Docker running on WSL2?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 76,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It’s very easy to manage your docker container, images, network and compose projects from VS Code.\nJust install the official extension and launch it from the left side icon.\nIt will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux.\nDocker - How to stop a container?\nUse the following command:\n$ docker stop <container_id>",
        "question": "What command is used to stop a Docker container?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 76,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It’s very easy to manage your docker container, images, network and compose projects from VS Code.\nJust install the official extension and launch it from the left side icon.\nIt will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux.\nDocker - How to stop a container?\nUse the following command:\n$ docker stop <container_id>",
        "question": "Where is the Docker extension found in VS Code?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 76,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you see this in logs, your container with postgres is not accepting any requests, so if you attempt to connect, you'll get this error:\nconnection failed: server closed the connection unexpectedly\nThis probably means the server terminated abnormally before or while processing the request.\nIn this case, you need to delete the directory with data (the one you map to the container with the -v flag) and restart the container.",
        "question": "What does the error message 'connection failed: server closed the connection unexpectedly' indicate?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 77,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you see this in logs, your container with postgres is not accepting any requests, so if you attempt to connect, you'll get this error:\nconnection failed: server closed the connection unexpectedly\nThis probably means the server terminated abnormally before or while processing the request.\nIn this case, you need to delete the directory with data (the one you map to the container with the -v flag) and restart the container.",
        "question": "What could cause a Postgres container to terminate abnormally?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 77,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you see this in logs, your container with postgres is not accepting any requests, so if you attempt to connect, you'll get this error:\nconnection failed: server closed the connection unexpectedly\nThis probably means the server terminated abnormally before or while processing the request.\nIn this case, you need to delete the directory with data (the one you map to the container with the -v flag) and restart the container.",
        "question": "What should you do if your Postgres container is not accepting requests?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 77,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you see this in logs, your container with postgres is not accepting any requests, so if you attempt to connect, you'll get this error:\nconnection failed: server closed the connection unexpectedly\nThis probably means the server terminated abnormally before or while processing the request.\nIn this case, you need to delete the directory with data (the one you map to the container with the -v flag) and restart the container.",
        "question": "Which flag is used when mapping a directory to a container in Docker?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 77,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you see this in logs, your container with postgres is not accepting any requests, so if you attempt to connect, you'll get this error:\nconnection failed: server closed the connection unexpectedly\nThis probably means the server terminated abnormally before or while processing the request.\nIn this case, you need to delete the directory with data (the one you map to the container with the -v flag) and restart the container.",
        "question": "What is the first step you should take to resolve the issue with the Postgres container?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 77,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "On few versions of Ubuntu, snap command can be used to install Docker.\nsudo snap install docker",
        "question": "What command is used to install Docker on certain versions of Ubuntu?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 78,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "On few versions of Ubuntu, snap command can be used to install Docker.\nsudo snap install docker",
        "question": "Which package manager allows the installation of Docker on some Ubuntu versions?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 78,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "On few versions of Ubuntu, snap command can be used to install Docker.\nsudo snap install docker",
        "question": "Can Docker be installed using snap on Ubuntu?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 78,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "On few versions of Ubuntu, snap command can be used to install Docker.\nsudo snap install docker",
        "question": "What is the sudo command used for when installing Docker with snap?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 78,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "On few versions of Ubuntu, snap command can be used to install Docker.\nsudo snap install docker",
        "question": "Is Docker installation via snap available on all versions of Ubuntu?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 78,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "error: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\nif you have used the prev answer (just before this) and have created a local docker volume, then you need to tell the compose file about the named volume:\nvolumes:\ndtc_postgres_volume_local:  # Define the named volume here\n# services mentioned in the compose file auto become part of the same network!\nservices:\nyour remaining code here . . .\nnow use docker volume inspect dtc_postgres_volume_local to see the location by checking the value of Mountpoint\nIn my case, after i ran docker compose up the mounting dir created was named ‘docker_sql_dtc_postgres_volume_local’ whereas it should have used the already existing ‘dtc_postgres_volume_local’\nAll i did to fix this is that I renamed the existing ‘dtc_postgres_volume_local’ to ‘docker_sql_dtc_postgres_volume_local’ and removed the newly created one (just be careful when doing this)\nrun docker compose up again and check if the table is there or not!",
        "question": "What error message is encountered when trying to change permissions of the directory '/var/lib/postgresql/data'?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 79,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "error: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\nif you have used the prev answer (just before this) and have created a local docker volume, then you need to tell the compose file about the named volume:\nvolumes:\ndtc_postgres_volume_local:  # Define the named volume here\n# services mentioned in the compose file auto become part of the same network!\nservices:\nyour remaining code here . . .\nnow use docker volume inspect dtc_postgres_volume_local to see the location by checking the value of Mountpoint\nIn my case, after i ran docker compose up the mounting dir created was named ‘docker_sql_dtc_postgres_volume_local’ whereas it should have used the already existing ‘dtc_postgres_volume_local’\nAll i did to fix this is that I renamed the existing ‘dtc_postgres_volume_local’ to ‘docker_sql_dtc_postgres_volume_local’ and removed the newly created one (just be careful when doing this)\nrun docker compose up again and check if the table is there or not!",
        "question": "What step is required to inform the compose file about the named volume created in Docker?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 79,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "error: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\nif you have used the prev answer (just before this) and have created a local docker volume, then you need to tell the compose file about the named volume:\nvolumes:\ndtc_postgres_volume_local:  # Define the named volume here\n# services mentioned in the compose file auto become part of the same network!\nservices:\nyour remaining code here . . .\nnow use docker volume inspect dtc_postgres_volume_local to see the location by checking the value of Mountpoint\nIn my case, after i ran docker compose up the mounting dir created was named ‘docker_sql_dtc_postgres_volume_local’ whereas it should have used the already existing ‘dtc_postgres_volume_local’\nAll i did to fix this is that I renamed the existing ‘dtc_postgres_volume_local’ to ‘docker_sql_dtc_postgres_volume_local’ and removed the newly created one (just be careful when doing this)\nrun docker compose up again and check if the table is there or not!",
        "question": "What command can be used to inspect the location of the named volume 'dtc_postgres_volume_local'?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 79,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "error: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\nif you have used the prev answer (just before this) and have created a local docker volume, then you need to tell the compose file about the named volume:\nvolumes:\ndtc_postgres_volume_local:  # Define the named volume here\n# services mentioned in the compose file auto become part of the same network!\nservices:\nyour remaining code here . . .\nnow use docker volume inspect dtc_postgres_volume_local to see the location by checking the value of Mountpoint\nIn my case, after i ran docker compose up the mounting dir created was named ‘docker_sql_dtc_postgres_volume_local’ whereas it should have used the already existing ‘dtc_postgres_volume_local’\nAll i did to fix this is that I renamed the existing ‘dtc_postgres_volume_local’ to ‘docker_sql_dtc_postgres_volume_local’ and removed the newly created one (just be careful when doing this)\nrun docker compose up again and check if the table is there or not!",
        "question": "What was the issue encountered after running 'docker compose up' regarding the mounting directory?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 79,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "error: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\nif you have used the prev answer (just before this) and have created a local docker volume, then you need to tell the compose file about the named volume:\nvolumes:\ndtc_postgres_volume_local:  # Define the named volume here\n# services mentioned in the compose file auto become part of the same network!\nservices:\nyour remaining code here . . .\nnow use docker volume inspect dtc_postgres_volume_local to see the location by checking the value of Mountpoint\nIn my case, after i ran docker compose up the mounting dir created was named ‘docker_sql_dtc_postgres_volume_local’ whereas it should have used the already existing ‘dtc_postgres_volume_local’\nAll i did to fix this is that I renamed the existing ‘dtc_postgres_volume_local’ to ‘docker_sql_dtc_postgres_volume_local’ and removed the newly created one (just be careful when doing this)\nrun docker compose up again and check if the table is there or not!",
        "question": "What actions did the author take to resolve the naming conflict with the Docker volume?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 79,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Couldn’t translate host name to address\nMake sure postgres database is running.\n\n​​Use the command to start containers in detached mode: docker-compose up -d\n(data-engineering-zoomcamp) hw % docker compose up -d\n[+] Running 2/2\n⠿ Container pg-admin     Started                                                                                                                                                                      0.6s\n⠿ Container pg-database  Started\nTo view the containers use: docker ps.\n(data-engineering-zoomcamp) hw % docker ps\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\nfaf05090972e   postgres:13      \"docker-entrypoint.s…\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432->5432/tcp          pg-database\n6344dcecd58f   dpage/pgadmin4   \"/entrypoint.sh\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-admin\nhw\nTo view logs for a container: docker logs <containerid>\n(data-engineering-zoomcamp) hw % docker logs faf05090972e\nPostgreSQL Database directory appears to contain a database; Skipping initialization\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in\nprogress\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\n2022-01-25 05:59:33.726 UTC [28\n] LOG:  redo done at 0/98A3C128\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections\nIf docker ps doesn’t show pgdatabase running, run: docker ps -a\nThis should show all containers, either running or stopped.\nGet the container id for pgdatabase-1, and run",
        "question": "What does the error message 'Couldn’t translate host name to address' indicate?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 80,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Couldn’t translate host name to address\nMake sure postgres database is running.\n\n​​Use the command to start containers in detached mode: docker-compose up -d\n(data-engineering-zoomcamp) hw % docker compose up -d\n[+] Running 2/2\n⠿ Container pg-admin     Started                                                                                                                                                                      0.6s\n⠿ Container pg-database  Started\nTo view the containers use: docker ps.\n(data-engineering-zoomcamp) hw % docker ps\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\nfaf05090972e   postgres:13      \"docker-entrypoint.s…\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432->5432/tcp          pg-database\n6344dcecd58f   dpage/pgadmin4   \"/entrypoint.sh\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-admin\nhw\nTo view logs for a container: docker logs <containerid>\n(data-engineering-zoomcamp) hw % docker logs faf05090972e\nPostgreSQL Database directory appears to contain a database; Skipping initialization\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in\nprogress\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\n2022-01-25 05:59:33.726 UTC [28\n] LOG:  redo done at 0/98A3C128\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections\nIf docker ps doesn’t show pgdatabase running, run: docker ps -a\nThis should show all containers, either running or stopped.\nGet the container id for pgdatabase-1, and run",
        "question": "How can you ensure that the PostgreSQL database is running?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 80,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Couldn’t translate host name to address\nMake sure postgres database is running.\n\n​​Use the command to start containers in detached mode: docker-compose up -d\n(data-engineering-zoomcamp) hw % docker compose up -d\n[+] Running 2/2\n⠿ Container pg-admin     Started                                                                                                                                                                      0.6s\n⠿ Container pg-database  Started\nTo view the containers use: docker ps.\n(data-engineering-zoomcamp) hw % docker ps\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\nfaf05090972e   postgres:13      \"docker-entrypoint.s…\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432->5432/tcp          pg-database\n6344dcecd58f   dpage/pgadmin4   \"/entrypoint.sh\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-admin\nhw\nTo view logs for a container: docker logs <containerid>\n(data-engineering-zoomcamp) hw % docker logs faf05090972e\nPostgreSQL Database directory appears to contain a database; Skipping initialization\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in\nprogress\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\n2022-01-25 05:59:33.726 UTC [28\n] LOG:  redo done at 0/98A3C128\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections\nIf docker ps doesn’t show pgdatabase running, run: docker ps -a\nThis should show all containers, either running or stopped.\nGet the container id for pgdatabase-1, and run",
        "question": "What is the command used to start Docker containers in detached mode?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 80,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Couldn’t translate host name to address\nMake sure postgres database is running.\n\n​​Use the command to start containers in detached mode: docker-compose up -d\n(data-engineering-zoomcamp) hw % docker compose up -d\n[+] Running 2/2\n⠿ Container pg-admin     Started                                                                                                                                                                      0.6s\n⠿ Container pg-database  Started\nTo view the containers use: docker ps.\n(data-engineering-zoomcamp) hw % docker ps\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\nfaf05090972e   postgres:13      \"docker-entrypoint.s…\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432->5432/tcp          pg-database\n6344dcecd58f   dpage/pgadmin4   \"/entrypoint.sh\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-admin\nhw\nTo view logs for a container: docker logs <containerid>\n(data-engineering-zoomcamp) hw % docker logs faf05090972e\nPostgreSQL Database directory appears to contain a database; Skipping initialization\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in\nprogress\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\n2022-01-25 05:59:33.726 UTC [28\n] LOG:  redo done at 0/98A3C128\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections\nIf docker ps doesn’t show pgdatabase running, run: docker ps -a\nThis should show all containers, either running or stopped.\nGet the container id for pgdatabase-1, and run",
        "question": "How can you view the logs for a specific Docker container?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 80,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Couldn’t translate host name to address\nMake sure postgres database is running.\n\n​​Use the command to start containers in detached mode: docker-compose up -d\n(data-engineering-zoomcamp) hw % docker compose up -d\n[+] Running 2/2\n⠿ Container pg-admin     Started                                                                                                                                                                      0.6s\n⠿ Container pg-database  Started\nTo view the containers use: docker ps.\n(data-engineering-zoomcamp) hw % docker ps\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\nfaf05090972e   postgres:13      \"docker-entrypoint.s…\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432->5432/tcp          pg-database\n6344dcecd58f   dpage/pgadmin4   \"/entrypoint.sh\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-admin\nhw\nTo view logs for a container: docker logs <containerid>\n(data-engineering-zoomcamp) hw % docker logs faf05090972e\nPostgreSQL Database directory appears to contain a database; Skipping initialization\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in\nprogress\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\n2022-01-25 05:59:33.726 UTC [28\n] LOG:  redo done at 0/98A3C128\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections\nIf docker ps doesn’t show pgdatabase running, run: docker ps -a\nThis should show all containers, either running or stopped.\nGet the container id for pgdatabase-1, and run",
        "question": "What should you do if 'docker ps' does not show the pg-database running?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 80,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After executing `docker-compose up` - if you lose database data and are unable to successfully execute your Ingestion script (to re-populate your database) but receive the following error:\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name /data_pgadmin:/var/lib/pgadmin\"pg-database\" to address: Name or service not known\nDocker compose is creating its own default network since it is no longer specified in a docker execution command or file. Docker Compose will emit to logs the new network name. See the logs after executing `docker compose up` to find the network name and change the network name argument in your Ingestion script.\nIf problems persist with pgcli, we can use HeidiSQL,usql\nKrishna Anand",
        "question": "What error message is received when there is a database connectivity issue after executing 'docker-compose up'?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 81,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After executing `docker-compose up` - if you lose database data and are unable to successfully execute your Ingestion script (to re-populate your database) but receive the following error:\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name /data_pgadmin:/var/lib/pgadmin\"pg-database\" to address: Name or service not known\nDocker compose is creating its own default network since it is no longer specified in a docker execution command or file. Docker Compose will emit to logs the new network name. See the logs after executing `docker compose up` to find the network name and change the network name argument in your Ingestion script.\nIf problems persist with pgcli, we can use HeidiSQL,usql\nKrishna Anand",
        "question": "What does Docker Compose emit to logs after executing 'docker compose up'?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 81,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After executing `docker-compose up` - if you lose database data and are unable to successfully execute your Ingestion script (to re-populate your database) but receive the following error:\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name /data_pgadmin:/var/lib/pgadmin\"pg-database\" to address: Name or service not known\nDocker compose is creating its own default network since it is no longer specified in a docker execution command or file. Docker Compose will emit to logs the new network name. See the logs after executing `docker compose up` to find the network name and change the network name argument in your Ingestion script.\nIf problems persist with pgcli, we can use HeidiSQL,usql\nKrishna Anand",
        "question": "How can one resolve the issue of losing database data when using Docker Compose?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 81,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After executing `docker-compose up` - if you lose database data and are unable to successfully execute your Ingestion script (to re-populate your database) but receive the following error:\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name /data_pgadmin:/var/lib/pgadmin\"pg-database\" to address: Name or service not known\nDocker compose is creating its own default network since it is no longer specified in a docker execution command or file. Docker Compose will emit to logs the new network name. See the logs after executing `docker compose up` to find the network name and change the network name argument in your Ingestion script.\nIf problems persist with pgcli, we can use HeidiSQL,usql\nKrishna Anand",
        "question": "What alternative tools can be used if there are problems with pgcli?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 81,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After executing `docker-compose up` - if you lose database data and are unable to successfully execute your Ingestion script (to re-populate your database) but receive the following error:\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name /data_pgadmin:/var/lib/pgadmin\"pg-database\" to address: Name or service not known\nDocker compose is creating its own default network since it is no longer specified in a docker execution command or file. Docker Compose will emit to logs the new network name. See the logs after executing `docker compose up` to find the network name and change the network name argument in your Ingestion script.\nIf problems persist with pgcli, we can use HeidiSQL,usql\nKrishna Anand",
        "question": "What should be changed in the Ingestion script if a new network name is created by Docker Compose?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 81,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It returns --> Error response from daemon: network 66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 not found\nTry:\ndocker ps -a to see all the stopped & running containers\nd to nuke all the containers\nTry: docker-compose up -d again ports\nOn localhost:8080 server → Unable to connect to server: could not translate host name 'pg-database' to address: Name does not resolve\nTry: new host name, best without “ - ” e.g. pgdatabase\nAnd on docker-compose.yml, should specify docker network & specify the same network in both  containers\nservices:\npgdatabase:\nimage: postgres:13\nenvironment:\n- POSTGRES_USER=root\n- POSTGRES_PASSWORD=root\n- POSTGRES_DB=ny_taxi\nvolumes:\n- \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\nports:\n- \"5431:5432\"\nnetworks:\n- pg-network\npgadmin:\nimage: dpage/pgadmin4\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root\nports:\n- \"8080:80\"\nnetworks:\n- pg-network\nnetworks:\npg-network:\nname: pg-network",
        "question": "What error response does the daemon return when the network is not found?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 82,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It returns --> Error response from daemon: network 66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 not found\nTry:\ndocker ps -a to see all the stopped & running containers\nd to nuke all the containers\nTry: docker-compose up -d again ports\nOn localhost:8080 server → Unable to connect to server: could not translate host name 'pg-database' to address: Name does not resolve\nTry: new host name, best without “ - ” e.g. pgdatabase\nAnd on docker-compose.yml, should specify docker network & specify the same network in both  containers\nservices:\npgdatabase:\nimage: postgres:13\nenvironment:\n- POSTGRES_USER=root\n- POSTGRES_PASSWORD=root\n- POSTGRES_DB=ny_taxi\nvolumes:\n- \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\nports:\n- \"5431:5432\"\nnetworks:\n- pg-network\npgadmin:\nimage: dpage/pgadmin4\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root\nports:\n- \"8080:80\"\nnetworks:\n- pg-network\nnetworks:\npg-network:\nname: pg-network",
        "question": "What command can be used to view all stopped and running Docker containers?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 82,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It returns --> Error response from daemon: network 66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 not found\nTry:\ndocker ps -a to see all the stopped & running containers\nd to nuke all the containers\nTry: docker-compose up -d again ports\nOn localhost:8080 server → Unable to connect to server: could not translate host name 'pg-database' to address: Name does not resolve\nTry: new host name, best without “ - ” e.g. pgdatabase\nAnd on docker-compose.yml, should specify docker network & specify the same network in both  containers\nservices:\npgdatabase:\nimage: postgres:13\nenvironment:\n- POSTGRES_USER=root\n- POSTGRES_PASSWORD=root\n- POSTGRES_DB=ny_taxi\nvolumes:\n- \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\nports:\n- \"5431:5432\"\nnetworks:\n- pg-network\npgadmin:\nimage: dpage/pgadmin4\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root\nports:\n- \"8080:80\"\nnetworks:\n- pg-network\nnetworks:\npg-network:\nname: pg-network",
        "question": "How can you resolve the issue of not being able to connect to the server due to host name resolution?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 82,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It returns --> Error response from daemon: network 66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 not found\nTry:\ndocker ps -a to see all the stopped & running containers\nd to nuke all the containers\nTry: docker-compose up -d again ports\nOn localhost:8080 server → Unable to connect to server: could not translate host name 'pg-database' to address: Name does not resolve\nTry: new host name, best without “ - ” e.g. pgdatabase\nAnd on docker-compose.yml, should specify docker network & specify the same network in both  containers\nservices:\npgdatabase:\nimage: postgres:13\nenvironment:\n- POSTGRES_USER=root\n- POSTGRES_PASSWORD=root\n- POSTGRES_DB=ny_taxi\nvolumes:\n- \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\nports:\n- \"5431:5432\"\nnetworks:\n- pg-network\npgadmin:\nimage: dpage/pgadmin4\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root\nports:\n- \"8080:80\"\nnetworks:\n- pg-network\nnetworks:\npg-network:\nname: pg-network",
        "question": "What network specification is required in the docker-compose.yml file for the pgdatabase and pgadmin services?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 82,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It returns --> Error response from daemon: network 66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 not found\nTry:\ndocker ps -a to see all the stopped & running containers\nd to nuke all the containers\nTry: docker-compose up -d again ports\nOn localhost:8080 server → Unable to connect to server: could not translate host name 'pg-database' to address: Name does not resolve\nTry: new host name, best without “ - ” e.g. pgdatabase\nAnd on docker-compose.yml, should specify docker network & specify the same network in both  containers\nservices:\npgdatabase:\nimage: postgres:13\nenvironment:\n- POSTGRES_USER=root\n- POSTGRES_PASSWORD=root\n- POSTGRES_DB=ny_taxi\nvolumes:\n- \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\nports:\n- \"5431:5432\"\nnetworks:\n- pg-network\npgadmin:\nimage: dpage/pgadmin4\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root\nports:\n- \"8080:80\"\nnetworks:\n- pg-network\nnetworks:\npg-network:\nname: pg-network",
        "question": "What environment variables are set for the pgdatabase service in the docker-compose configuration?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 82,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "So one common issue is when you run docker-compose on GCP, postgres won’t persist it’s data to mentioned path for example:\nservices:\n…\n…\npgadmin:\n…\n…\nVolumes:\n“./pgadmin”:/var/lib/pgadmin:wr”\nMight not work so in this use you can use Docker Volume to make it persist, by simply changing\nservices:\n…\n….\npgadmin:\n…\n…\nVolumes:\npgadmin:/var/lib/pgadmin\nvolumes:\nPgadmin:",
        "question": "What is a common issue when running docker-compose on GCP with PostgreSQL?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 83,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "So one common issue is when you run docker-compose on GCP, postgres won’t persist it’s data to mentioned path for example:\nservices:\n…\n…\npgadmin:\n…\n…\nVolumes:\n“./pgadmin”:/var/lib/pgadmin:wr”\nMight not work so in this use you can use Docker Volume to make it persist, by simply changing\nservices:\n…\n….\npgadmin:\n…\n…\nVolumes:\npgadmin:/var/lib/pgadmin\nvolumes:\nPgadmin:",
        "question": "How can PostgreSQL data be made persistent when using docker-compose?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 83,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "So one common issue is when you run docker-compose on GCP, postgres won’t persist it’s data to mentioned path for example:\nservices:\n…\n…\npgadmin:\n…\n…\nVolumes:\n“./pgadmin”:/var/lib/pgadmin:wr”\nMight not work so in this use you can use Docker Volume to make it persist, by simply changing\nservices:\n…\n….\npgadmin:\n…\n…\nVolumes:\npgadmin:/var/lib/pgadmin\nvolumes:\nPgadmin:",
        "question": "What is the suggested alternative for persisting data in pgAdmin with Docker?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 83,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "So one common issue is when you run docker-compose on GCP, postgres won’t persist it’s data to mentioned path for example:\nservices:\n…\n…\npgadmin:\n…\n…\nVolumes:\n“./pgadmin”:/var/lib/pgadmin:wr”\nMight not work so in this use you can use Docker Volume to make it persist, by simply changing\nservices:\n…\n….\npgadmin:\n…\n…\nVolumes:\npgadmin:/var/lib/pgadmin\nvolumes:\nPgadmin:",
        "question": "What volume configuration might not work for pgAdmin in docker-compose?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 83,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "So one common issue is when you run docker-compose on GCP, postgres won’t persist it’s data to mentioned path for example:\nservices:\n…\n…\npgadmin:\n…\n…\nVolumes:\n“./pgadmin”:/var/lib/pgadmin:wr”\nMight not work so in this use you can use Docker Volume to make it persist, by simply changing\nservices:\n…\n….\npgadmin:\n…\n…\nVolumes:\npgadmin:/var/lib/pgadmin\nvolumes:\nPgadmin:",
        "question": "What is the correct way to define a volume for pgAdmin to ensure data persistence?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 83,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The docker will keep on crashing continuously\nNot working after restart\ndocker engine stopped\nAnd failed to fetch extensions pop ups will on screen non-stop\nSolution :\nTry checking if latest version of docker is installed / Try updating the docker\nIf Problem still persist then final solution is to reinstall docker\n(Just have to fetch images again else no issues)",
        "question": "What issue is the docker experiencing?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 84,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The docker will keep on crashing continuously\nNot working after restart\ndocker engine stopped\nAnd failed to fetch extensions pop ups will on screen non-stop\nSolution :\nTry checking if latest version of docker is installed / Try updating the docker\nIf Problem still persist then final solution is to reinstall docker\n(Just have to fetch images again else no issues)",
        "question": "What should you check if docker is not working after a restart?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 84,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The docker will keep on crashing continuously\nNot working after restart\ndocker engine stopped\nAnd failed to fetch extensions pop ups will on screen non-stop\nSolution :\nTry checking if latest version of docker is installed / Try updating the docker\nIf Problem still persist then final solution is to reinstall docker\n(Just have to fetch images again else no issues)",
        "question": "What is the final solution if the problem with docker persists?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 84,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The docker will keep on crashing continuously\nNot working after restart\ndocker engine stopped\nAnd failed to fetch extensions pop ups will on screen non-stop\nSolution :\nTry checking if latest version of docker is installed / Try updating the docker\nIf Problem still persist then final solution is to reinstall docker\n(Just have to fetch images again else no issues)",
        "question": "What happens if you reinstall docker?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 84,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The docker will keep on crashing continuously\nNot working after restart\ndocker engine stopped\nAnd failed to fetch extensions pop ups will on screen non-stop\nSolution :\nTry checking if latest version of docker is installed / Try updating the docker\nIf Problem still persist then final solution is to reinstall docker\n(Just have to fetch images again else no issues)",
        "question": "What should you do if there are continuous pop-ups about failed extensions?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 84,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "As per the lessons,\nPersisting pgAdmin configuration (i.e. server name) is done by adding a “volumes” section:\nservices:\npgdatabase:\n[...]\npgadmin:\nimage: dpage/pgadmin4\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root\nvolumes:\n- \"./pgAdmin_data:/var/lib/pgadmin/sessions:rw\"\nports:\n- \"8080:80\"\nIn the example above, ”pgAdmin_data” is a folder on the host machine, and “/var/lib/pgadmin/sessions” is the session settings folder in the pgAdmin container.\nBefore running docker-compose up on the YAML file, we also need to give the pgAdmin container access to write to the “pgAdmin_data” folder. The container runs with a username called “5050” and user group “5050”. The bash command to give access over the mounted volume is:\nsudo chown -R 5050:5050 pgAdmin_data",
        "question": "What is the purpose of the 'volumes' section in the pgAdmin configuration?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 85,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "As per the lessons,\nPersisting pgAdmin configuration (i.e. server name) is done by adding a “volumes” section:\nservices:\npgdatabase:\n[...]\npgadmin:\nimage: dpage/pgadmin4\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root\nvolumes:\n- \"./pgAdmin_data:/var/lib/pgadmin/sessions:rw\"\nports:\n- \"8080:80\"\nIn the example above, ”pgAdmin_data” is a folder on the host machine, and “/var/lib/pgadmin/sessions” is the session settings folder in the pgAdmin container.\nBefore running docker-compose up on the YAML file, we also need to give the pgAdmin container access to write to the “pgAdmin_data” folder. The container runs with a username called “5050” and user group “5050”. The bash command to give access over the mounted volume is:\nsudo chown -R 5050:5050 pgAdmin_data",
        "question": "What are the default email and password set for pgAdmin in the environment configuration?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 85,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "As per the lessons,\nPersisting pgAdmin configuration (i.e. server name) is done by adding a “volumes” section:\nservices:\npgdatabase:\n[...]\npgadmin:\nimage: dpage/pgadmin4\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root\nvolumes:\n- \"./pgAdmin_data:/var/lib/pgadmin/sessions:rw\"\nports:\n- \"8080:80\"\nIn the example above, ”pgAdmin_data” is a folder on the host machine, and “/var/lib/pgadmin/sessions” is the session settings folder in the pgAdmin container.\nBefore running docker-compose up on the YAML file, we also need to give the pgAdmin container access to write to the “pgAdmin_data” folder. The container runs with a username called “5050” and user group “5050”. The bash command to give access over the mounted volume is:\nsudo chown -R 5050:5050 pgAdmin_data",
        "question": "Where is the 'pgAdmin_data' folder located, and what is its role?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 85,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "As per the lessons,\nPersisting pgAdmin configuration (i.e. server name) is done by adding a “volumes” section:\nservices:\npgdatabase:\n[...]\npgadmin:\nimage: dpage/pgadmin4\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root\nvolumes:\n- \"./pgAdmin_data:/var/lib/pgadmin/sessions:rw\"\nports:\n- \"8080:80\"\nIn the example above, ”pgAdmin_data” is a folder on the host machine, and “/var/lib/pgadmin/sessions” is the session settings folder in the pgAdmin container.\nBefore running docker-compose up on the YAML file, we also need to give the pgAdmin container access to write to the “pgAdmin_data” folder. The container runs with a username called “5050” and user group “5050”. The bash command to give access over the mounted volume is:\nsudo chown -R 5050:5050 pgAdmin_data",
        "question": "What command is used to give the pgAdmin container access to the 'pgAdmin_data' folder?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 85,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "As per the lessons,\nPersisting pgAdmin configuration (i.e. server name) is done by adding a “volumes” section:\nservices:\npgdatabase:\n[...]\npgadmin:\nimage: dpage/pgadmin4\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root\nvolumes:\n- \"./pgAdmin_data:/var/lib/pgadmin/sessions:rw\"\nports:\n- \"8080:80\"\nIn the example above, ”pgAdmin_data” is a folder on the host machine, and “/var/lib/pgadmin/sessions” is the session settings folder in the pgAdmin container.\nBefore running docker-compose up on the YAML file, we also need to give the pgAdmin container access to write to the “pgAdmin_data” folder. The container runs with a username called “5050” and user group “5050”. The bash command to give access over the mounted volume is:\nsudo chown -R 5050:5050 pgAdmin_data",
        "question": "Which user and group does the pgAdmin container run as?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 85,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens if you did not create the docker group and added your user. Follow these steps from the link:\nguides/docker-without-sudo.md at main · sindresorhus/guides · GitHub\nAnd then press ctrl+D to log-out and log-in again. pgAdmin: Maintain state so that it remembers your previous connection\nIf you are tired of having to setup your database connection each time that you fire up the containers, all you have to do is create a volume for pgAdmin:\nIn your docker-compose.yaml file, enter the following into your pgAdmin declaration:\nvolumes:\n- type: volume\nsource: pgadmin_data\ntarget: /var/lib/pgadmin\nAlso add the following to the end of the file:ls\nvolumes:\nPgadmin_data:",
        "question": "What should you do if you did not create the docker group and add your user?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 86,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens if you did not create the docker group and added your user. Follow these steps from the link:\nguides/docker-without-sudo.md at main · sindresorhus/guides · GitHub\nAnd then press ctrl+D to log-out and log-in again. pgAdmin: Maintain state so that it remembers your previous connection\nIf you are tired of having to setup your database connection each time that you fire up the containers, all you have to do is create a volume for pgAdmin:\nIn your docker-compose.yaml file, enter the following into your pgAdmin declaration:\nvolumes:\n- type: volume\nsource: pgadmin_data\ntarget: /var/lib/pgadmin\nAlso add the following to the end of the file:ls\nvolumes:\nPgadmin_data:",
        "question": "Where can you find the steps to create the docker group and add your user?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 86,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens if you did not create the docker group and added your user. Follow these steps from the link:\nguides/docker-without-sudo.md at main · sindresorhus/guides · GitHub\nAnd then press ctrl+D to log-out and log-in again. pgAdmin: Maintain state so that it remembers your previous connection\nIf you are tired of having to setup your database connection each time that you fire up the containers, all you have to do is create a volume for pgAdmin:\nIn your docker-compose.yaml file, enter the following into your pgAdmin declaration:\nvolumes:\n- type: volume\nsource: pgadmin_data\ntarget: /var/lib/pgadmin\nAlso add the following to the end of the file:ls\nvolumes:\nPgadmin_data:",
        "question": "How can pgAdmin remember your previous connection settings?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 86,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens if you did not create the docker group and added your user. Follow these steps from the link:\nguides/docker-without-sudo.md at main · sindresorhus/guides · GitHub\nAnd then press ctrl+D to log-out and log-in again. pgAdmin: Maintain state so that it remembers your previous connection\nIf you are tired of having to setup your database connection each time that you fire up the containers, all you have to do is create a volume for pgAdmin:\nIn your docker-compose.yaml file, enter the following into your pgAdmin declaration:\nvolumes:\n- type: volume\nsource: pgadmin_data\ntarget: /var/lib/pgadmin\nAlso add the following to the end of the file:ls\nvolumes:\nPgadmin_data:",
        "question": "What changes should be made in the docker-compose.yaml file to maintain pgAdmin state?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 86,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens if you did not create the docker group and added your user. Follow these steps from the link:\nguides/docker-without-sudo.md at main · sindresorhus/guides · GitHub\nAnd then press ctrl+D to log-out and log-in again. pgAdmin: Maintain state so that it remembers your previous connection\nIf you are tired of having to setup your database connection each time that you fire up the containers, all you have to do is create a volume for pgAdmin:\nIn your docker-compose.yaml file, enter the following into your pgAdmin declaration:\nvolumes:\n- type: volume\nsource: pgadmin_data\ntarget: /var/lib/pgadmin\nAlso add the following to the end of the file:ls\nvolumes:\nPgadmin_data:",
        "question": "What is the purpose of creating a volume for pgAdmin in Docker?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 86,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is happen to me after following 1.4.1 video where we are installing docker compose in our Google Cloud VM. In my case, the docker-compose file downloaded from github named docker-compose-linux-x86_64 while it is more convenient to use docker-compose command instead. So just change the docker-compose-linux-x86_64 into docker-compose.",
        "question": "What video was followed for installing Docker Compose?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 87,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is happen to me after following 1.4.1 video where we are installing docker compose in our Google Cloud VM. In my case, the docker-compose file downloaded from github named docker-compose-linux-x86_64 while it is more convenient to use docker-compose command instead. So just change the docker-compose-linux-x86_64 into docker-compose.",
        "question": "Where was Docker Compose installed in the user's case?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 87,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is happen to me after following 1.4.1 video where we are installing docker compose in our Google Cloud VM. In my case, the docker-compose file downloaded from github named docker-compose-linux-x86_64 while it is more convenient to use docker-compose command instead. So just change the docker-compose-linux-x86_64 into docker-compose.",
        "question": "What is the name of the downloaded docker-compose file from GitHub?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 87,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is happen to me after following 1.4.1 video where we are installing docker compose in our Google Cloud VM. In my case, the docker-compose file downloaded from github named docker-compose-linux-x86_64 while it is more convenient to use docker-compose command instead. So just change the docker-compose-linux-x86_64 into docker-compose.",
        "question": "Why does the user prefer to use 'docker-compose' command instead?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 87,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is happen to me after following 1.4.1 video where we are installing docker compose in our Google Cloud VM. In my case, the docker-compose file downloaded from github named docker-compose-linux-x86_64 while it is more convenient to use docker-compose command instead. So just change the docker-compose-linux-x86_64 into docker-compose.",
        "question": "What change does the user suggest making to the downloaded file name?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 87,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Installing pass via ‘sudo apt install pass’ helped to solve the issue. More about this can be found here: https://github.com/moby/buildkit/issues/1078",
        "question": "What command is used to install 'pass'?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 88,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Installing pass via ‘sudo apt install pass’ helped to solve the issue. More about this can be found here: https://github.com/moby/buildkit/issues/1078",
        "question": "What issue does installing 'pass' help to solve?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 88,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Installing pass via ‘sudo apt install pass’ helped to solve the issue. More about this can be found here: https://github.com/moby/buildkit/issues/1078",
        "question": "Where can more information about the issue be found?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 88,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Installing pass via ‘sudo apt install pass’ helped to solve the issue. More about this can be found here: https://github.com/moby/buildkit/issues/1078",
        "question": "What platform is being referenced in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 88,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Installing pass via ‘sudo apt install pass’ helped to solve the issue. More about this can be found here: https://github.com/moby/buildkit/issues/1078",
        "question": "What is the URL provided in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 88,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For everyone who's having problem with Docker compose, getting the data in postgres and similar issues, please take care of the following:\ncreate a new volume on docker (either using the command line or docker desktop app)\nmake the following changes to your docker-compose.yml file (see attachment)\nset low_memory=false when importing the csv file (df = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False))\nuse the below function (in the upload-data.ipynb) for better tracking of your ingestion process (see attachment)\nOrder of execution:\n(1) open terminal in 2_docker_sql folder and run docker compose up\n(2) ensure no other containers are running except the one you just executed (pgadmin and pgdatabase)\n(3) open jupyter notebook and begin the data ingestion\n(4) open pgadmin and set up a server (make sure you use the same configurations as your docker-compose.yml file like the same name (pgdatabase), port, databasename (ny_taxi) etc.",
        "question": "What is the first step to take if someone is having issues with Docker compose and Postgres?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 89,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For everyone who's having problem with Docker compose, getting the data in postgres and similar issues, please take care of the following:\ncreate a new volume on docker (either using the command line or docker desktop app)\nmake the following changes to your docker-compose.yml file (see attachment)\nset low_memory=false when importing the csv file (df = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False))\nuse the below function (in the upload-data.ipynb) for better tracking of your ingestion process (see attachment)\nOrder of execution:\n(1) open terminal in 2_docker_sql folder and run docker compose up\n(2) ensure no other containers are running except the one you just executed (pgadmin and pgdatabase)\n(3) open jupyter notebook and begin the data ingestion\n(4) open pgadmin and set up a server (make sure you use the same configurations as your docker-compose.yml file like the same name (pgdatabase), port, databasename (ny_taxi) etc.",
        "question": "How should the docker-compose.yml file be modified according to the provided text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 89,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For everyone who's having problem with Docker compose, getting the data in postgres and similar issues, please take care of the following:\ncreate a new volume on docker (either using the command line or docker desktop app)\nmake the following changes to your docker-compose.yml file (see attachment)\nset low_memory=false when importing the csv file (df = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False))\nuse the below function (in the upload-data.ipynb) for better tracking of your ingestion process (see attachment)\nOrder of execution:\n(1) open terminal in 2_docker_sql folder and run docker compose up\n(2) ensure no other containers are running except the one you just executed (pgadmin and pgdatabase)\n(3) open jupyter notebook and begin the data ingestion\n(4) open pgadmin and set up a server (make sure you use the same configurations as your docker-compose.yml file like the same name (pgdatabase), port, databasename (ny_taxi) etc.",
        "question": "What is the purpose of setting low_memory=false when importing a CSV file in the context of this document?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 89,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For everyone who's having problem with Docker compose, getting the data in postgres and similar issues, please take care of the following:\ncreate a new volume on docker (either using the command line or docker desktop app)\nmake the following changes to your docker-compose.yml file (see attachment)\nset low_memory=false when importing the csv file (df = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False))\nuse the below function (in the upload-data.ipynb) for better tracking of your ingestion process (see attachment)\nOrder of execution:\n(1) open terminal in 2_docker_sql folder and run docker compose up\n(2) ensure no other containers are running except the one you just executed (pgadmin and pgdatabase)\n(3) open jupyter notebook and begin the data ingestion\n(4) open pgadmin and set up a server (make sure you use the same configurations as your docker-compose.yml file like the same name (pgdatabase), port, databasename (ny_taxi) etc.",
        "question": "What is the order of execution for the steps related to data ingestion as outlined in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 89,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For everyone who's having problem with Docker compose, getting the data in postgres and similar issues, please take care of the following:\ncreate a new volume on docker (either using the command line or docker desktop app)\nmake the following changes to your docker-compose.yml file (see attachment)\nset low_memory=false when importing the csv file (df = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False))\nuse the below function (in the upload-data.ipynb) for better tracking of your ingestion process (see attachment)\nOrder of execution:\n(1) open terminal in 2_docker_sql folder and run docker compose up\n(2) ensure no other containers are running except the one you just executed (pgadmin and pgdatabase)\n(3) open jupyter notebook and begin the data ingestion\n(4) open pgadmin and set up a server (make sure you use the same configurations as your docker-compose.yml file like the same name (pgdatabase), port, databasename (ny_taxi) etc.",
        "question": "What configurations should be used when setting up a server in pgadmin according to the provided instructions?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 89,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Locate config.json file for docker (check your home directory; Users/username/.docker).\nModify credsStore to credStore\nSave and re-run",
        "question": "Where can the config.json file for Docker be found?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 90,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Locate config.json file for docker (check your home directory; Users/username/.docker).\nModify credsStore to credStore\nSave and re-run",
        "question": "What should be modified in the config.json file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 90,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Locate config.json file for docker (check your home directory; Users/username/.docker).\nModify credsStore to credStore\nSave and re-run",
        "question": "What do you need to change 'credsStore' to in the config.json file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 90,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Locate config.json file for docker (check your home directory; Users/username/.docker).\nModify credsStore to credStore\nSave and re-run",
        "question": "What is the file path to check for the config.json file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 90,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Locate config.json file for docker (check your home directory; Users/username/.docker).\nModify credsStore to credStore\nSave and re-run",
        "question": "What should you do after modifying the config.json file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 90,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you wrote the docker-compose.yaml file exactly like the video, you might run into an error like this:dev\nservice \"pgdatabase\" refers to undefined volume dtc_postgres_volume_local: invalid compose project\nIn order to make it work, you need to include the volume in your docker-compose file. Just add the following:\nvolumes:\ndtc_postgres_volume_local:\n(Make sure volumes are at the same level as services.)",
        "question": "What error might occur if the docker-compose.yaml file is written exactly as in the video?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 92,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you wrote the docker-compose.yaml file exactly like the video, you might run into an error like this:dev\nservice \"pgdatabase\" refers to undefined volume dtc_postgres_volume_local: invalid compose project\nIn order to make it work, you need to include the volume in your docker-compose file. Just add the following:\nvolumes:\ndtc_postgres_volume_local:\n(Make sure volumes are at the same level as services.)",
        "question": "What does the error message regarding 'pgdatabase' indicate?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 92,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you wrote the docker-compose.yaml file exactly like the video, you might run into an error like this:dev\nservice \"pgdatabase\" refers to undefined volume dtc_postgres_volume_local: invalid compose project\nIn order to make it work, you need to include the volume in your docker-compose file. Just add the following:\nvolumes:\ndtc_postgres_volume_local:\n(Make sure volumes are at the same level as services.)",
        "question": "How can you resolve the error related to the undefined volume in the docker-compose file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 92,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you wrote the docker-compose.yaml file exactly like the video, you might run into an error like this:dev\nservice \"pgdatabase\" refers to undefined volume dtc_postgres_volume_local: invalid compose project\nIn order to make it work, you need to include the volume in your docker-compose file. Just add the following:\nvolumes:\ndtc_postgres_volume_local:\n(Make sure volumes are at the same level as services.)",
        "question": "What line should be added to the docker-compose.yaml file to define the volume?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 92,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you wrote the docker-compose.yaml file exactly like the video, you might run into an error like this:dev\nservice \"pgdatabase\" refers to undefined volume dtc_postgres_volume_local: invalid compose project\nIn order to make it work, you need to include the volume in your docker-compose file. Just add the following:\nvolumes:\ndtc_postgres_volume_local:\n(Make sure volumes are at the same level as services.)",
        "question": "At what level should the volumes be located in the docker-compose file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 92,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error:  initdb: error: could not change permissions of directory\nIssue: WSL and Windows do not manage permissions in the same way causing conflict if using the Windows file system rather than the WSL file system.\nSolution: Use Docker volumes.\nWhy: Volume is used for storage of persistent data and not for use of transferring files. A local volume is unnecessary.\nBenefit: This resolves permission issues and allows for better management of volumes.\nNOTE: the ‘user:’ is not necessary if using docker volumes, but is if using local drive.\n</>  docker-compose.yaml\nservices:\npostgres:\nimage: postgres:15-alpine\ncontainer_name: postgres\nuser: \"0:0\"\nenvironment:\n- POSTGRES_USER=postgres\n- POSTGRES_PASSWORD=postgres\n- POSTGRES_DB=ny_taxi\nvolumes:\n- \"pg-data:/var/lib/postgresql/data\"\nports:\n- \"5432:5432\"\nnetworks:\n- pg-network\npgadmin:\nimage: dpage/pgadmin4\ncontainer_name: pgadmin\nuser: \"${UID}:${GID}\"\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=email@some-site.com\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\nvolumes:\n- \"pg-admin:/var/lib/pgadmin\"\nports:\n- \"8080:80\"\nnetworks:\n- pg-network\nnetworks:\npg-network:\nname: pg-network\nvolumes:\npg-data:\nname: ingest_pgdata\npg-admin:\nname: ingest_pgadmin",
        "question": "What error is encountered when trying to change permissions of a directory?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 93,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error:  initdb: error: could not change permissions of directory\nIssue: WSL and Windows do not manage permissions in the same way causing conflict if using the Windows file system rather than the WSL file system.\nSolution: Use Docker volumes.\nWhy: Volume is used for storage of persistent data and not for use of transferring files. A local volume is unnecessary.\nBenefit: This resolves permission issues and allows for better management of volumes.\nNOTE: the ‘user:’ is not necessary if using docker volumes, but is if using local drive.\n</>  docker-compose.yaml\nservices:\npostgres:\nimage: postgres:15-alpine\ncontainer_name: postgres\nuser: \"0:0\"\nenvironment:\n- POSTGRES_USER=postgres\n- POSTGRES_PASSWORD=postgres\n- POSTGRES_DB=ny_taxi\nvolumes:\n- \"pg-data:/var/lib/postgresql/data\"\nports:\n- \"5432:5432\"\nnetworks:\n- pg-network\npgadmin:\nimage: dpage/pgadmin4\ncontainer_name: pgadmin\nuser: \"${UID}:${GID}\"\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=email@some-site.com\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\nvolumes:\n- \"pg-admin:/var/lib/pgadmin\"\nports:\n- \"8080:80\"\nnetworks:\n- pg-network\nnetworks:\npg-network:\nname: pg-network\nvolumes:\npg-data:\nname: ingest_pgdata\npg-admin:\nname: ingest_pgadmin",
        "question": "How do WSL and Windows differ in managing permissions?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 93,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error:  initdb: error: could not change permissions of directory\nIssue: WSL and Windows do not manage permissions in the same way causing conflict if using the Windows file system rather than the WSL file system.\nSolution: Use Docker volumes.\nWhy: Volume is used for storage of persistent data and not for use of transferring files. A local volume is unnecessary.\nBenefit: This resolves permission issues and allows for better management of volumes.\nNOTE: the ‘user:’ is not necessary if using docker volumes, but is if using local drive.\n</>  docker-compose.yaml\nservices:\npostgres:\nimage: postgres:15-alpine\ncontainer_name: postgres\nuser: \"0:0\"\nenvironment:\n- POSTGRES_USER=postgres\n- POSTGRES_PASSWORD=postgres\n- POSTGRES_DB=ny_taxi\nvolumes:\n- \"pg-data:/var/lib/postgresql/data\"\nports:\n- \"5432:5432\"\nnetworks:\n- pg-network\npgadmin:\nimage: dpage/pgadmin4\ncontainer_name: pgadmin\nuser: \"${UID}:${GID}\"\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=email@some-site.com\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\nvolumes:\n- \"pg-admin:/var/lib/pgadmin\"\nports:\n- \"8080:80\"\nnetworks:\n- pg-network\nnetworks:\npg-network:\nname: pg-network\nvolumes:\npg-data:\nname: ingest_pgdata\npg-admin:\nname: ingest_pgadmin",
        "question": "What is suggested as a solution to the permission issues encountered?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 93,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error:  initdb: error: could not change permissions of directory\nIssue: WSL and Windows do not manage permissions in the same way causing conflict if using the Windows file system rather than the WSL file system.\nSolution: Use Docker volumes.\nWhy: Volume is used for storage of persistent data and not for use of transferring files. A local volume is unnecessary.\nBenefit: This resolves permission issues and allows for better management of volumes.\nNOTE: the ‘user:’ is not necessary if using docker volumes, but is if using local drive.\n</>  docker-compose.yaml\nservices:\npostgres:\nimage: postgres:15-alpine\ncontainer_name: postgres\nuser: \"0:0\"\nenvironment:\n- POSTGRES_USER=postgres\n- POSTGRES_PASSWORD=postgres\n- POSTGRES_DB=ny_taxi\nvolumes:\n- \"pg-data:/var/lib/postgresql/data\"\nports:\n- \"5432:5432\"\nnetworks:\n- pg-network\npgadmin:\nimage: dpage/pgadmin4\ncontainer_name: pgadmin\nuser: \"${UID}:${GID}\"\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=email@some-site.com\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\nvolumes:\n- \"pg-admin:/var/lib/pgadmin\"\nports:\n- \"8080:80\"\nnetworks:\n- pg-network\nnetworks:\npg-network:\nname: pg-network\nvolumes:\npg-data:\nname: ingest_pgdata\npg-admin:\nname: ingest_pgadmin",
        "question": "What is the purpose of using Docker volumes?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 93,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error:  initdb: error: could not change permissions of directory\nIssue: WSL and Windows do not manage permissions in the same way causing conflict if using the Windows file system rather than the WSL file system.\nSolution: Use Docker volumes.\nWhy: Volume is used for storage of persistent data and not for use of transferring files. A local volume is unnecessary.\nBenefit: This resolves permission issues and allows for better management of volumes.\nNOTE: the ‘user:’ is not necessary if using docker volumes, but is if using local drive.\n</>  docker-compose.yaml\nservices:\npostgres:\nimage: postgres:15-alpine\ncontainer_name: postgres\nuser: \"0:0\"\nenvironment:\n- POSTGRES_USER=postgres\n- POSTGRES_PASSWORD=postgres\n- POSTGRES_DB=ny_taxi\nvolumes:\n- \"pg-data:/var/lib/postgresql/data\"\nports:\n- \"5432:5432\"\nnetworks:\n- pg-network\npgadmin:\nimage: dpage/pgadmin4\ncontainer_name: pgadmin\nuser: \"${UID}:${GID}\"\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=email@some-site.com\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\nvolumes:\n- \"pg-admin:/var/lib/pgadmin\"\nports:\n- \"8080:80\"\nnetworks:\n- pg-network\nnetworks:\npg-network:\nname: pg-network\nvolumes:\npg-data:\nname: ingest_pgdata\npg-admin:\nname: ingest_pgadmin",
        "question": "Is the 'user:' specification necessary when using Docker volumes?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 93,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Cause : If Running on git bash or vm in windows pgadmin doesnt work easily LIbraries like psycopg2 and libpq ar required still the error persists.\nSolution- I use psql instead of pgadmin totally same\nPip install psycopg2\ndock",
        "question": "What issues might arise when using pgAdmin on Windows with git bash or VM?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 94,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Cause : If Running on git bash or vm in windows pgadmin doesnt work easily LIbraries like psycopg2 and libpq ar required still the error persists.\nSolution- I use psql instead of pgadmin totally same\nPip install psycopg2\ndock",
        "question": "What libraries are mentioned as being required for pgAdmin to work?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 94,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Cause : If Running on git bash or vm in windows pgadmin doesnt work easily LIbraries like psycopg2 and libpq ar required still the error persists.\nSolution- I use psql instead of pgadmin totally same\nPip install psycopg2\ndock",
        "question": "What solution is suggested for using pgAdmin?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 94,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Cause : If Running on git bash or vm in windows pgadmin doesnt work easily LIbraries like psycopg2 and libpq ar required still the error persists.\nSolution- I use psql instead of pgadmin totally same\nPip install psycopg2\ndock",
        "question": "How can psycopg2 be installed according to the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 94,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Cause : If Running on git bash or vm in windows pgadmin doesnt work easily LIbraries like psycopg2 and libpq ar required still the error persists.\nSolution- I use psql instead of pgadmin totally same\nPip install psycopg2\ndock",
        "question": "What alternative to pgAdmin is mentioned in the solution?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 94,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Cause:\nIt happens because the apps are not updated. To be specific, search for any pending updates for Windows Terminal, WSL and Windows Security updates.\nSolution\nfor updating Windows terminal which worked for me:\nGo to Microsoft Store.\nGo to the library of apps installed in your system.\nSearch for Windows terminal.\nUpdate the app and restart your system to  see the changes.\nFor updating the Windows security updates:\nGo to Windows updates and check if there are any pending updates from Windows, especially security updates.\nDo restart your system once the updates are downloaded and installed successfully.",
        "question": "What causes the issues mentioned in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 95,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Cause:\nIt happens because the apps are not updated. To be specific, search for any pending updates for Windows Terminal, WSL and Windows Security updates.\nSolution\nfor updating Windows terminal which worked for me:\nGo to Microsoft Store.\nGo to the library of apps installed in your system.\nSearch for Windows terminal.\nUpdate the app and restart your system to  see the changes.\nFor updating the Windows security updates:\nGo to Windows updates and check if there are any pending updates from Windows, especially security updates.\nDo restart your system once the updates are downloaded and installed successfully.",
        "question": "How can you update the Windows Terminal?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 95,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Cause:\nIt happens because the apps are not updated. To be specific, search for any pending updates for Windows Terminal, WSL and Windows Security updates.\nSolution\nfor updating Windows terminal which worked for me:\nGo to Microsoft Store.\nGo to the library of apps installed in your system.\nSearch for Windows terminal.\nUpdate the app and restart your system to  see the changes.\nFor updating the Windows security updates:\nGo to Windows updates and check if there are any pending updates from Windows, especially security updates.\nDo restart your system once the updates are downloaded and installed successfully.",
        "question": "Where can you find the library of apps installed on your system?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 95,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Cause:\nIt happens because the apps are not updated. To be specific, search for any pending updates for Windows Terminal, WSL and Windows Security updates.\nSolution\nfor updating Windows terminal which worked for me:\nGo to Microsoft Store.\nGo to the library of apps installed in your system.\nSearch for Windows terminal.\nUpdate the app and restart your system to  see the changes.\nFor updating the Windows security updates:\nGo to Windows updates and check if there are any pending updates from Windows, especially security updates.\nDo restart your system once the updates are downloaded and installed successfully.",
        "question": "What should you do after updating the Windows Terminal?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 95,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Cause:\nIt happens because the apps are not updated. To be specific, search for any pending updates for Windows Terminal, WSL and Windows Security updates.\nSolution\nfor updating Windows terminal which worked for me:\nGo to Microsoft Store.\nGo to the library of apps installed in your system.\nSearch for Windows terminal.\nUpdate the app and restart your system to  see the changes.\nFor updating the Windows security updates:\nGo to Windows updates and check if there are any pending updates from Windows, especially security updates.\nDo restart your system once the updates are downloaded and installed successfully.",
        "question": "How do you check for pending Windows security updates?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 95,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Up restardoting the same issue appears. Happens out of the blue on windows.\nSolution 1: Fixing DNS Issue (credit: reddit) this worked for me personally\nreg add \"HKLM\\System\\CurrentControlSet\\Services\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"4\" /f\nRestart your computer and then enable it with the following\nreg add \"HKLM\\System\\CurrentControlSet\\Services\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"2\" /f\nRestart your OS again. It should work.\nSolution 2: right click on running Docker icon (next to clock) and chose \"Switch to Linux containers\"\nbash: conda: command not found\nDatabase is uninitialized and superuser password is not specified.\nDatabase is uninitialized and superuser password is not specified.",
        "question": "What issue is being reported in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 96,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Up restardoting the same issue appears. Happens out of the blue on windows.\nSolution 1: Fixing DNS Issue (credit: reddit) this worked for me personally\nreg add \"HKLM\\System\\CurrentControlSet\\Services\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"4\" /f\nRestart your computer and then enable it with the following\nreg add \"HKLM\\System\\CurrentControlSet\\Services\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"2\" /f\nRestart your OS again. It should work.\nSolution 2: right click on running Docker icon (next to clock) and chose \"Switch to Linux containers\"\nbash: conda: command not found\nDatabase is uninitialized and superuser password is not specified.\nDatabase is uninitialized and superuser password is not specified.",
        "question": "What is the first solution suggested to fix the DNS issue?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 96,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Up restardoting the same issue appears. Happens out of the blue on windows.\nSolution 1: Fixing DNS Issue (credit: reddit) this worked for me personally\nreg add \"HKLM\\System\\CurrentControlSet\\Services\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"4\" /f\nRestart your computer and then enable it with the following\nreg add \"HKLM\\System\\CurrentControlSet\\Services\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"2\" /f\nRestart your OS again. It should work.\nSolution 2: right click on running Docker icon (next to clock) and chose \"Switch to Linux containers\"\nbash: conda: command not found\nDatabase is uninitialized and superuser password is not specified.\nDatabase is uninitialized and superuser password is not specified.",
        "question": "What command is used to enable the Dnscache service after a restart?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 96,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Up restardoting the same issue appears. Happens out of the blue on windows.\nSolution 1: Fixing DNS Issue (credit: reddit) this worked for me personally\nreg add \"HKLM\\System\\CurrentControlSet\\Services\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"4\" /f\nRestart your computer and then enable it with the following\nreg add \"HKLM\\System\\CurrentControlSet\\Services\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"2\" /f\nRestart your OS again. It should work.\nSolution 2: right click on running Docker icon (next to clock) and chose \"Switch to Linux containers\"\nbash: conda: command not found\nDatabase is uninitialized and superuser password is not specified.\nDatabase is uninitialized and superuser password is not specified.",
        "question": "What is the purpose of right-clicking on the Docker icon in the second solution?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 96,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Up restardoting the same issue appears. Happens out of the blue on windows.\nSolution 1: Fixing DNS Issue (credit: reddit) this worked for me personally\nreg add \"HKLM\\System\\CurrentControlSet\\Services\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"4\" /f\nRestart your computer and then enable it with the following\nreg add \"HKLM\\System\\CurrentControlSet\\Services\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"2\" /f\nRestart your OS again. It should work.\nSolution 2: right click on running Docker icon (next to clock) and chose \"Switch to Linux containers\"\nbash: conda: command not found\nDatabase is uninitialized and superuser password is not specified.\nDatabase is uninitialized and superuser password is not specified.",
        "question": "What error message is mentioned regarding the database in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 96,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue when trying to run the GPC VM through SSH through WSL2,  probably because WSL2 isn’t looking for .ssh keys in the correct folder. My case I was trying to run this command in the terminal and getting an error\nPC:/mnt/c/Users/User/.ssh$ ssh -i gpc [username]@[my external IP]\nYou can try to use sudo before the command\nSudo .ssh$ ssh -i gpc [username]@[my external IP]\nYou can also try to cd to your folder and change the permissions for the private key SSH file.\nchmod 600 gpc\nIf that doesn’t work, create a .ssh folder in the home diretory of WSL2 and copy the content of windows .ssh folder to that new folder.\ncd ~\nmkdir .ssh\ncp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\nYou might need to adjust the permissions of the files and folders in the .ssh directory.",
        "question": "What issue is being encountered when trying to run the GPC VM through SSH in WSL2?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 97,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue when trying to run the GPC VM through SSH through WSL2,  probably because WSL2 isn’t looking for .ssh keys in the correct folder. My case I was trying to run this command in the terminal and getting an error\nPC:/mnt/c/Users/User/.ssh$ ssh -i gpc [username]@[my external IP]\nYou can try to use sudo before the command\nSudo .ssh$ ssh -i gpc [username]@[my external IP]\nYou can also try to cd to your folder and change the permissions for the private key SSH file.\nchmod 600 gpc\nIf that doesn’t work, create a .ssh folder in the home diretory of WSL2 and copy the content of windows .ssh folder to that new folder.\ncd ~\nmkdir .ssh\ncp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\nYou might need to adjust the permissions of the files and folders in the .ssh directory.",
        "question": "What command is suggested to run the SSH connection with elevated privileges?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 97,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue when trying to run the GPC VM through SSH through WSL2,  probably because WSL2 isn’t looking for .ssh keys in the correct folder. My case I was trying to run this command in the terminal and getting an error\nPC:/mnt/c/Users/User/.ssh$ ssh -i gpc [username]@[my external IP]\nYou can try to use sudo before the command\nSudo .ssh$ ssh -i gpc [username]@[my external IP]\nYou can also try to cd to your folder and change the permissions for the private key SSH file.\nchmod 600 gpc\nIf that doesn’t work, create a .ssh folder in the home diretory of WSL2 and copy the content of windows .ssh folder to that new folder.\ncd ~\nmkdir .ssh\ncp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\nYou might need to adjust the permissions of the files and folders in the .ssh directory.",
        "question": "How can one change the permissions of the SSH private key file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 97,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue when trying to run the GPC VM through SSH through WSL2,  probably because WSL2 isn’t looking for .ssh keys in the correct folder. My case I was trying to run this command in the terminal and getting an error\nPC:/mnt/c/Users/User/.ssh$ ssh -i gpc [username]@[my external IP]\nYou can try to use sudo before the command\nSudo .ssh$ ssh -i gpc [username]@[my external IP]\nYou can also try to cd to your folder and change the permissions for the private key SSH file.\nchmod 600 gpc\nIf that doesn’t work, create a .ssh folder in the home diretory of WSL2 and copy the content of windows .ssh folder to that new folder.\ncd ~\nmkdir .ssh\ncp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\nYou might need to adjust the permissions of the files and folders in the .ssh directory.",
        "question": "What steps should be taken if the SSH command does not work initially?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 97,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue when trying to run the GPC VM through SSH through WSL2,  probably because WSL2 isn’t looking for .ssh keys in the correct folder. My case I was trying to run this command in the terminal and getting an error\nPC:/mnt/c/Users/User/.ssh$ ssh -i gpc [username]@[my external IP]\nYou can try to use sudo before the command\nSudo .ssh$ ssh -i gpc [username]@[my external IP]\nYou can also try to cd to your folder and change the permissions for the private key SSH file.\nchmod 600 gpc\nIf that doesn’t work, create a .ssh folder in the home diretory of WSL2 and copy the content of windows .ssh folder to that new folder.\ncd ~\nmkdir .ssh\ncp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\nYou might need to adjust the permissions of the files and folders in the .ssh directory.",
        "question": "Why might it be necessary to create a .ssh folder in the home directory of WSL2?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 97,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Such as the issue above, WSL2 may not be referencing the correct .ssh/config path from Windows. You can create a config file at the home directory of WSL2.\ncd ~\nmkdir .ssh\nCreate a config file in this new .ssh/ folder referencing this folder:\nHostName [GPC VM external IP]\nUser [username]\nIdentityFile ~/.ssh/[private key]",
        "question": "What issue is mentioned regarding WSL2 in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 98,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Such as the issue above, WSL2 may not be referencing the correct .ssh/config path from Windows. You can create a config file at the home directory of WSL2.\ncd ~\nmkdir .ssh\nCreate a config file in this new .ssh/ folder referencing this folder:\nHostName [GPC VM external IP]\nUser [username]\nIdentityFile ~/.ssh/[private key]",
        "question": "Where can you create a config file in WSL2?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 98,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Such as the issue above, WSL2 may not be referencing the correct .ssh/config path from Windows. You can create a config file at the home directory of WSL2.\ncd ~\nmkdir .ssh\nCreate a config file in this new .ssh/ folder referencing this folder:\nHostName [GPC VM external IP]\nUser [username]\nIdentityFile ~/.ssh/[private key]",
        "question": "What command is used to navigate to the home directory in WSL2?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 98,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Such as the issue above, WSL2 may not be referencing the correct .ssh/config path from Windows. You can create a config file at the home directory of WSL2.\ncd ~\nmkdir .ssh\nCreate a config file in this new .ssh/ folder referencing this folder:\nHostName [GPC VM external IP]\nUser [username]\nIdentityFile ~/.ssh/[private key]",
        "question": "What information should be included in the config file for WSL2?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 98,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Such as the issue above, WSL2 may not be referencing the correct .ssh/config path from Windows. You can create a config file at the home directory of WSL2.\ncd ~\nmkdir .ssh\nCreate a config file in this new .ssh/ folder referencing this folder:\nHostName [GPC VM external IP]\nUser [username]\nIdentityFile ~/.ssh/[private key]",
        "question": "What is the purpose of the IdentityFile entry in the config file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 98,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change TO Socket\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi",
        "question": "What command is used to connect to the database in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 99,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change TO Socket\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi",
        "question": "Which host is specified in the connection command?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 99,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change TO Socket\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi",
        "question": "What is the database name being connected to?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 99,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change TO Socket\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi",
        "question": "What port number is used in the connection command?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 99,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change TO Socket\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi",
        "question": "Who is the user that is attempting to connect to the database?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 99,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "probably some installation error, check out sy",
        "question": "What is likely the cause of the issue mentioned?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 100,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "probably some installation error, check out sy",
        "question": "What should be checked to resolve the installation error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 100,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "probably some installation error, check out sy",
        "question": "Is there a specific component or setting referred to by 'sy'?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 100,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "probably some installation error, check out sy",
        "question": "What steps can be taken to troubleshoot the installation error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 100,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "probably some installation error, check out sy",
        "question": "Could there be other errors besides installation that need to be considered?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 100,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In this section of the course, the 5432 port of pgsql is mapped to your computer’s 5432 port. Which means you can access the postgres database via pgcli directly from your computer.\nSo No, you don’t need to run it inside another container. Your local system will do.",
        "question": "What database is accessed through port 5432 in this course?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 101,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In this section of the course, the 5432 port of pgsql is mapped to your computer’s 5432 port. Which means you can access the postgres database via pgcli directly from your computer.\nSo No, you don’t need to run it inside another container. Your local system will do.",
        "question": "Do you need to run pgsql inside another container to access the database?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 101,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In this section of the course, the 5432 port of pgsql is mapped to your computer’s 5432 port. Which means you can access the postgres database via pgcli directly from your computer.\nSo No, you don’t need to run it inside another container. Your local system will do.",
        "question": "Which client can be used to access the postgres database?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 101,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In this section of the course, the 5432 port of pgsql is mapped to your computer’s 5432 port. Which means you can access the postgres database via pgcli directly from your computer.\nSo No, you don’t need to run it inside another container. Your local system will do.",
        "question": "What port on your computer is mapped to the pgsql port?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 101,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In this section of the course, the 5432 port of pgsql is mapped to your computer’s 5432 port. Which means you can access the postgres database via pgcli directly from your computer.\nSo No, you don’t need to run it inside another container. Your local system will do.",
        "question": "Can you access the postgres database directly from your local system?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 101,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "FATAL:  password authentication failed for user \"root\"\nobservations: Below in bold do not forget the folder that was created ny_taxi_postgres_data\nThis happens if you have a local Postgres installation in your computer. To mitigate this, use a different port, like 5431, when creating the docker container, as in: -p 5431: 5432\nThen, we need to use this port when connecting to pgcli, as shown below:\npgcli -h localhost -p 5431 -u root -d ny_taxi\nThis will connect you to your postgres docker container, which is mapped to your host’s 5431 port (though you might choose any port of your liking as long as it is not occupied).\nFor a more visual and detailed explanation, feel free to check the video 1.4.2 - Port Mapping and Networks in Docker\nIf you want to debug: the following can help (on a MacOS)\nTo find out if something is blocking your port (on a MacOS):\nYou can use the lsof command to find out which application is using a specific port on your local machine. `lsof -i :5432`wi\nOr list the running postgres services on your local machine with launchctl\nTo unload the running service on your local machine (on a MacOS):\nunload the launch agent for the PostgreSQL service, which will stop the service and free up the port  \n`launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\nthis one to start it again\n`launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\nChanging port from 5432:5432 to 5431:5432 helped me to avoid this error.",
        "question": "What does the error message 'FATAL: password authentication failed for user \"root\"' indicate?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 102,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "FATAL:  password authentication failed for user \"root\"\nobservations: Below in bold do not forget the folder that was created ny_taxi_postgres_data\nThis happens if you have a local Postgres installation in your computer. To mitigate this, use a different port, like 5431, when creating the docker container, as in: -p 5431: 5432\nThen, we need to use this port when connecting to pgcli, as shown below:\npgcli -h localhost -p 5431 -u root -d ny_taxi\nThis will connect you to your postgres docker container, which is mapped to your host’s 5431 port (though you might choose any port of your liking as long as it is not occupied).\nFor a more visual and detailed explanation, feel free to check the video 1.4.2 - Port Mapping and Networks in Docker\nIf you want to debug: the following can help (on a MacOS)\nTo find out if something is blocking your port (on a MacOS):\nYou can use the lsof command to find out which application is using a specific port on your local machine. `lsof -i :5432`wi\nOr list the running postgres services on your local machine with launchctl\nTo unload the running service on your local machine (on a MacOS):\nunload the launch agent for the PostgreSQL service, which will stop the service and free up the port  \n`launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\nthis one to start it again\n`launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\nChanging port from 5432:5432 to 5431:5432 helped me to avoid this error.",
        "question": "What is the significance of the folder named ny_taxi_postgres_data?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 102,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "FATAL:  password authentication failed for user \"root\"\nobservations: Below in bold do not forget the folder that was created ny_taxi_postgres_data\nThis happens if you have a local Postgres installation in your computer. To mitigate this, use a different port, like 5431, when creating the docker container, as in: -p 5431: 5432\nThen, we need to use this port when connecting to pgcli, as shown below:\npgcli -h localhost -p 5431 -u root -d ny_taxi\nThis will connect you to your postgres docker container, which is mapped to your host’s 5431 port (though you might choose any port of your liking as long as it is not occupied).\nFor a more visual and detailed explanation, feel free to check the video 1.4.2 - Port Mapping and Networks in Docker\nIf you want to debug: the following can help (on a MacOS)\nTo find out if something is blocking your port (on a MacOS):\nYou can use the lsof command to find out which application is using a specific port on your local machine. `lsof -i :5432`wi\nOr list the running postgres services on your local machine with launchctl\nTo unload the running service on your local machine (on a MacOS):\nunload the launch agent for the PostgreSQL service, which will stop the service and free up the port  \n`launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\nthis one to start it again\n`launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\nChanging port from 5432:5432 to 5431:5432 helped me to avoid this error.",
        "question": "How can you mitigate the password authentication error when using a local Postgres installation?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 102,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "FATAL:  password authentication failed for user \"root\"\nobservations: Below in bold do not forget the folder that was created ny_taxi_postgres_data\nThis happens if you have a local Postgres installation in your computer. To mitigate this, use a different port, like 5431, when creating the docker container, as in: -p 5431: 5432\nThen, we need to use this port when connecting to pgcli, as shown below:\npgcli -h localhost -p 5431 -u root -d ny_taxi\nThis will connect you to your postgres docker container, which is mapped to your host’s 5431 port (though you might choose any port of your liking as long as it is not occupied).\nFor a more visual and detailed explanation, feel free to check the video 1.4.2 - Port Mapping and Networks in Docker\nIf you want to debug: the following can help (on a MacOS)\nTo find out if something is blocking your port (on a MacOS):\nYou can use the lsof command to find out which application is using a specific port on your local machine. `lsof -i :5432`wi\nOr list the running postgres services on your local machine with launchctl\nTo unload the running service on your local machine (on a MacOS):\nunload the launch agent for the PostgreSQL service, which will stop the service and free up the port  \n`launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\nthis one to start it again\n`launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\nChanging port from 5432:5432 to 5431:5432 helped me to avoid this error.",
        "question": "What command can be used to check which application is using a specific port on a MacOS?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 102,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "FATAL:  password authentication failed for user \"root\"\nobservations: Below in bold do not forget the folder that was created ny_taxi_postgres_data\nThis happens if you have a local Postgres installation in your computer. To mitigate this, use a different port, like 5431, when creating the docker container, as in: -p 5431: 5432\nThen, we need to use this port when connecting to pgcli, as shown below:\npgcli -h localhost -p 5431 -u root -d ny_taxi\nThis will connect you to your postgres docker container, which is mapped to your host’s 5431 port (though you might choose any port of your liking as long as it is not occupied).\nFor a more visual and detailed explanation, feel free to check the video 1.4.2 - Port Mapping and Networks in Docker\nIf you want to debug: the following can help (on a MacOS)\nTo find out if something is blocking your port (on a MacOS):\nYou can use the lsof command to find out which application is using a specific port on your local machine. `lsof -i :5432`wi\nOr list the running postgres services on your local machine with launchctl\nTo unload the running service on your local machine (on a MacOS):\nunload the launch agent for the PostgreSQL service, which will stop the service and free up the port  \n`launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\nthis one to start it again\n`launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\nChanging port from 5432:5432 to 5431:5432 helped me to avoid this error.",
        "question": "How can you unload and reload the PostgreSQL launch agent on a MacOS?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 102,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I get this error\npgcli -h localhost -p 5432 -U root -d ny_taxi\nTraceback (most recent call last):\nFile \"/opt/anaconda3/bin/pgcli\", line 8, in <module>\nsys.exit(cli())\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\nreturn self.main(*args, **kwargs)\nFile \"/opt/anaconda3/lib/python3.9/sitYe-packages/click/core.py\", line\n1053, in main\nrv = self.invoke(ctx)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\nreturn ctx.invoke(self.callback, **ctx.params)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\nreturn __callback(*args, **kwargs)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\", line 880, in cli\nos.makedirs(config_dir)\nFile \"/opt/anaconda3/lib/python3.9/os.py\", line 225, in makedirspython\nmkdir(name, mode)PermissionError: [Errno 13] Permission denied: '/Users/vray/.config/pgcli'\nMake sure you install pgcli without sudo.\nThe recommended approach is to use conda/anaconda to make sure your system python is not affected.\nIf conda install gets stuck at \"Solving environment\" try these alternatives: https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda",
        "question": "What command was used to run pgcli?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 103,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I get this error\npgcli -h localhost -p 5432 -U root -d ny_taxi\nTraceback (most recent call last):\nFile \"/opt/anaconda3/bin/pgcli\", line 8, in <module>\nsys.exit(cli())\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\nreturn self.main(*args, **kwargs)\nFile \"/opt/anaconda3/lib/python3.9/sitYe-packages/click/core.py\", line\n1053, in main\nrv = self.invoke(ctx)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\nreturn ctx.invoke(self.callback, **ctx.params)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\nreturn __callback(*args, **kwargs)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\", line 880, in cli\nos.makedirs(config_dir)\nFile \"/opt/anaconda3/lib/python3.9/os.py\", line 225, in makedirspython\nmkdir(name, mode)PermissionError: [Errno 13] Permission denied: '/Users/vray/.config/pgcli'\nMake sure you install pgcli without sudo.\nThe recommended approach is to use conda/anaconda to make sure your system python is not affected.\nIf conda install gets stuck at \"Solving environment\" try these alternatives: https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda",
        "question": "What type of error occurred when attempting to run pgcli?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 103,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I get this error\npgcli -h localhost -p 5432 -U root -d ny_taxi\nTraceback (most recent call last):\nFile \"/opt/anaconda3/bin/pgcli\", line 8, in <module>\nsys.exit(cli())\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\nreturn self.main(*args, **kwargs)\nFile \"/opt/anaconda3/lib/python3.9/sitYe-packages/click/core.py\", line\n1053, in main\nrv = self.invoke(ctx)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\nreturn ctx.invoke(self.callback, **ctx.params)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\nreturn __callback(*args, **kwargs)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\", line 880, in cli\nos.makedirs(config_dir)\nFile \"/opt/anaconda3/lib/python3.9/os.py\", line 225, in makedirspython\nmkdir(name, mode)PermissionError: [Errno 13] Permission denied: '/Users/vray/.config/pgcli'\nMake sure you install pgcli without sudo.\nThe recommended approach is to use conda/anaconda to make sure your system python is not affected.\nIf conda install gets stuck at \"Solving environment\" try these alternatives: https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda",
        "question": "What is the specific error message related to permissions in the traceback?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 103,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I get this error\npgcli -h localhost -p 5432 -U root -d ny_taxi\nTraceback (most recent call last):\nFile \"/opt/anaconda3/bin/pgcli\", line 8, in <module>\nsys.exit(cli())\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\nreturn self.main(*args, **kwargs)\nFile \"/opt/anaconda3/lib/python3.9/sitYe-packages/click/core.py\", line\n1053, in main\nrv = self.invoke(ctx)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\nreturn ctx.invoke(self.callback, **ctx.params)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\nreturn __callback(*args, **kwargs)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\", line 880, in cli\nos.makedirs(config_dir)\nFile \"/opt/anaconda3/lib/python3.9/os.py\", line 225, in makedirspython\nmkdir(name, mode)PermissionError: [Errno 13] Permission denied: '/Users/vray/.config/pgcli'\nMake sure you install pgcli without sudo.\nThe recommended approach is to use conda/anaconda to make sure your system python is not affected.\nIf conda install gets stuck at \"Solving environment\" try these alternatives: https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda",
        "question": "What is the recommended approach for installing pgcli according to the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 103,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I get this error\npgcli -h localhost -p 5432 -U root -d ny_taxi\nTraceback (most recent call last):\nFile \"/opt/anaconda3/bin/pgcli\", line 8, in <module>\nsys.exit(cli())\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\nreturn self.main(*args, **kwargs)\nFile \"/opt/anaconda3/lib/python3.9/sitYe-packages/click/core.py\", line\n1053, in main\nrv = self.invoke(ctx)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\nreturn ctx.invoke(self.callback, **ctx.params)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\nreturn __callback(*args, **kwargs)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\", line 880, in cli\nos.makedirs(config_dir)\nFile \"/opt/anaconda3/lib/python3.9/os.py\", line 225, in makedirspython\nmkdir(name, mode)PermissionError: [Errno 13] Permission denied: '/Users/vray/.config/pgcli'\nMake sure you install pgcli without sudo.\nThe recommended approach is to use conda/anaconda to make sure your system python is not affected.\nIf conda install gets stuck at \"Solving environment\" try these alternatives: https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda",
        "question": "Where can users find alternatives if conda install gets stuck?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 103,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ImportError: no pq wrapper available.\nAttempts made:\n- couldn't import \\dt\nopg 'c' implementation: No module named 'psycopg_c'\n- couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'\n- couldn't import psycopg 'python' implementation: libpq library not found\nSolution:\nFirst, make sure your Python is set to 3.9, at least.\nAnd the reason for that is we have had cases of 'psycopg2-binary' failing to install because of an old version of Python (3.7.3). \n\n0. You can check your current python version with: \n$ python -V(the V must be capital)\n1. Based on the previous output, if you've got a 3.9, skip to Step #2\n   Otherwispye better off with a new environment with 3.9\n$ conda create –name de-zoomcamp python=3.9\n$ conda activate de-zoomcamp\n2. Next, you should be able to install the lib for postgres like this:\n```\n$ e\n$ pip install psycopg2_binary\n```\n3. Finally, make sure you're also installing pgcli, but use conda for that:\n```\n$ pgcli -h localhost -U root -d ny_taxisudo\n```\nThere, you should be good to go now!\nAnother solution:\nRun this\npip install \"psycopg[binary,pool]\"",
        "question": "What does the ImportError 'no pq wrapper available' indicate?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 104,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ImportError: no pq wrapper available.\nAttempts made:\n- couldn't import \\dt\nopg 'c' implementation: No module named 'psycopg_c'\n- couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'\n- couldn't import psycopg 'python' implementation: libpq library not found\nSolution:\nFirst, make sure your Python is set to 3.9, at least.\nAnd the reason for that is we have had cases of 'psycopg2-binary' failing to install because of an old version of Python (3.7.3). \n\n0. You can check your current python version with: \n$ python -V(the V must be capital)\n1. Based on the previous output, if you've got a 3.9, skip to Step #2\n   Otherwispye better off with a new environment with 3.9\n$ conda create –name de-zoomcamp python=3.9\n$ conda activate de-zoomcamp\n2. Next, you should be able to install the lib for postgres like this:\n```\n$ e\n$ pip install psycopg2_binary\n```\n3. Finally, make sure you're also installing pgcli, but use conda for that:\n```\n$ pgcli -h localhost -U root -d ny_taxisudo\n```\nThere, you should be good to go now!\nAnother solution:\nRun this\npip install \"psycopg[binary,pool]\"",
        "question": "What Python version is recommended for installing psycopg2-binary?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 104,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ImportError: no pq wrapper available.\nAttempts made:\n- couldn't import \\dt\nopg 'c' implementation: No module named 'psycopg_c'\n- couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'\n- couldn't import psycopg 'python' implementation: libpq library not found\nSolution:\nFirst, make sure your Python is set to 3.9, at least.\nAnd the reason for that is we have had cases of 'psycopg2-binary' failing to install because of an old version of Python (3.7.3). \n\n0. You can check your current python version with: \n$ python -V(the V must be capital)\n1. Based on the previous output, if you've got a 3.9, skip to Step #2\n   Otherwispye better off with a new environment with 3.9\n$ conda create –name de-zoomcamp python=3.9\n$ conda activate de-zoomcamp\n2. Next, you should be able to install the lib for postgres like this:\n```\n$ e\n$ pip install psycopg2_binary\n```\n3. Finally, make sure you're also installing pgcli, but use conda for that:\n```\n$ pgcli -h localhost -U root -d ny_taxisudo\n```\nThere, you should be good to go now!\nAnother solution:\nRun this\npip install \"psycopg[binary,pool]\"",
        "question": "What command can you use to check your current Python version?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 104,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ImportError: no pq wrapper available.\nAttempts made:\n- couldn't import \\dt\nopg 'c' implementation: No module named 'psycopg_c'\n- couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'\n- couldn't import psycopg 'python' implementation: libpq library not found\nSolution:\nFirst, make sure your Python is set to 3.9, at least.\nAnd the reason for that is we have had cases of 'psycopg2-binary' failing to install because of an old version of Python (3.7.3). \n\n0. You can check your current python version with: \n$ python -V(the V must be capital)\n1. Based on the previous output, if you've got a 3.9, skip to Step #2\n   Otherwispye better off with a new environment with 3.9\n$ conda create –name de-zoomcamp python=3.9\n$ conda activate de-zoomcamp\n2. Next, you should be able to install the lib for postgres like this:\n```\n$ e\n$ pip install psycopg2_binary\n```\n3. Finally, make sure you're also installing pgcli, but use conda for that:\n```\n$ pgcli -h localhost -U root -d ny_taxisudo\n```\nThere, you should be good to go now!\nAnother solution:\nRun this\npip install \"psycopg[binary,pool]\"",
        "question": "How can you create a new conda environment with Python 3.9?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 104,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ImportError: no pq wrapper available.\nAttempts made:\n- couldn't import \\dt\nopg 'c' implementation: No module named 'psycopg_c'\n- couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'\n- couldn't import psycopg 'python' implementation: libpq library not found\nSolution:\nFirst, make sure your Python is set to 3.9, at least.\nAnd the reason for that is we have had cases of 'psycopg2-binary' failing to install because of an old version of Python (3.7.3). \n\n0. You can check your current python version with: \n$ python -V(the V must be capital)\n1. Based on the previous output, if you've got a 3.9, skip to Step #2\n   Otherwispye better off with a new environment with 3.9\n$ conda create –name de-zoomcamp python=3.9\n$ conda activate de-zoomcamp\n2. Next, you should be able to install the lib for postgres like this:\n```\n$ e\n$ pip install psycopg2_binary\n```\n3. Finally, make sure you're also installing pgcli, but use conda for that:\n```\n$ pgcli -h localhost -U root -d ny_taxisudo\n```\nThere, you should be good to go now!\nAnother solution:\nRun this\npip install \"psycopg[binary,pool]\"",
        "question": "What command should be run to install psycopg2_binary?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 104,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If your Bash prompt is stuck on the password command for postgres\nUse winpty:\nwinpty pgcli -h localhost -p 5432 -u root -d ny_taxi\nAlternatively, try using Windows terminal or terminal in VS code.\nEditPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\nThe error above was faced continually despite inputting the correct password\nSolution\nOption 1: Stop the PostgreSQL service on Windows\nOption 2 (using WSL): Completely uninstall Protgres 12 from Windows and install postgresql-client on WSL (sudo apt install postgresql-client-common postgresql-client libpq-dev)\nOption 3: Change the port of the docker container\nNEW SOLUTION: 27/01/2024\nPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\nIf you’ve got the error above, it’s probably because you were just like me, closed the connection to the Postgres:13 image in the previous step of the tutorial, which is\n\ndocker run -it \\\n-e POSTGRES_USER=root \\\n-e POSTGRES_PASSWORD=root \\\n-e POSTGRES_DB=ny_taxi \\\n-v d:/git/data-engineering-zoomcamp/week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\nSo keep the database connected and you will be able to implement all the next steps of the tutorial.",
        "question": "What command should you use if your Bash prompt is stuck on the password command for postgres?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 105,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If your Bash prompt is stuck on the password command for postgres\nUse winpty:\nwinpty pgcli -h localhost -p 5432 -u root -d ny_taxi\nAlternatively, try using Windows terminal or terminal in VS code.\nEditPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\nThe error above was faced continually despite inputting the correct password\nSolution\nOption 1: Stop the PostgreSQL service on Windows\nOption 2 (using WSL): Completely uninstall Protgres 12 from Windows and install postgresql-client on WSL (sudo apt install postgresql-client-common postgresql-client libpq-dev)\nOption 3: Change the port of the docker container\nNEW SOLUTION: 27/01/2024\nPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\nIf you’ve got the error above, it’s probably because you were just like me, closed the connection to the Postgres:13 image in the previous step of the tutorial, which is\n\ndocker run -it \\\n-e POSTGRES_USER=root \\\n-e POSTGRES_PASSWORD=root \\\n-e POSTGRES_DB=ny_taxi \\\n-v d:/git/data-engineering-zoomcamp/week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\nSo keep the database connected and you will be able to implement all the next steps of the tutorial.",
        "question": "What are the steps to resolve the 'password authentication failed for user \"root\"' error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 105,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If your Bash prompt is stuck on the password command for postgres\nUse winpty:\nwinpty pgcli -h localhost -p 5432 -u root -d ny_taxi\nAlternatively, try using Windows terminal or terminal in VS code.\nEditPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\nThe error above was faced continually despite inputting the correct password\nSolution\nOption 1: Stop the PostgreSQL service on Windows\nOption 2 (using WSL): Completely uninstall Protgres 12 from Windows and install postgresql-client on WSL (sudo apt install postgresql-client-common postgresql-client libpq-dev)\nOption 3: Change the port of the docker container\nNEW SOLUTION: 27/01/2024\nPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\nIf you’ve got the error above, it’s probably because you were just like me, closed the connection to the Postgres:13 image in the previous step of the tutorial, which is\n\ndocker run -it \\\n-e POSTGRES_USER=root \\\n-e POSTGRES_PASSWORD=root \\\n-e POSTGRES_DB=ny_taxi \\\n-v d:/git/data-engineering-zoomcamp/week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\nSo keep the database connected and you will be able to implement all the next steps of the tutorial.",
        "question": "What is one option to stop the PostgreSQL service on Windows?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 105,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If your Bash prompt is stuck on the password command for postgres\nUse winpty:\nwinpty pgcli -h localhost -p 5432 -u root -d ny_taxi\nAlternatively, try using Windows terminal or terminal in VS code.\nEditPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\nThe error above was faced continually despite inputting the correct password\nSolution\nOption 1: Stop the PostgreSQL service on Windows\nOption 2 (using WSL): Completely uninstall Protgres 12 from Windows and install postgresql-client on WSL (sudo apt install postgresql-client-common postgresql-client libpq-dev)\nOption 3: Change the port of the docker container\nNEW SOLUTION: 27/01/2024\nPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\nIf you’ve got the error above, it’s probably because you were just like me, closed the connection to the Postgres:13 image in the previous step of the tutorial, which is\n\ndocker run -it \\\n-e POSTGRES_USER=root \\\n-e POSTGRES_PASSWORD=root \\\n-e POSTGRES_DB=ny_taxi \\\n-v d:/git/data-engineering-zoomcamp/week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\nSo keep the database connected and you will be able to implement all the next steps of the tutorial.",
        "question": "What do you need to uninstall when using WSL to resolve PostgreSQL connection issues?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 105,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If your Bash prompt is stuck on the password command for postgres\nUse winpty:\nwinpty pgcli -h localhost -p 5432 -u root -d ny_taxi\nAlternatively, try using Windows terminal or terminal in VS code.\nEditPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\nThe error above was faced continually despite inputting the correct password\nSolution\nOption 1: Stop the PostgreSQL service on Windows\nOption 2 (using WSL): Completely uninstall Protgres 12 from Windows and install postgresql-client on WSL (sudo apt install postgresql-client-common postgresql-client libpq-dev)\nOption 3: Change the port of the docker container\nNEW SOLUTION: 27/01/2024\nPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\nIf you’ve got the error above, it’s probably because you were just like me, closed the connection to the Postgres:13 image in the previous step of the tutorial, which is\n\ndocker run -it \\\n-e POSTGRES_USER=root \\\n-e POSTGRES_PASSWORD=root \\\n-e POSTGRES_DB=ny_taxi \\\n-v d:/git/data-engineering-zoomcamp/week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\nSo keep the database connected and you will be able to implement all the next steps of the tutorial.",
        "question": "How can you maintain the connection to the Postgres:13 image during the tutorial?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 105,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: If you have already installed pgcli but bash doesn't recognize pgcli\nOn Git bash: bash: pgcli: command not found\nOn Windows Terminal: pgcli: The term 'pgcli' is not recognized…\nSolution: Try adding a Python path C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\Scripts to Windows PATH\nFor details:\nGet the location: pip list -v\nCopy C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\site-packages\n3. Replace site-packages with Scripts: C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\Scripts\nIt can also be that you have Python installed elsewhere.\nFor me it was under c:\\python310\\lib\\site-packages\nSo I had to add c:\\python310\\lib\\Scripts to PATH, as shown below.\nPut the above path in \"Path\" (or \"PATH\") in System Variables\nReference: https://stackoverflow.com/a/68233660",
        "question": "What error message do you receive when pgcli is not recognized in Git bash?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 106,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: If you have already installed pgcli but bash doesn't recognize pgcli\nOn Git bash: bash: pgcli: command not found\nOn Windows Terminal: pgcli: The term 'pgcli' is not recognized…\nSolution: Try adding a Python path C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\Scripts to Windows PATH\nFor details:\nGet the location: pip list -v\nCopy C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\site-packages\n3. Replace site-packages with Scripts: C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\Scripts\nIt can also be that you have Python installed elsewhere.\nFor me it was under c:\\python310\\lib\\site-packages\nSo I had to add c:\\python310\\lib\\Scripts to PATH, as shown below.\nPut the above path in \"Path\" (or \"PATH\") in System Variables\nReference: https://stackoverflow.com/a/68233660",
        "question": "How can you add the Python path to the Windows PATH variable?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 106,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: If you have already installed pgcli but bash doesn't recognize pgcli\nOn Git bash: bash: pgcli: command not found\nOn Windows Terminal: pgcli: The term 'pgcli' is not recognized…\nSolution: Try adding a Python path C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\Scripts to Windows PATH\nFor details:\nGet the location: pip list -v\nCopy C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\site-packages\n3. Replace site-packages with Scripts: C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\Scripts\nIt can also be that you have Python installed elsewhere.\nFor me it was under c:\\python310\\lib\\site-packages\nSo I had to add c:\\python310\\lib\\Scripts to PATH, as shown below.\nPut the above path in \"Path\" (or \"PATH\") in System Variables\nReference: https://stackoverflow.com/a/68233660",
        "question": "What command can you use to find the location of installed packages?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 106,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: If you have already installed pgcli but bash doesn't recognize pgcli\nOn Git bash: bash: pgcli: command not found\nOn Windows Terminal: pgcli: The term 'pgcli' is not recognized…\nSolution: Try adding a Python path C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\Scripts to Windows PATH\nFor details:\nGet the location: pip list -v\nCopy C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\site-packages\n3. Replace site-packages with Scripts: C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\Scripts\nIt can also be that you have Python installed elsewhere.\nFor me it was under c:\\python310\\lib\\site-packages\nSo I had to add c:\\python310\\lib\\Scripts to PATH, as shown below.\nPut the above path in \"Path\" (or \"PATH\") in System Variables\nReference: https://stackoverflow.com/a/68233660",
        "question": "What might you need to do if Python is installed in a different directory?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 106,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: If you have already installed pgcli but bash doesn't recognize pgcli\nOn Git bash: bash: pgcli: command not found\nOn Windows Terminal: pgcli: The term 'pgcli' is not recognized…\nSolution: Try adding a Python path C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\Scripts to Windows PATH\nFor details:\nGet the location: pip list -v\nCopy C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\site-packages\n3. Replace site-packages with Scripts: C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\Scripts\nIt can also be that you have Python installed elsewhere.\nFor me it was under c:\\python310\\lib\\site-packages\nSo I had to add c:\\python310\\lib\\Scripts to PATH, as shown below.\nPut the above path in \"Path\" (or \"PATH\") in System Variables\nReference: https://stackoverflow.com/a/68233660",
        "question": "Where can you find more details about the issue and its solution?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 106,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In case running pgcli  locally causes issues or you do not want to install it locally you can use it running in a Docker container instead.\nBelow the usage with values used in the videos of the course for:\nnetwork name (docker network)\npostgres related variables for pgcli\nHostname\nUsername\nPort\nDatabase name\n$ docker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\n175dd47cda07:/# pgcli -h pg-database -U root -p 5432 -d ny_taxi\nPassword for root:\nServer: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\nVersion: 4.0.1\nHome: http://pgcli.com\nroot@pg-database:ny_taxi> \\dt\n+--------+------------------+-------+-------+\n| Schema | Name             | Type  | Owner |\n|--------+------------------+-------+-------|\n| public | yellow_taxi_data | table | root  |\n+--------+------------------+-------+-------+\nSELECT 1\nTime: 0.009s\nroot@pg-database:ny_taxi>",
        "question": "What is the purpose of using Docker to run pgcli?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 107,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In case running pgcli  locally causes issues or you do not want to install it locally you can use it running in a Docker container instead.\nBelow the usage with values used in the videos of the course for:\nnetwork name (docker network)\npostgres related variables for pgcli\nHostname\nUsername\nPort\nDatabase name\n$ docker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\n175dd47cda07:/# pgcli -h pg-database -U root -p 5432 -d ny_taxi\nPassword for root:\nServer: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\nVersion: 4.0.1\nHome: http://pgcli.com\nroot@pg-database:ny_taxi> \\dt\n+--------+------------------+-------+-------+\n| Schema | Name             | Type  | Owner |\n|--------+------------------+-------+-------|\n| public | yellow_taxi_data | table | root  |\n+--------+------------------+-------+-------+\nSELECT 1\nTime: 0.009s\nroot@pg-database:ny_taxi>",
        "question": "What command is used to run pgcli in a Docker container?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 107,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In case running pgcli  locally causes issues or you do not want to install it locally you can use it running in a Docker container instead.\nBelow the usage with values used in the videos of the course for:\nnetwork name (docker network)\npostgres related variables for pgcli\nHostname\nUsername\nPort\nDatabase name\n$ docker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\n175dd47cda07:/# pgcli -h pg-database -U root -p 5432 -d ny_taxi\nPassword for root:\nServer: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\nVersion: 4.0.1\nHome: http://pgcli.com\nroot@pg-database:ny_taxi> \\dt\n+--------+------------------+-------+-------+\n| Schema | Name             | Type  | Owner |\n|--------+------------------+-------+-------|\n| public | yellow_taxi_data | table | root  |\n+--------+------------------+-------+-------+\nSELECT 1\nTime: 0.009s\nroot@pg-database:ny_taxi>",
        "question": "What PostgreSQL version is mentioned in the provided text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 107,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In case running pgcli  locally causes issues or you do not want to install it locally you can use it running in a Docker container instead.\nBelow the usage with values used in the videos of the course for:\nnetwork name (docker network)\npostgres related variables for pgcli\nHostname\nUsername\nPort\nDatabase name\n$ docker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\n175dd47cda07:/# pgcli -h pg-database -U root -p 5432 -d ny_taxi\nPassword for root:\nServer: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\nVersion: 4.0.1\nHome: http://pgcli.com\nroot@pg-database:ny_taxi> \\dt\n+--------+------------------+-------+-------+\n| Schema | Name             | Type  | Owner |\n|--------+------------------+-------+-------|\n| public | yellow_taxi_data | table | root  |\n+--------+------------------+-------+-------+\nSELECT 1\nTime: 0.009s\nroot@pg-database:ny_taxi>",
        "question": "What username and database are used in the example pgcli command?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 107,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In case running pgcli  locally causes issues or you do not want to install it locally you can use it running in a Docker container instead.\nBelow the usage with values used in the videos of the course for:\nnetwork name (docker network)\npostgres related variables for pgcli\nHostname\nUsername\nPort\nDatabase name\n$ docker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\n175dd47cda07:/# pgcli -h pg-database -U root -p 5432 -d ny_taxi\nPassword for root:\nServer: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\nVersion: 4.0.1\nHome: http://pgcli.com\nroot@pg-database:ny_taxi> \\dt\n+--------+------------------+-------+-------+\n| Schema | Name             | Type  | Owner |\n|--------+------------------+-------+-------|\n| public | yellow_taxi_data | table | root  |\n+--------+------------------+-------+-------+\nSELECT 1\nTime: 0.009s\nroot@pg-database:ny_taxi>",
        "question": "How can you view the tables in the 'ny_taxi' database using pgcli?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 107,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "PULocationID will not be recognized but “PULocationID” will be. This is because unquoted \"Localidentifiers are case insensitive. See docs.",
        "question": "What is the significance of using quotes around 'PULocationID'?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 108,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "PULocationID will not be recognized but “PULocationID” will be. This is because unquoted \"Localidentifiers are case insensitive. See docs.",
        "question": "Why is 'PULocationID' recognized while PULocationID is not?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 108,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "PULocationID will not be recognized but “PULocationID” will be. This is because unquoted \"Localidentifiers are case insensitive. See docs.",
        "question": "What does it mean for local identifiers to be case insensitive?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 108,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "PULocationID will not be recognized but “PULocationID” will be. This is because unquoted \"Localidentifiers are case insensitive. See docs.",
        "question": "Where can more information about this topic be found?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 108,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "PULocationID will not be recognized but “PULocationID” will be. This is because unquoted \"Localidentifiers are case insensitive. See docs.",
        "question": "How does case sensitivity impact the recognition of identifiers?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 108,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When using the command `\\d <database name>` you get the error column `c.relhasoids does not exist`.\nResolution:\nUninstall pgcli\nReinstall pgclidatabase \"ny_taxi\" does not exist\nRestart pc",
        "question": "What command results in the error 'column c.relhasoids does not exist'?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 109,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When using the command `\\d <database name>` you get the error column `c.relhasoids does not exist`.\nResolution:\nUninstall pgcli\nReinstall pgclidatabase \"ny_taxi\" does not exist\nRestart pc",
        "question": "What is the first step suggested to resolve the error mentioned?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 109,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When using the command `\\d <database name>` you get the error column `c.relhasoids does not exist`.\nResolution:\nUninstall pgcli\nReinstall pgclidatabase \"ny_taxi\" does not exist\nRestart pc",
        "question": "What does the error indicate about the database in question?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 109,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When using the command `\\d <database name>` you get the error column `c.relhasoids does not exist`.\nResolution:\nUninstall pgcli\nReinstall pgclidatabase \"ny_taxi\" does not exist\nRestart pc",
        "question": "What should a user do if the database 'ny_taxi' does not exist?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 109,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When using the command `\\d <database name>` you get the error column `c.relhasoids does not exist`.\nResolution:\nUninstall pgcli\nReinstall pgclidatabase \"ny_taxi\" does not exist\nRestart pc",
        "question": "What is a suggested action to take after reinstalling pgcli?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 109,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens while uploading data via the connection in jupyter notebook\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nThe port 5432 was taken by another postgres. We are not connecting to the port in docker, but to the port on our machine. Substitute 5431 or whatever port you mapped to for port 5432.\nAlso if this error is still persistent , kindly check if you have a service in windows running postgres , Stopping that service will resolve the issue",
        "question": "What is the purpose of the 'create_engine' function in the context of the given text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 110,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens while uploading data via the connection in jupyter notebook\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nThe port 5432 was taken by another postgres. We are not connecting to the port in docker, but to the port on our machine. Substitute 5431 or whatever port you mapped to for port 5432.\nAlso if this error is still persistent , kindly check if you have a service in windows running postgres , Stopping that service will resolve the issue",
        "question": "What does the error indicate about port 5432 in the provided scenario?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 110,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens while uploading data via the connection in jupyter notebook\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nThe port 5432 was taken by another postgres. We are not connecting to the port in docker, but to the port on our machine. Substitute 5431 or whatever port you mapped to for port 5432.\nAlso if this error is still persistent , kindly check if you have a service in windows running postgres , Stopping that service will resolve the issue",
        "question": "If port 5432 is already in use, what alternative should be used as mentioned in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 110,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens while uploading data via the connection in jupyter notebook\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nThe port 5432 was taken by another postgres. We are not connecting to the port in docker, but to the port on our machine. Substitute 5431 or whatever port you mapped to for port 5432.\nAlso if this error is still persistent , kindly check if you have a service in windows running postgres , Stopping that service will resolve the issue",
        "question": "What steps are suggested if the error persists even after changing the port?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 110,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens while uploading data via the connection in jupyter notebook\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nThe port 5432 was taken by another postgres. We are not connecting to the port in docker, but to the port on our machine. Substitute 5431 or whatever port you mapped to for port 5432.\nAlso if this error is still persistent , kindly check if you have a service in windows running postgres , Stopping that service will resolve the issue",
        "question": "What role does the Windows service play in the issue described in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 110,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Can happen when connecting via pgcli\npgcli -h localhost -p 5432 -U root -d ny_taxi\nOr while uploading data via the connection in jupyter notebook\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nThis can happen when Postgres is already installed on your computer. Changing the port can resolve that (e.g. from 5432 to 5431).\nTo check whether there even is a root user with the ability to login:\nTry: docker exec -it <your_container_name> /bin/bash\nAnd then run\n???\nAlso, you could change port from 5432:5432 to 5431:5432\nOther solution that worked:\nChanging `POSTGRES_USER=juroot` to `PGUSER=postgres`\nBased on this: postgres with docker compose gives FATAL: role \"root\" does not exist error - Stack Overflow\nAlso `docker compose down`, removing folder that had postgres volume, running `docker compose up` again.",
        "question": "What command is used to connect to a PostgreSQL database using pgcli?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 111,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Can happen when connecting via pgcli\npgcli -h localhost -p 5432 -U root -d ny_taxi\nOr while uploading data via the connection in jupyter notebook\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nThis can happen when Postgres is already installed on your computer. Changing the port can resolve that (e.g. from 5432 to 5431).\nTo check whether there even is a root user with the ability to login:\nTry: docker exec -it <your_container_name> /bin/bash\nAnd then run\n???\nAlso, you could change port from 5432:5432 to 5431:5432\nOther solution that worked:\nChanging `POSTGRES_USER=juroot` to `PGUSER=postgres`\nBased on this: postgres with docker compose gives FATAL: role \"root\" does not exist error - Stack Overflow\nAlso `docker compose down`, removing folder that had postgres volume, running `docker compose up` again.",
        "question": "How can you change the port in a PostgreSQL connection string?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 111,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Can happen when connecting via pgcli\npgcli -h localhost -p 5432 -U root -d ny_taxi\nOr while uploading data via the connection in jupyter notebook\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nThis can happen when Postgres is already installed on your computer. Changing the port can resolve that (e.g. from 5432 to 5431).\nTo check whether there even is a root user with the ability to login:\nTry: docker exec -it <your_container_name> /bin/bash\nAnd then run\n???\nAlso, you could change port from 5432:5432 to 5431:5432\nOther solution that worked:\nChanging `POSTGRES_USER=juroot` to `PGUSER=postgres`\nBased on this: postgres with docker compose gives FATAL: role \"root\" does not exist error - Stack Overflow\nAlso `docker compose down`, removing folder that had postgres volume, running `docker compose up` again.",
        "question": "What command can be used to check if a root user can log in to a PostgreSQL instance in Docker?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 111,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Can happen when connecting via pgcli\npgcli -h localhost -p 5432 -U root -d ny_taxi\nOr while uploading data via the connection in jupyter notebook\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nThis can happen when Postgres is already installed on your computer. Changing the port can resolve that (e.g. from 5432 to 5431).\nTo check whether there even is a root user with the ability to login:\nTry: docker exec -it <your_container_name> /bin/bash\nAnd then run\n???\nAlso, you could change port from 5432:5432 to 5431:5432\nOther solution that worked:\nChanging `POSTGRES_USER=juroot` to `PGUSER=postgres`\nBased on this: postgres with docker compose gives FATAL: role \"root\" does not exist error - Stack Overflow\nAlso `docker compose down`, removing folder that had postgres volume, running `docker compose up` again.",
        "question": "What is a potential solution for resolving the FATAL: role 'root' does not exist error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 111,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Can happen when connecting via pgcli\npgcli -h localhost -p 5432 -U root -d ny_taxi\nOr while uploading data via the connection in jupyter notebook\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nThis can happen when Postgres is already installed on your computer. Changing the port can resolve that (e.g. from 5432 to 5431).\nTo check whether there even is a root user with the ability to login:\nTry: docker exec -it <your_container_name> /bin/bash\nAnd then run\n???\nAlso, you could change port from 5432:5432 to 5431:5432\nOther solution that worked:\nChanging `POSTGRES_USER=juroot` to `PGUSER=postgres`\nBased on this: postgres with docker compose gives FATAL: role \"root\" does not exist error - Stack Overflow\nAlso `docker compose down`, removing folder that had postgres volume, running `docker compose up` again.",
        "question": "What steps can you take to reset your PostgreSQL setup using Docker Compose?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 111,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "~\\anaconda3\\lib\\site-packages\\psycopg2\\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)\n120\n121     dsn = _ext.make_dsn(dsn, **kwargs)\n--> 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\n123     if cursor_factory is not None:\n124         conn.cursor_factory = cursor_factory\nOperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist\nMake sure postgres is running. You can check that by running `docker ps`\n✅Solution: If you have postgres software installed on your computer before now, build your instance on a different port like 8080 instead of 5432",
        "question": "What does the error message indicate about the database 'ny_taxi'?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 112,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "~\\anaconda3\\lib\\site-packages\\psycopg2\\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)\n120\n121     dsn = _ext.make_dsn(dsn, **kwargs)\n--> 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\n123     if cursor_factory is not None:\n124         conn.cursor_factory = cursor_factory\nOperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist\nMake sure postgres is running. You can check that by running `docker ps`\n✅Solution: If you have postgres software installed on your computer before now, build your instance on a different port like 8080 instead of 5432",
        "question": "What command can be used to check if PostgreSQL is running?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 112,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "~\\anaconda3\\lib\\site-packages\\psycopg2\\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)\n120\n121     dsn = _ext.make_dsn(dsn, **kwargs)\n--> 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\n123     if cursor_factory is not None:\n124         conn.cursor_factory = cursor_factory\nOperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist\nMake sure postgres is running. You can check that by running `docker ps`\n✅Solution: If you have postgres software installed on your computer before now, build your instance on a different port like 8080 instead of 5432",
        "question": "What does the connection attempt to 'localhost' on port 5432 signify?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 112,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "~\\anaconda3\\lib\\site-packages\\psycopg2\\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)\n120\n121     dsn = _ext.make_dsn(dsn, **kwargs)\n--> 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\n123     if cursor_factory is not None:\n124         conn.cursor_factory = cursor_factory\nOperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist\nMake sure postgres is running. You can check that by running `docker ps`\n✅Solution: If you have postgres software installed on your computer before now, build your instance on a different port like 8080 instead of 5432",
        "question": "What is one proposed solution if the 'ny_taxi' database does not exist?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 112,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "~\\anaconda3\\lib\\site-packages\\psycopg2\\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)\n120\n121     dsn = _ext.make_dsn(dsn, **kwargs)\n--> 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\n123     if cursor_factory is not None:\n124         conn.cursor_factory = cursor_factory\nOperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist\nMake sure postgres is running. You can check that by running `docker ps`\n✅Solution: If you have postgres software installed on your computer before now, build your instance on a different port like 8080 instead of 5432",
        "question": "What is the significance of using a different port like 8080 instead of 5432?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 112,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue:\ne…\nSolution:\npip install psycopg2-binary\nIf you already have it, you might need to update it:\npip install psycopg2-binary --upgrade\nOther methods, if the above fails:\nif you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\nFirst uninstall the psycopg package\nThen update conda or pip\nThen install psycopg again using pip.\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql",
        "question": "What command should be used to install psycopg2-binary?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 113,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue:\ne…\nSolution:\npip install psycopg2-binary\nIf you already have it, you might need to update it:\npip install psycopg2-binary --upgrade\nOther methods, if the above fails:\nif you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\nFirst uninstall the psycopg package\nThen update conda or pip\nThen install psycopg again using pip.\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql",
        "question": "What steps should be taken if the ModuleNotFoundError occurs after installation?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 113,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue:\ne…\nSolution:\npip install psycopg2-binary\nIf you already have it, you might need to update it:\npip install psycopg2-binary --upgrade\nOther methods, if the above fails:\nif you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\nFirst uninstall the psycopg package\nThen update conda or pip\nThen install psycopg again using pip.\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql",
        "question": "How can you update conda to resolve issues with psycopg2?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 113,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue:\ne…\nSolution:\npip install psycopg2-binary\nIf you already have it, you might need to update it:\npip install psycopg2-binary --upgrade\nOther methods, if the above fails:\nif you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\nFirst uninstall the psycopg package\nThen update conda or pip\nThen install psycopg again using pip.\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql",
        "question": "What should you do if you encounter the pg_config not found error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 113,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue:\ne…\nSolution:\npip install psycopg2-binary\nIf you already have it, you might need to update it:\npip install psycopg2-binary --upgrade\nOther methods, if the above fails:\nif you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\nFirst uninstall the psycopg package\nThen update conda or pip\nThen install psycopg again using pip.\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql",
        "question": "What is the recommended installation command for PostgreSQL on a MAC?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 113,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the join queries, if we mention the column name directly or enclosed in single quotes it’ll throw an error says “column does not exist”.\n✅Solution: But if we enclose the column names in double quotes then it will work",
        "question": "What error is thrown if column names are mentioned directly or in single quotes in join queries?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 114,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the join queries, if we mention the column name directly or enclosed in single quotes it’ll throw an error says “column does not exist”.\n✅Solution: But if we enclose the column names in double quotes then it will work",
        "question": "What is the solution to the error encountered when using column names in join queries?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 114,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the join queries, if we mention the column name directly or enclosed in single quotes it’ll throw an error says “column does not exist”.\n✅Solution: But if we enclose the column names in double quotes then it will work",
        "question": "How should column names be enclosed to avoid the 'column does not exist' error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 114,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the join queries, if we mention the column name directly or enclosed in single quotes it’ll throw an error says “column does not exist”.\n✅Solution: But if we enclose the column names in double quotes then it will work",
        "question": "What happens if column names are enclosed in double quotes in join queries?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 114,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the join queries, if we mention the column name directly or enclosed in single quotes it’ll throw an error says “column does not exist”.\n✅Solution: But if we enclose the column names in double quotes then it will work",
        "question": "What type of queries is discussed in the provided text regarding column name usage?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 114,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "pgAdmin has a new version. Create server dialog may not appear. Try using register-> server instead.",
        "question": "What is the new feature in pgAdmin?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 115,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "pgAdmin has a new version. Create server dialog may not appear. Try using register-> server instead.",
        "question": "What issue might occur when starting the new version of pgAdmin?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 115,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "pgAdmin has a new version. Create server dialog may not appear. Try using register-> server instead.",
        "question": "How can users create a server in the new version of pgAdmin?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 115,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "pgAdmin has a new version. Create server dialog may not appear. Try using register-> server instead.",
        "question": "What is the alternative method to access the server dialog in pgAdmin?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 115,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "pgAdmin has a new version. Create server dialog may not appear. Try using register-> server instead.",
        "question": "Is the create server dialog a guaranteed feature in the new version of pgAdmin?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 115,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Using GitHub Codespaces in the browser resulted in a blank screen after the login to pgAdmin (running in a Docker container). The terminal of the pgAdmin container was showing the following error message:\nCSRFError: 400 Bad Request: The referrer does not match the host.\nSolution #1:\nAs recommended in the following issue  https://github.com/pgadmin-org/pgadmin4/issues/5432 setting the following environment variable solved it.\nPGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\"\nModified “docker run” command\ndocker run --rm -it \\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n-e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\n-p \"8080:80\" \\\n--name pgadmin \\\n--network=pg-network \\\ndpage/pgadmin4:8.2\nSolution #2:\nUsing the local installed VSCode to display GitHub Codespaces.\nWhen using GitHub Codespaces in the locally installed VSCode (opening a Codespace or creating/starting one) this issue did not occur.",
        "question": "What error message was displayed in the terminal of the pgAdmin container?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 116,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Using GitHub Codespaces in the browser resulted in a blank screen after the login to pgAdmin (running in a Docker container). The terminal of the pgAdmin container was showing the following error message:\nCSRFError: 400 Bad Request: The referrer does not match the host.\nSolution #1:\nAs recommended in the following issue  https://github.com/pgadmin-org/pgadmin4/issues/5432 setting the following environment variable solved it.\nPGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\"\nModified “docker run” command\ndocker run --rm -it \\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n-e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\n-p \"8080:80\" \\\n--name pgadmin \\\n--network=pg-network \\\ndpage/pgadmin4:8.2\nSolution #2:\nUsing the local installed VSCode to display GitHub Codespaces.\nWhen using GitHub Codespaces in the locally installed VSCode (opening a Codespace or creating/starting one) this issue did not occur.",
        "question": "What environment variable was set to resolve the CSRF error in pgAdmin?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 116,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Using GitHub Codespaces in the browser resulted in a blank screen after the login to pgAdmin (running in a Docker container). The terminal of the pgAdmin container was showing the following error message:\nCSRFError: 400 Bad Request: The referrer does not match the host.\nSolution #1:\nAs recommended in the following issue  https://github.com/pgadmin-org/pgadmin4/issues/5432 setting the following environment variable solved it.\nPGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\"\nModified “docker run” command\ndocker run --rm -it \\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n-e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\n-p \"8080:80\" \\\n--name pgadmin \\\n--network=pg-network \\\ndpage/pgadmin4:8.2\nSolution #2:\nUsing the local installed VSCode to display GitHub Codespaces.\nWhen using GitHub Codespaces in the locally installed VSCode (opening a Codespace or creating/starting one) this issue did not occur.",
        "question": "What command was modified to run the pgAdmin container with the required settings?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 116,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Using GitHub Codespaces in the browser resulted in a blank screen after the login to pgAdmin (running in a Docker container). The terminal of the pgAdmin container was showing the following error message:\nCSRFError: 400 Bad Request: The referrer does not match the host.\nSolution #1:\nAs recommended in the following issue  https://github.com/pgadmin-org/pgadmin4/issues/5432 setting the following environment variable solved it.\nPGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\"\nModified “docker run” command\ndocker run --rm -it \\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n-e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\n-p \"8080:80\" \\\n--name pgadmin \\\n--network=pg-network \\\ndpage/pgadmin4:8.2\nSolution #2:\nUsing the local installed VSCode to display GitHub Codespaces.\nWhen using GitHub Codespaces in the locally installed VSCode (opening a Codespace or creating/starting one) this issue did not occur.",
        "question": "What alternative solution was suggested for using GitHub Codespaces without encountering the error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 116,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Using GitHub Codespaces in the browser resulted in a blank screen after the login to pgAdmin (running in a Docker container). The terminal of the pgAdmin container was showing the following error message:\nCSRFError: 400 Bad Request: The referrer does not match the host.\nSolution #1:\nAs recommended in the following issue  https://github.com/pgadmin-org/pgadmin4/issues/5432 setting the following environment variable solved it.\nPGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\"\nModified “docker run” command\ndocker run --rm -it \\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n-e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\n-p \"8080:80\" \\\n--name pgadmin \\\n--network=pg-network \\\ndpage/pgadmin4:8.2\nSolution #2:\nUsing the local installed VSCode to display GitHub Codespaces.\nWhen using GitHub Codespaces in the locally installed VSCode (opening a Codespace or creating/starting one) this issue did not occur.",
        "question": "What does the error 'CSRFError: 400 Bad Request: The referrer does not match the host' indicate?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 116,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I am using a Mac Pro device and connect to the GCP Compute Engine via Remote SSH - VSCode. But when I trying to run the PgAdmin container via docker run or docker compose command, I am failed to access the pgAdmin address via my browser. I have switched to another browser, but still can not access the pgAdmin address. So I modified a little bit the configuration from the previous DE Zoomcamp repository like below and can access the pgAdmin address:\nSolution #1:\nModified “docker run” command\ndocker run --rm -it \\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n-e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\n-e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\n-e PGADMIN_LISTEN_PORT=5050 \\\n-p 5050:5050 \\\n--network=de-zoomcamp-network \\\n--name pgadmin-container \\\n--link postgres-container \\\n-t dpage/pgadmin4\nSolution #2:\nModified docker-compose.yaml configuration (via “docker compose up” command)\npgadmin:\nimage: dpage/pgadmin4\ncontainer_name: pgadmin-conntainer\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\n- PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\n- PGADMIN_LISTEN_ADDRESS=0.0.0.0\n- PGADMIN_LISTEN_PORT=5050\nvolumes:\n- \"./pgadmin_data:/var/lib/pgadmin/data\"\nports:\n- \"5050:5050\"\nnetworks:\n- de-zoomcamp-network\ndepends_on:\n- postgres-conntainer\nPython - ModuleNotFoundError: No module named 'pysqlite2'\nImportError: DLL load failed while importing _sqlite3: The specified module could not be found. ModuleNotFoundError: No module named 'pysqlite2'\nThe issue seems to arise from the missing of sqlite3.dll in path \".\\Anaconda\\Dlls\\\".\n✅I solved it by simply copying that .dll file from \\Anaconda3\\Library\\bin and put it under the path mentioned above. (if you are using anaconda)",
        "question": "What is the main issue encountered when trying to access the pgAdmin address via the browser?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 117,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I am using a Mac Pro device and connect to the GCP Compute Engine via Remote SSH - VSCode. But when I trying to run the PgAdmin container via docker run or docker compose command, I am failed to access the pgAdmin address via my browser. I have switched to another browser, but still can not access the pgAdmin address. So I modified a little bit the configuration from the previous DE Zoomcamp repository like below and can access the pgAdmin address:\nSolution #1:\nModified “docker run” command\ndocker run --rm -it \\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n-e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\n-e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\n-e PGADMIN_LISTEN_PORT=5050 \\\n-p 5050:5050 \\\n--network=de-zoomcamp-network \\\n--name pgadmin-container \\\n--link postgres-container \\\n-t dpage/pgadmin4\nSolution #2:\nModified docker-compose.yaml configuration (via “docker compose up” command)\npgadmin:\nimage: dpage/pgadmin4\ncontainer_name: pgadmin-conntainer\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\n- PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\n- PGADMIN_LISTEN_ADDRESS=0.0.0.0\n- PGADMIN_LISTEN_PORT=5050\nvolumes:\n- \"./pgadmin_data:/var/lib/pgadmin/data\"\nports:\n- \"5050:5050\"\nnetworks:\n- de-zoomcamp-network\ndepends_on:\n- postgres-conntainer\nPython - ModuleNotFoundError: No module named 'pysqlite2'\nImportError: DLL load failed while importing _sqlite3: The specified module could not be found. ModuleNotFoundError: No module named 'pysqlite2'\nThe issue seems to arise from the missing of sqlite3.dll in path \".\\Anaconda\\Dlls\\\".\n✅I solved it by simply copying that .dll file from \\Anaconda3\\Library\\bin and put it under the path mentioned above. (if you are using anaconda)",
        "question": "What modifications were made to the 'docker run' command to successfully access pgAdmin?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 117,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I am using a Mac Pro device and connect to the GCP Compute Engine via Remote SSH - VSCode. But when I trying to run the PgAdmin container via docker run or docker compose command, I am failed to access the pgAdmin address via my browser. I have switched to another browser, but still can not access the pgAdmin address. So I modified a little bit the configuration from the previous DE Zoomcamp repository like below and can access the pgAdmin address:\nSolution #1:\nModified “docker run” command\ndocker run --rm -it \\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n-e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\n-e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\n-e PGADMIN_LISTEN_PORT=5050 \\\n-p 5050:5050 \\\n--network=de-zoomcamp-network \\\n--name pgadmin-container \\\n--link postgres-container \\\n-t dpage/pgadmin4\nSolution #2:\nModified docker-compose.yaml configuration (via “docker compose up” command)\npgadmin:\nimage: dpage/pgadmin4\ncontainer_name: pgadmin-conntainer\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\n- PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\n- PGADMIN_LISTEN_ADDRESS=0.0.0.0\n- PGADMIN_LISTEN_PORT=5050\nvolumes:\n- \"./pgadmin_data:/var/lib/pgadmin/data\"\nports:\n- \"5050:5050\"\nnetworks:\n- de-zoomcamp-network\ndepends_on:\n- postgres-conntainer\nPython - ModuleNotFoundError: No module named 'pysqlite2'\nImportError: DLL load failed while importing _sqlite3: The specified module could not be found. ModuleNotFoundError: No module named 'pysqlite2'\nThe issue seems to arise from the missing of sqlite3.dll in path \".\\Anaconda\\Dlls\\\".\n✅I solved it by simply copying that .dll file from \\Anaconda3\\Library\\bin and put it under the path mentioned above. (if you are using anaconda)",
        "question": "How is the configuration for pgAdmin altered in the 'docker-compose.yaml' file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 117,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I am using a Mac Pro device and connect to the GCP Compute Engine via Remote SSH - VSCode. But when I trying to run the PgAdmin container via docker run or docker compose command, I am failed to access the pgAdmin address via my browser. I have switched to another browser, but still can not access the pgAdmin address. So I modified a little bit the configuration from the previous DE Zoomcamp repository like below and can access the pgAdmin address:\nSolution #1:\nModified “docker run” command\ndocker run --rm -it \\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n-e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\n-e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\n-e PGADMIN_LISTEN_PORT=5050 \\\n-p 5050:5050 \\\n--network=de-zoomcamp-network \\\n--name pgadmin-container \\\n--link postgres-container \\\n-t dpage/pgadmin4\nSolution #2:\nModified docker-compose.yaml configuration (via “docker compose up” command)\npgadmin:\nimage: dpage/pgadmin4\ncontainer_name: pgadmin-conntainer\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\n- PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\n- PGADMIN_LISTEN_ADDRESS=0.0.0.0\n- PGADMIN_LISTEN_PORT=5050\nvolumes:\n- \"./pgadmin_data:/var/lib/pgadmin/data\"\nports:\n- \"5050:5050\"\nnetworks:\n- de-zoomcamp-network\ndepends_on:\n- postgres-conntainer\nPython - ModuleNotFoundError: No module named 'pysqlite2'\nImportError: DLL load failed while importing _sqlite3: The specified module could not be found. ModuleNotFoundError: No module named 'pysqlite2'\nThe issue seems to arise from the missing of sqlite3.dll in path \".\\Anaconda\\Dlls\\\".\n✅I solved it by simply copying that .dll file from \\Anaconda3\\Library\\bin and put it under the path mentioned above. (if you are using anaconda)",
        "question": "What error messages were encountered related to Python and SQLite during the process?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 117,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I am using a Mac Pro device and connect to the GCP Compute Engine via Remote SSH - VSCode. But when I trying to run the PgAdmin container via docker run or docker compose command, I am failed to access the pgAdmin address via my browser. I have switched to another browser, but still can not access the pgAdmin address. So I modified a little bit the configuration from the previous DE Zoomcamp repository like below and can access the pgAdmin address:\nSolution #1:\nModified “docker run” command\ndocker run --rm -it \\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n-e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\n-e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\n-e PGADMIN_LISTEN_PORT=5050 \\\n-p 5050:5050 \\\n--network=de-zoomcamp-network \\\n--name pgadmin-container \\\n--link postgres-container \\\n-t dpage/pgadmin4\nSolution #2:\nModified docker-compose.yaml configuration (via “docker compose up” command)\npgadmin:\nimage: dpage/pgadmin4\ncontainer_name: pgadmin-conntainer\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\n- PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\n- PGADMIN_LISTEN_ADDRESS=0.0.0.0\n- PGADMIN_LISTEN_PORT=5050\nvolumes:\n- \"./pgadmin_data:/var/lib/pgadmin/data\"\nports:\n- \"5050:5050\"\nnetworks:\n- de-zoomcamp-network\ndepends_on:\n- postgres-conntainer\nPython - ModuleNotFoundError: No module named 'pysqlite2'\nImportError: DLL load failed while importing _sqlite3: The specified module could not be found. ModuleNotFoundError: No module named 'pysqlite2'\nThe issue seems to arise from the missing of sqlite3.dll in path \".\\Anaconda\\Dlls\\\".\n✅I solved it by simply copying that .dll file from \\Anaconda3\\Library\\bin and put it under the path mentioned above. (if you are using anaconda)",
        "question": "How was the issue of the missing 'sqlite3.dll' file resolved?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 117,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you follow the video 1.2.2 - Ingesting NY Taxi Data to Postgres and you execute all the same\nsteps as Alexey does, you will ingest all the data (~1.3 million rows) into the table yellow_taxi_data as expected.\nHowever, if you try to run the whole script in the Jupyter notebook for a second time from top to bottom, you will be missing the first chunk of 100000 records. This is because there is a call to the iterator before the while loop that puts the data in the table. The while loop therefore starts by ingesting the second chunk, not the first.\n✅Solution: remove the cell “df=next(df_iter)” that appears higher up in the notebook than the while loop. The first time w(df_iter) is called should be within the while loop.\n📔Note: As this notebook is just used as a way to test the code, it was not intended to be run top to bottom, and the logic is tidied up in a later step when it is instead inserted into a .py file for the pipeline",
        "question": "What happens if the script is run a second time in the Jupyter notebook?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 118,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you follow the video 1.2.2 - Ingesting NY Taxi Data to Postgres and you execute all the same\nsteps as Alexey does, you will ingest all the data (~1.3 million rows) into the table yellow_taxi_data as expected.\nHowever, if you try to run the whole script in the Jupyter notebook for a second time from top to bottom, you will be missing the first chunk of 100000 records. This is because there is a call to the iterator before the while loop that puts the data in the table. The while loop therefore starts by ingesting the second chunk, not the first.\n✅Solution: remove the cell “df=next(df_iter)” that appears higher up in the notebook than the while loop. The first time w(df_iter) is called should be within the while loop.\n📔Note: As this notebook is just used as a way to test the code, it was not intended to be run top to bottom, and the logic is tidied up in a later step when it is instead inserted into a .py file for the pipeline",
        "question": "Why is the first chunk of 100000 records missing upon rerunning the script?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 118,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you follow the video 1.2.2 - Ingesting NY Taxi Data to Postgres and you execute all the same\nsteps as Alexey does, you will ingest all the data (~1.3 million rows) into the table yellow_taxi_data as expected.\nHowever, if you try to run the whole script in the Jupyter notebook for a second time from top to bottom, you will be missing the first chunk of 100000 records. This is because there is a call to the iterator before the while loop that puts the data in the table. The while loop therefore starts by ingesting the second chunk, not the first.\n✅Solution: remove the cell “df=next(df_iter)” that appears higher up in the notebook than the while loop. The first time w(df_iter) is called should be within the while loop.\n📔Note: As this notebook is just used as a way to test the code, it was not intended to be run top to bottom, and the logic is tidied up in a later step when it is instead inserted into a .py file for the pipeline",
        "question": "What specific cell needs to be removed to fix the issue with data ingestion?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 118,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you follow the video 1.2.2 - Ingesting NY Taxi Data to Postgres and you execute all the same\nsteps as Alexey does, you will ingest all the data (~1.3 million rows) into the table yellow_taxi_data as expected.\nHowever, if you try to run the whole script in the Jupyter notebook for a second time from top to bottom, you will be missing the first chunk of 100000 records. This is because there is a call to the iterator before the while loop that puts the data in the table. The while loop therefore starts by ingesting the second chunk, not the first.\n✅Solution: remove the cell “df=next(df_iter)” that appears higher up in the notebook than the while loop. The first time w(df_iter) is called should be within the while loop.\n📔Note: As this notebook is just used as a way to test the code, it was not intended to be run top to bottom, and the logic is tidied up in a later step when it is instead inserted into a .py file for the pipeline",
        "question": "How many rows of data are being ingested into the table yellow_taxi_data?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 118,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you follow the video 1.2.2 - Ingesting NY Taxi Data to Postgres and you execute all the same\nsteps as Alexey does, you will ingest all the data (~1.3 million rows) into the table yellow_taxi_data as expected.\nHowever, if you try to run the whole script in the Jupyter notebook for a second time from top to bottom, you will be missing the first chunk of 100000 records. This is because there is a call to the iterator before the while loop that puts the data in the table. The while loop therefore starts by ingesting the second chunk, not the first.\n✅Solution: remove the cell “df=next(df_iter)” that appears higher up in the notebook than the while loop. The first time w(df_iter) is called should be within the while loop.\n📔Note: As this notebook is just used as a way to test the code, it was not intended to be run top to bottom, and the logic is tidied up in a later step when it is instead inserted into a .py file for the pipeline",
        "question": "What is the intended purpose of the Jupyter notebook according to the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 118,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "{t_end - t_start} seconds\")\nimport pandas as pd\ndf = pd.read_csv('path/to/file.csv.gz', /app/ingest_data.py:1: DeprecationWarning:)\nIf you prefer to keep the uncompressed csv (easier preview in vscode and similar), gzip files can be unzipped using gunzip (but not unzip). On a Ubuntu local or virtual machine, you may need to apt-get install gunzip first.",
        "question": "What is the recommended method to handle gzip files mentioned in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 119,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "{t_end - t_start} seconds\")\nimport pandas as pd\ndf = pd.read_csv('path/to/file.csv.gz', /app/ingest_data.py:1: DeprecationWarning:)\nIf you prefer to keep the uncompressed csv (easier preview in vscode and similar), gzip files can be unzipped using gunzip (but not unzip). On a Ubuntu local or virtual machine, you may need to apt-get install gunzip first.",
        "question": "What package is used to read the CSV file in the provided code?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 119,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "{t_end - t_start} seconds\")\nimport pandas as pd\ndf = pd.read_csv('path/to/file.csv.gz', /app/ingest_data.py:1: DeprecationWarning:)\nIf you prefer to keep the uncompressed csv (easier preview in vscode and similar), gzip files can be unzipped using gunzip (but not unzip). On a Ubuntu local or virtual machine, you may need to apt-get install gunzip first.",
        "question": "What warning is encountered when importing the CSV file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 119,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "{t_end - t_start} seconds\")\nimport pandas as pd\ndf = pd.read_csv('path/to/file.csv.gz', /app/ingest_data.py:1: DeprecationWarning:)\nIf you prefer to keep the uncompressed csv (easier preview in vscode and similar), gzip files can be unzipped using gunzip (but not unzip). On a Ubuntu local or virtual machine, you may need to apt-get install gunzip first.",
        "question": "What is a potential benefit of keeping the uncompressed CSV file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 119,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "{t_end - t_start} seconds\")\nimport pandas as pd\ndf = pd.read_csv('path/to/file.csv.gz', /app/ingest_data.py:1: DeprecationWarning:)\nIf you prefer to keep the uncompressed csv (easier preview in vscode and similar), gzip files can be unzipped using gunzip (but not unzip). On a Ubuntu local or virtual machine, you may need to apt-get install gunzip first.",
        "question": "What command might you need to run on a Ubuntu machine to use gunzip?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 119,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Pandas can interpret “string” column values as “datetime” directly when reading the CSV file using “pd.read_csv” using the parameter “parse_dates”, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\npandas.read_csv — pandas 2.1.4 documentation (pydata.org)\nExample from week 1\nimport pandas as pd\ndf = pd.read_csv(\n'yellow_tripdata_2021-01.csv',\nnrows=100,\nparse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\ndf.info()\nwhich will output\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100 entries, 0 to 99\nData columns (total 18 columns):\n#   Column                 Non-Null Count  Dtype\n---  ------                 --------------  -----\n0   VendorID               100 non-null    int64\n1   tpep_pickup_datetime   100 non-null    datetime64[ns]\n2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\n3   passenger_count        100 non-null    int64\n4   trip_distance          100 non-null    float64\n5   RatecodeID             100 non-null    int64\n6   store_and_fwd_flag     100 non-null    object\n7   PULocationID           100 non-null    int64\n8   DOLocationID           100 non-null    int64\n9   payment_type           100 non-null    int64\n10  fare_amount            100 non-null    float64\n11  extra                  100 non-null    float64\n12  mta_tax                100 non-null    float64\n13  tip_amount             100 non-null    float64\n14  tolls_amount           100 non-null    float64\n15  improvement_surcharge  100 non-null    float64\n16  total_amount           100 non-null    float64\n17  congestion_surcharge   100 non-null    float64\ndtypes: datetime64[ns](2), float64(9), int64(6), object(1)\nmemory usage: 14.2+ KB",
        "question": "What parameter is used in pd.read_csv to convert string column values to datetime directly?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 120,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Pandas can interpret “string” column values as “datetime” directly when reading the CSV file using “pd.read_csv” using the parameter “parse_dates”, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\npandas.read_csv — pandas 2.1.4 documentation (pydata.org)\nExample from week 1\nimport pandas as pd\ndf = pd.read_csv(\n'yellow_tripdata_2021-01.csv',\nnrows=100,\nparse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\ndf.info()\nwhich will output\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100 entries, 0 to 99\nData columns (total 18 columns):\n#   Column                 Non-Null Count  Dtype\n---  ------                 --------------  -----\n0   VendorID               100 non-null    int64\n1   tpep_pickup_datetime   100 non-null    datetime64[ns]\n2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\n3   passenger_count        100 non-null    int64\n4   trip_distance          100 non-null    float64\n5   RatecodeID             100 non-null    int64\n6   store_and_fwd_flag     100 non-null    object\n7   PULocationID           100 non-null    int64\n8   DOLocationID           100 non-null    int64\n9   payment_type           100 non-null    int64\n10  fare_amount            100 non-null    float64\n11  extra                  100 non-null    float64\n12  mta_tax                100 non-null    float64\n13  tip_amount             100 non-null    float64\n14  tolls_amount           100 non-null    float64\n15  improvement_surcharge  100 non-null    float64\n16  total_amount           100 non-null    float64\n17  congestion_surcharge   100 non-null    float64\ndtypes: datetime64[ns](2), float64(9), int64(6), object(1)\nmemory usage: 14.2+ KB",
        "question": "How many entries are in the DataFrame after reading the CSV file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 120,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Pandas can interpret “string” column values as “datetime” directly when reading the CSV file using “pd.read_csv” using the parameter “parse_dates”, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\npandas.read_csv — pandas 2.1.4 documentation (pydata.org)\nExample from week 1\nimport pandas as pd\ndf = pd.read_csv(\n'yellow_tripdata_2021-01.csv',\nnrows=100,\nparse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\ndf.info()\nwhich will output\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100 entries, 0 to 99\nData columns (total 18 columns):\n#   Column                 Non-Null Count  Dtype\n---  ------                 --------------  -----\n0   VendorID               100 non-null    int64\n1   tpep_pickup_datetime   100 non-null    datetime64[ns]\n2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\n3   passenger_count        100 non-null    int64\n4   trip_distance          100 non-null    float64\n5   RatecodeID             100 non-null    int64\n6   store_and_fwd_flag     100 non-null    object\n7   PULocationID           100 non-null    int64\n8   DOLocationID           100 non-null    int64\n9   payment_type           100 non-null    int64\n10  fare_amount            100 non-null    float64\n11  extra                  100 non-null    float64\n12  mta_tax                100 non-null    float64\n13  tip_amount             100 non-null    float64\n14  tolls_amount           100 non-null    float64\n15  improvement_surcharge  100 non-null    float64\n16  total_amount           100 non-null    float64\n17  congestion_surcharge   100 non-null    float64\ndtypes: datetime64[ns](2), float64(9), int64(6), object(1)\nmemory usage: 14.2+ KB",
        "question": "What are the names of the datetime columns specified in the parse_dates parameter?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 120,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Pandas can interpret “string” column values as “datetime” directly when reading the CSV file using “pd.read_csv” using the parameter “parse_dates”, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\npandas.read_csv — pandas 2.1.4 documentation (pydata.org)\nExample from week 1\nimport pandas as pd\ndf = pd.read_csv(\n'yellow_tripdata_2021-01.csv',\nnrows=100,\nparse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\ndf.info()\nwhich will output\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100 entries, 0 to 99\nData columns (total 18 columns):\n#   Column                 Non-Null Count  Dtype\n---  ------                 --------------  -----\n0   VendorID               100 non-null    int64\n1   tpep_pickup_datetime   100 non-null    datetime64[ns]\n2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\n3   passenger_count        100 non-null    int64\n4   trip_distance          100 non-null    float64\n5   RatecodeID             100 non-null    int64\n6   store_and_fwd_flag     100 non-null    object\n7   PULocationID           100 non-null    int64\n8   DOLocationID           100 non-null    int64\n9   payment_type           100 non-null    int64\n10  fare_amount            100 non-null    float64\n11  extra                  100 non-null    float64\n12  mta_tax                100 non-null    float64\n13  tip_amount             100 non-null    float64\n14  tolls_amount           100 non-null    float64\n15  improvement_surcharge  100 non-null    float64\n16  total_amount           100 non-null    float64\n17  congestion_surcharge   100 non-null    float64\ndtypes: datetime64[ns](2), float64(9), int64(6), object(1)\nmemory usage: 14.2+ KB",
        "question": "What is the data type of the tpep_pickup_datetime and tpep_dropoff_datetime columns?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 120,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Pandas can interpret “string” column values as “datetime” directly when reading the CSV file using “pd.read_csv” using the parameter “parse_dates”, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\npandas.read_csv — pandas 2.1.4 documentation (pydata.org)\nExample from week 1\nimport pandas as pd\ndf = pd.read_csv(\n'yellow_tripdata_2021-01.csv',\nnrows=100,\nparse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\ndf.info()\nwhich will output\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100 entries, 0 to 99\nData columns (total 18 columns):\n#   Column                 Non-Null Count  Dtype\n---  ------                 --------------  -----\n0   VendorID               100 non-null    int64\n1   tpep_pickup_datetime   100 non-null    datetime64[ns]\n2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\n3   passenger_count        100 non-null    int64\n4   trip_distance          100 non-null    float64\n5   RatecodeID             100 non-null    int64\n6   store_and_fwd_flag     100 non-null    object\n7   PULocationID           100 non-null    int64\n8   DOLocationID           100 non-null    int64\n9   payment_type           100 non-null    int64\n10  fare_amount            100 non-null    float64\n11  extra                  100 non-null    float64\n12  mta_tax                100 non-null    float64\n13  tip_amount             100 non-null    float64\n14  tolls_amount           100 non-null    float64\n15  improvement_surcharge  100 non-null    float64\n16  total_amount           100 non-null    float64\n17  congestion_surcharge   100 non-null    float64\ndtypes: datetime64[ns](2), float64(9), int64(6), object(1)\nmemory usage: 14.2+ KB",
        "question": "Which function from pandas is used to read a CSV file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 120,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "os.system(f\"curl -LO {url} -o {csv_name}\")",
        "question": "What command is being executed in the provided text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 121,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "os.system(f\"curl -LO {url} -o {csv_name}\")",
        "question": "What does the 'curl' command do in this context?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 121,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "os.system(f\"curl -LO {url} -o {csv_name}\")",
        "question": "What is the purpose of the 'url' variable in the command?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 121,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "os.system(f\"curl -LO {url} -o {csv_name}\")",
        "question": "What does the '-o' option specify in the curl command?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 121,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "os.system(f\"curl -LO {url} -o {csv_name}\")",
        "question": "What programming language is being used in the provided code snippet?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 121,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When a CSV file is compressed using Gzip, it is saved with a \".csv.gz\" file extension. This file type is also known as a Gzip compressed CSV file. When you want to read a Gzip compressed CSV file using Pandas, you can use the read_csv() function, which is specifically designed to read CSV files. The read_csv() function accepts several parameters, including a file path or a file-like object. To read a Gzip compressed CSV file, you can pass the file path of the \".csv.gz\" file as an argument to the read_csv() function.\nHere is an example of how to read a Gzip compressed CSV file using Pandas:\ndf = pd.read_csv('file.csv.gz'\n, compression='gzip'\n, low_memory=False\n)",
        "question": "What file extension is used for a Gzip compressed CSV file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 122,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When a CSV file is compressed using Gzip, it is saved with a \".csv.gz\" file extension. This file type is also known as a Gzip compressed CSV file. When you want to read a Gzip compressed CSV file using Pandas, you can use the read_csv() function, which is specifically designed to read CSV files. The read_csv() function accepts several parameters, including a file path or a file-like object. To read a Gzip compressed CSV file, you can pass the file path of the \".csv.gz\" file as an argument to the read_csv() function.\nHere is an example of how to read a Gzip compressed CSV file using Pandas:\ndf = pd.read_csv('file.csv.gz'\n, compression='gzip'\n, low_memory=False\n)",
        "question": "Which function is used to read a Gzip compressed CSV file in Pandas?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 122,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When a CSV file is compressed using Gzip, it is saved with a \".csv.gz\" file extension. This file type is also known as a Gzip compressed CSV file. When you want to read a Gzip compressed CSV file using Pandas, you can use the read_csv() function, which is specifically designed to read CSV files. The read_csv() function accepts several parameters, including a file path or a file-like object. To read a Gzip compressed CSV file, you can pass the file path of the \".csv.gz\" file as an argument to the read_csv() function.\nHere is an example of how to read a Gzip compressed CSV file using Pandas:\ndf = pd.read_csv('file.csv.gz'\n, compression='gzip'\n, low_memory=False\n)",
        "question": "What parameters does the read_csv() function accept?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 122,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When a CSV file is compressed using Gzip, it is saved with a \".csv.gz\" file extension. This file type is also known as a Gzip compressed CSV file. When you want to read a Gzip compressed CSV file using Pandas, you can use the read_csv() function, which is specifically designed to read CSV files. The read_csv() function accepts several parameters, including a file path or a file-like object. To read a Gzip compressed CSV file, you can pass the file path of the \".csv.gz\" file as an argument to the read_csv() function.\nHere is an example of how to read a Gzip compressed CSV file using Pandas:\ndf = pd.read_csv('file.csv.gz'\n, compression='gzip'\n, low_memory=False\n)",
        "question": "How do you specify the compression type when reading a Gzip compressed CSV file in Pandas?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 122,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When a CSV file is compressed using Gzip, it is saved with a \".csv.gz\" file extension. This file type is also known as a Gzip compressed CSV file. When you want to read a Gzip compressed CSV file using Pandas, you can use the read_csv() function, which is specifically designed to read CSV files. The read_csv() function accepts several parameters, including a file path or a file-like object. To read a Gzip compressed CSV file, you can pass the file path of the \".csv.gz\" file as an argument to the read_csv() function.\nHere is an example of how to read a Gzip compressed CSV file using Pandas:\ndf = pd.read_csv('file.csv.gz'\n, compression='gzip'\n, low_memory=False\n)",
        "question": "What does the low_memory parameter do in the read_csv() function?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 122,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Contrary to panda’s read_csv method there’s no such easy way to iterate through and set chunksize for parquet files. We can use PyArrow (Apache Arrow Python bindings) to resolve that.\nimport pyarrow.parquet as pq\noutput_name = “https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet”\nparquet_file = pq.ParquetFile(output_name)\nparquet_size = parquet_file.metadata.num_rows\nengine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\ntable_name=”yellow_taxi_schema”\n# Clear table if exists\npq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\n# default (and max) batch size\nindex = 65536\nfor i in parquet_file.iter_batches(use_threads=True):\nt_start = time()\nprint(f'Ingesting {index} out of {parquet_size} rows ({index / parquet_size:.0%})')\ni.to_pandas().to_sql(name=table_name, con=engine, if_exists='append')\nindex += 65536\nt_end = time()\nprint(f'\\t- it took %.1f seconds' % (t_end - t_start))",
        "question": "What is the main difference between pandas' read_csv method and the method for handling parquet files?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 123,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Contrary to panda’s read_csv method there’s no such easy way to iterate through and set chunksize for parquet files. We can use PyArrow (Apache Arrow Python bindings) to resolve that.\nimport pyarrow.parquet as pq\noutput_name = “https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet”\nparquet_file = pq.ParquetFile(output_name)\nparquet_size = parquet_file.metadata.num_rows\nengine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\ntable_name=”yellow_taxi_schema”\n# Clear table if exists\npq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\n# default (and max) batch size\nindex = 65536\nfor i in parquet_file.iter_batches(use_threads=True):\nt_start = time()\nprint(f'Ingesting {index} out of {parquet_size} rows ({index / parquet_size:.0%})')\ni.to_pandas().to_sql(name=table_name, con=engine, if_exists='append')\nindex += 65536\nt_end = time()\nprint(f'\\t- it took %.1f seconds' % (t_end - t_start))",
        "question": "Which library is suggested for iterating through and setting chunksize for parquet files?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 123,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Contrary to panda’s read_csv method there’s no such easy way to iterate through and set chunksize for parquet files. We can use PyArrow (Apache Arrow Python bindings) to resolve that.\nimport pyarrow.parquet as pq\noutput_name = “https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet”\nparquet_file = pq.ParquetFile(output_name)\nparquet_size = parquet_file.metadata.num_rows\nengine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\ntable_name=”yellow_taxi_schema”\n# Clear table if exists\npq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\n# default (and max) batch size\nindex = 65536\nfor i in parquet_file.iter_batches(use_threads=True):\nt_start = time()\nprint(f'Ingesting {index} out of {parquet_size} rows ({index / parquet_size:.0%})')\ni.to_pandas().to_sql(name=table_name, con=engine, if_exists='append')\nindex += 65536\nt_end = time()\nprint(f'\\t- it took %.1f seconds' % (t_end - t_start))",
        "question": "How is the total number of rows in a parquet file accessed using PyArrow?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 123,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Contrary to panda’s read_csv method there’s no such easy way to iterate through and set chunksize for parquet files. We can use PyArrow (Apache Arrow Python bindings) to resolve that.\nimport pyarrow.parquet as pq\noutput_name = “https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet”\nparquet_file = pq.ParquetFile(output_name)\nparquet_size = parquet_file.metadata.num_rows\nengine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\ntable_name=”yellow_taxi_schema”\n# Clear table if exists\npq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\n# default (and max) batch size\nindex = 65536\nfor i in parquet_file.iter_batches(use_threads=True):\nt_start = time()\nprint(f'Ingesting {index} out of {parquet_size} rows ({index / parquet_size:.0%})')\ni.to_pandas().to_sql(name=table_name, con=engine, if_exists='append')\nindex += 65536\nt_end = time()\nprint(f'\\t- it took %.1f seconds' % (t_end - t_start))",
        "question": "What SQL operation is performed to clear the existing table before inserting new data?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 123,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Contrary to panda’s read_csv method there’s no such easy way to iterate through and set chunksize for parquet files. We can use PyArrow (Apache Arrow Python bindings) to resolve that.\nimport pyarrow.parquet as pq\noutput_name = “https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet”\nparquet_file = pq.ParquetFile(output_name)\nparquet_size = parquet_file.metadata.num_rows\nengine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\ntable_name=”yellow_taxi_schema”\n# Clear table if exists\npq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\n# default (and max) batch size\nindex = 65536\nfor i in parquet_file.iter_batches(use_threads=True):\nt_start = time()\nprint(f'Ingesting {index} out of {parquet_size} rows ({index / parquet_size:.0%})')\ni.to_pandas().to_sql(name=table_name, con=engine, if_exists='append')\nindex += 65536\nt_end = time()\nprint(f'\\t- it took %.1f seconds' % (t_end - t_start))",
        "question": "How is the progress of data ingestion from parquet to SQL reported in the code?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 123,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error raised during the jupyter notebook’s cell execution:\nfrom sqlalchemy import create_engine.\nSolution: Version of Python module “typing_extensions” >= 4.6.0. Can be updated by Conda or pip.",
        "question": "What error is raised during the Jupyter notebook's cell execution?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 124,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error raised during the jupyter notebook’s cell execution:\nfrom sqlalchemy import create_engine.\nSolution: Version of Python module “typing_extensions” >= 4.6.0. Can be updated by Conda or pip.",
        "question": "Which module's version needs to be updated to resolve the issue?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 124,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error raised during the jupyter notebook’s cell execution:\nfrom sqlalchemy import create_engine.\nSolution: Version of Python module “typing_extensions” >= 4.6.0. Can be updated by Conda or pip.",
        "question": "What is the minimum required version of the 'typing_extensions' module?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 124,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error raised during the jupyter notebook’s cell execution:\nfrom sqlalchemy import create_engine.\nSolution: Version of Python module “typing_extensions” >= 4.6.0. Can be updated by Conda or pip.",
        "question": "How can the 'typing_extensions' module be updated?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 124,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error raised during the jupyter notebook’s cell execution:\nfrom sqlalchemy import create_engine.\nSolution: Version of Python module “typing_extensions” >= 4.6.0. Can be updated by Conda or pip.",
        "question": "Which libraries are mentioned for updating the module?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 124,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\nSolution:\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\nengine = create_engine(conn_string)",
        "question": "What error is encountered when using the initial connection string?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 125,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\nSolution:\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\nengine = create_engine(conn_string)",
        "question": "What is the correct format for the connection string to resolve the error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 125,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\nSolution:\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\nengine = create_engine(conn_string)",
        "question": "Which Python library is being used to create the database engine?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 125,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\nSolution:\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\nengine = create_engine(conn_string)",
        "question": "What does the 'postgresql+psycopg' indicate in the connection string?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 125,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\nSolution:\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\nengine = create_engine(conn_string)",
        "question": "How can one establish a connection to a PostgreSQL database using SQLAlchemy?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 125,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error raised during the jupyter notebook’s cell execution:\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\nSolution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.",
        "question": "What type of error is raised during the Jupyter Notebook's cell execution?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 126,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error raised during the jupyter notebook’s cell execution:\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\nSolution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.",
        "question": "What is the database connection string mentioned in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 126,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error raised during the jupyter notebook’s cell execution:\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\nSolution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.",
        "question": "Which Python module needs to be installed to resolve the error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 126,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error raised during the jupyter notebook’s cell execution:\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\nSolution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.",
        "question": "What are the two methods mentioned for installing the 'psycopg2' module?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 126,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error raised during the jupyter notebook’s cell execution:\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\nSolution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.",
        "question": "What is the purpose of the 'create_engine' function in this context?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 126,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Unable to add Google Cloud SDK PATH to Windows\nWindows error: The installer is unable to automatically update your system PATH. Please add  C:\\tools\\google-cloud-sdk\\bin\nif you are constantly getting this feedback. Might be that you needed to add Gitbash to your Windows path:\nOne way of doing that is to use conda: ‘If you are not already using it\nDownload the Anaconda Navigator\nMake sure to check the box (add conda to the path when installing navigator: although not recommended do it anyway)\nYou might also need to install git bash if you are not already using it(or you might need to uninstall it to reinstall it properly)\nMake sure to check the following boxes while you install Gitbash\nAdd a GitBash to Windows Terminal\nUse Git and optional Unix tools from the command prompt\nNow open up git bash and type conda init bash This should modify your bash profile\nAdditionally, you might want to use Gitbash as your default terminal.\nOpen your Windows terminal and go to settings, on the default profile change Windows power shell to git bash",
        "question": "What message does the installer display when it cannot update the system PATH?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 127,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Unable to add Google Cloud SDK PATH to Windows\nWindows error: The installer is unable to automatically update your system PATH. Please add  C:\\tools\\google-cloud-sdk\\bin\nif you are constantly getting this feedback. Might be that you needed to add Gitbash to your Windows path:\nOne way of doing that is to use conda: ‘If you are not already using it\nDownload the Anaconda Navigator\nMake sure to check the box (add conda to the path when installing navigator: although not recommended do it anyway)\nYou might also need to install git bash if you are not already using it(or you might need to uninstall it to reinstall it properly)\nMake sure to check the following boxes while you install Gitbash\nAdd a GitBash to Windows Terminal\nUse Git and optional Unix tools from the command prompt\nNow open up git bash and type conda init bash This should modify your bash profile\nAdditionally, you might want to use Gitbash as your default terminal.\nOpen your Windows terminal and go to settings, on the default profile change Windows power shell to git bash",
        "question": "How can you add Gitbash to your Windows PATH according to the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 127,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Unable to add Google Cloud SDK PATH to Windows\nWindows error: The installer is unable to automatically update your system PATH. Please add  C:\\tools\\google-cloud-sdk\\bin\nif you are constantly getting this feedback. Might be that you needed to add Gitbash to your Windows path:\nOne way of doing that is to use conda: ‘If you are not already using it\nDownload the Anaconda Navigator\nMake sure to check the box (add conda to the path when installing navigator: although not recommended do it anyway)\nYou might also need to install git bash if you are not already using it(or you might need to uninstall it to reinstall it properly)\nMake sure to check the following boxes while you install Gitbash\nAdd a GitBash to Windows Terminal\nUse Git and optional Unix tools from the command prompt\nNow open up git bash and type conda init bash This should modify your bash profile\nAdditionally, you might want to use Gitbash as your default terminal.\nOpen your Windows terminal and go to settings, on the default profile change Windows power shell to git bash",
        "question": "What is the recommended action regarding adding conda to the PATH during the installation of Anaconda Navigator?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 127,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Unable to add Google Cloud SDK PATH to Windows\nWindows error: The installer is unable to automatically update your system PATH. Please add  C:\\tools\\google-cloud-sdk\\bin\nif you are constantly getting this feedback. Might be that you needed to add Gitbash to your Windows path:\nOne way of doing that is to use conda: ‘If you are not already using it\nDownload the Anaconda Navigator\nMake sure to check the box (add conda to the path when installing navigator: although not recommended do it anyway)\nYou might also need to install git bash if you are not already using it(or you might need to uninstall it to reinstall it properly)\nMake sure to check the following boxes while you install Gitbash\nAdd a GitBash to Windows Terminal\nUse Git and optional Unix tools from the command prompt\nNow open up git bash and type conda init bash This should modify your bash profile\nAdditionally, you might want to use Gitbash as your default terminal.\nOpen your Windows terminal and go to settings, on the default profile change Windows power shell to git bash",
        "question": "What command should you type in Gitbash to modify the bash profile?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 127,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Unable to add Google Cloud SDK PATH to Windows\nWindows error: The installer is unable to automatically update your system PATH. Please add  C:\\tools\\google-cloud-sdk\\bin\nif you are constantly getting this feedback. Might be that you needed to add Gitbash to your Windows path:\nOne way of doing that is to use conda: ‘If you are not already using it\nDownload the Anaconda Navigator\nMake sure to check the box (add conda to the path when installing navigator: although not recommended do it anyway)\nYou might also need to install git bash if you are not already using it(or you might need to uninstall it to reinstall it properly)\nMake sure to check the following boxes while you install Gitbash\nAdd a GitBash to Windows Terminal\nUse Git and optional Unix tools from the command prompt\nNow open up git bash and type conda init bash This should modify your bash profile\nAdditionally, you might want to use Gitbash as your default terminal.\nOpen your Windows terminal and go to settings, on the default profile change Windows power shell to git bash",
        "question": "How can you set Gitbash as your default terminal in Windows?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 127,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It asked me to create a project. This should be done from the cloud console. So maybe we don’t need this FAQ.\nWARNING: Project creation failed: HttpError accessing <https://cloudresourcemanager.googleapis.com/v1/projects?alt=json>: response: <{'vtpep_pickup_datetimeary': 'Origin, X-Origin, Referer', 'content-type': 'application/json; charset=UTF-8', 'content-encoding': 'gzip', 'date': 'Mon, 24 Jan 2022 19:29:12 GMT', 'server': 'ESF', 'cache-control': 'private', 'x-xss-protection': '0', 'x-frame-options': 'SAMEORIGIN', 'x-content-type-options': 'nosniff', 'server-timing': 'gfet4t7; dur=189', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"', 'transfer-encoding': 'chunked', 'status': 409}>, content <{\n\"error\": {\n\"code\": 409,\n\"message\": \"Requested entity alreadytpep_pickup_datetime exists\",\n\"status\": \"ALREADY_EXISTS\"\n}\n}\nFrom Stackoverflow: https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1\nProject IDs are unique across all projects. That means if any user ever had a project with that ID, you cannot use it. testproject is pretty common, so it's not surprising it's already taken.",
        "question": "What steps are required to create a project from the cloud console?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 128,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It asked me to create a project. This should be done from the cloud console. So maybe we don’t need this FAQ.\nWARNING: Project creation failed: HttpError accessing <https://cloudresourcemanager.googleapis.com/v1/projects?alt=json>: response: <{'vtpep_pickup_datetimeary': 'Origin, X-Origin, Referer', 'content-type': 'application/json; charset=UTF-8', 'content-encoding': 'gzip', 'date': 'Mon, 24 Jan 2022 19:29:12 GMT', 'server': 'ESF', 'cache-control': 'private', 'x-xss-protection': '0', 'x-frame-options': 'SAMEORIGIN', 'x-content-type-options': 'nosniff', 'server-timing': 'gfet4t7; dur=189', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"', 'transfer-encoding': 'chunked', 'status': 409}>, content <{\n\"error\": {\n\"code\": 409,\n\"message\": \"Requested entity alreadytpep_pickup_datetime exists\",\n\"status\": \"ALREADY_EXISTS\"\n}\n}\nFrom Stackoverflow: https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1\nProject IDs are unique across all projects. That means if any user ever had a project with that ID, you cannot use it. testproject is pretty common, so it's not surprising it's already taken.",
        "question": "What was the error that occurred during project creation?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 128,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It asked me to create a project. This should be done from the cloud console. So maybe we don’t need this FAQ.\nWARNING: Project creation failed: HttpError accessing <https://cloudresourcemanager.googleapis.com/v1/projects?alt=json>: response: <{'vtpep_pickup_datetimeary': 'Origin, X-Origin, Referer', 'content-type': 'application/json; charset=UTF-8', 'content-encoding': 'gzip', 'date': 'Mon, 24 Jan 2022 19:29:12 GMT', 'server': 'ESF', 'cache-control': 'private', 'x-xss-protection': '0', 'x-frame-options': 'SAMEORIGIN', 'x-content-type-options': 'nosniff', 'server-timing': 'gfet4t7; dur=189', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"', 'transfer-encoding': 'chunked', 'status': 409}>, content <{\n\"error\": {\n\"code\": 409,\n\"message\": \"Requested entity alreadytpep_pickup_datetime exists\",\n\"status\": \"ALREADY_EXISTS\"\n}\n}\nFrom Stackoverflow: https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1\nProject IDs are unique across all projects. That means if any user ever had a project with that ID, you cannot use it. testproject is pretty common, so it's not surprising it's already taken.",
        "question": "Why did the project creation fail with a status code of 409?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 128,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It asked me to create a project. This should be done from the cloud console. So maybe we don’t need this FAQ.\nWARNING: Project creation failed: HttpError accessing <https://cloudresourcemanager.googleapis.com/v1/projects?alt=json>: response: <{'vtpep_pickup_datetimeary': 'Origin, X-Origin, Referer', 'content-type': 'application/json; charset=UTF-8', 'content-encoding': 'gzip', 'date': 'Mon, 24 Jan 2022 19:29:12 GMT', 'server': 'ESF', 'cache-control': 'private', 'x-xss-protection': '0', 'x-frame-options': 'SAMEORIGIN', 'x-content-type-options': 'nosniff', 'server-timing': 'gfet4t7; dur=189', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"', 'transfer-encoding': 'chunked', 'status': 409}>, content <{\n\"error\": {\n\"code\": 409,\n\"message\": \"Requested entity alreadytpep_pickup_datetime exists\",\n\"status\": \"ALREADY_EXISTS\"\n}\n}\nFrom Stackoverflow: https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1\nProject IDs are unique across all projects. That means if any user ever had a project with that ID, you cannot use it. testproject is pretty common, so it's not surprising it's already taken.",
        "question": "What does the error message indicate about the project ID?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 128,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It asked me to create a project. This should be done from the cloud console. So maybe we don’t need this FAQ.\nWARNING: Project creation failed: HttpError accessing <https://cloudresourcemanager.googleapis.com/v1/projects?alt=json>: response: <{'vtpep_pickup_datetimeary': 'Origin, X-Origin, Referer', 'content-type': 'application/json; charset=UTF-8', 'content-encoding': 'gzip', 'date': 'Mon, 24 Jan 2022 19:29:12 GMT', 'server': 'ESF', 'cache-control': 'private', 'x-xss-protection': '0', 'x-frame-options': 'SAMEORIGIN', 'x-content-type-options': 'nosniff', 'server-timing': 'gfet4t7; dur=189', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"', 'transfer-encoding': 'chunked', 'status': 409}>, content <{\n\"error\": {\n\"code\": 409,\n\"message\": \"Requested entity alreadytpep_pickup_datetime exists\",\n\"status\": \"ALREADY_EXISTS\"\n}\n}\nFrom Stackoverflow: https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1\nProject IDs are unique across all projects. That means if any user ever had a project with that ID, you cannot use it. testproject is pretty common, so it's not surprising it's already taken.",
        "question": "Why is 'testproject' mentioned as a commonly taken project ID?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 128,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you receive the error: “Error 403: The project to be billed is associated with an absent billing account., accountDisabled” It is most likely because you did not enter YOUR project ID. The snip below is from video 1.3.2\nThe value you enter here will be unique to each student. You can find this value on your GCP Dashboard when you login.\nAshish Agrawal\nAnother possibility is that you have not linked your billing account to your current project",
        "question": "What does the error message 'Error 403: The project to be billed is associated with an absent billing account, accountDisabled' indicate?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 129,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you receive the error: “Error 403: The project to be billed is associated with an absent billing account., accountDisabled” It is most likely because you did not enter YOUR project ID. The snip below is from video 1.3.2\nThe value you enter here will be unique to each student. You can find this value on your GCP Dashboard when you login.\nAshish Agrawal\nAnother possibility is that you have not linked your billing account to your current project",
        "question": "What should you enter to avoid the 403 error when billing a project?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 129,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you receive the error: “Error 403: The project to be billed is associated with an absent billing account., accountDisabled” It is most likely because you did not enter YOUR project ID. The snip below is from video 1.3.2\nThe value you enter here will be unique to each student. You can find this value on your GCP Dashboard when you login.\nAshish Agrawal\nAnother possibility is that you have not linked your billing account to your current project",
        "question": "Where can you find your unique project ID?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 129,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you receive the error: “Error 403: The project to be billed is associated with an absent billing account., accountDisabled” It is most likely because you did not enter YOUR project ID. The snip below is from video 1.3.2\nThe value you enter here will be unique to each student. You can find this value on your GCP Dashboard when you login.\nAshish Agrawal\nAnother possibility is that you have not linked your billing account to your current project",
        "question": "Who is mentioned as the source of the information in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 129,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you receive the error: “Error 403: The project to be billed is associated with an absent billing account., accountDisabled” It is most likely because you did not enter YOUR project ID. The snip below is from video 1.3.2\nThe value you enter here will be unique to each student. You can find this value on your GCP Dashboard when you login.\nAshish Agrawal\nAnother possibility is that you have not linked your billing account to your current project",
        "question": "What is another potential reason for receiving the 403 error besides not entering the project ID?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 129,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "GCP Account Suspension Inquiry\nIf Google refuses your credit/debit card, try another - I’ve got an issue with Kaspi (Kazakhstan) but it worked with TBC (Georgia).\nUnfortunately, there’s small hope that support will help.\nIt seems that Pyypl web-card should work too.",
        "question": "What should you do if Google refuses your credit/debit card?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 130,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "GCP Account Suspension Inquiry\nIf Google refuses your credit/debit card, try another - I’ve got an issue with Kaspi (Kazakhstan) but it worked with TBC (Georgia).\nUnfortunately, there’s small hope that support will help.\nIt seems that Pyypl web-card should work too.",
        "question": "Which alternative card worked for the author when Google refused their initial card?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 130,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "GCP Account Suspension Inquiry\nIf Google refuses your credit/debit card, try another - I’ve got an issue with Kaspi (Kazakhstan) but it worked with TBC (Georgia).\nUnfortunately, there’s small hope that support will help.\nIt seems that Pyypl web-card should work too.",
        "question": "What is the issue mentioned with the Kaspi card?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 130,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "GCP Account Suspension Inquiry\nIf Google refuses your credit/debit card, try another - I’ve got an issue with Kaspi (Kazakhstan) but it worked with TBC (Georgia).\nUnfortunately, there’s small hope that support will help.\nIt seems that Pyypl web-card should work too.",
        "question": "Is there a suggestion for another payment method that may work?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 130,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "GCP Account Suspension Inquiry\nIf Google refuses your credit/debit card, try another - I’ve got an issue with Kaspi (Kazakhstan) but it worked with TBC (Georgia).\nUnfortunately, there’s small hope that support will help.\nIt seems that Pyypl web-card should work too.",
        "question": "What is the author's opinion about the likelihood of getting help from support?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 130,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The ny-rides.json is your private file in Google Cloud Platform (GCP). \n\nAnd here’s the way to find it:\nGCP -> Select project with your  instance -> IAM & Admin -> Service Accounts Keys tab -> add key, JSON as key type, then click create\nNote: Once you go into Service Accounts Keys tab, click the email, then you can see the “KEYS” tab where you can add key as a JSON as its key type",
        "question": "What is the purpose of the ny-rides.json file in Google Cloud Platform?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 131,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The ny-rides.json is your private file in Google Cloud Platform (GCP). \n\nAnd here’s the way to find it:\nGCP -> Select project with your  instance -> IAM & Admin -> Service Accounts Keys tab -> add key, JSON as key type, then click create\nNote: Once you go into Service Accounts Keys tab, click the email, then you can see the “KEYS” tab where you can add key as a JSON as its key type",
        "question": "How do you access the Service Accounts Keys tab in GCP?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 131,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The ny-rides.json is your private file in Google Cloud Platform (GCP). \n\nAnd here’s the way to find it:\nGCP -> Select project with your  instance -> IAM & Admin -> Service Accounts Keys tab -> add key, JSON as key type, then click create\nNote: Once you go into Service Accounts Keys tab, click the email, then you can see the “KEYS” tab where you can add key as a JSON as its key type",
        "question": "What steps are involved in adding a new key to a service account in GCP?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 131,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The ny-rides.json is your private file in Google Cloud Platform (GCP). \n\nAnd here’s the way to find it:\nGCP -> Select project with your  instance -> IAM & Admin -> Service Accounts Keys tab -> add key, JSON as key type, then click create\nNote: Once you go into Service Accounts Keys tab, click the email, then you can see the “KEYS” tab where you can add key as a JSON as its key type",
        "question": "What type of key should be selected when adding a key in the Service Accounts Keys tab?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 131,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The ny-rides.json is your private file in Google Cloud Platform (GCP). \n\nAnd here’s the way to find it:\nGCP -> Select project with your  instance -> IAM & Admin -> Service Accounts Keys tab -> add key, JSON as key type, then click create\nNote: Once you go into Service Accounts Keys tab, click the email, then you can see the “KEYS” tab where you can add key as a JSON as its key type",
        "question": "Where can you find the 'KEYS' tab after navigating to the email in Service Accounts Keys?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 131,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In this lecture, Alexey deleted his instance in Google Cloud. Do I have to do it?\nNope. Do not delete your instance in Google Cloud platform. Otherwise, you have to do this twice for the week 1 readings.",
        "question": "What did Alexey do in Google Cloud during the lecture?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 132,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In this lecture, Alexey deleted his instance in Google Cloud. Do I have to do it?\nNope. Do not delete your instance in Google Cloud platform. Otherwise, you have to do this twice for the week 1 readings.",
        "question": "Is it necessary to delete your instance in Google Cloud?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 132,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In this lecture, Alexey deleted his instance in Google Cloud. Do I have to do it?\nNope. Do not delete your instance in Google Cloud platform. Otherwise, you have to do this twice for the week 1 readings.",
        "question": "What happens if you delete your instance in Google Cloud?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 132,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In this lecture, Alexey deleted his instance in Google Cloud. Do I have to do it?\nNope. Do not delete your instance in Google Cloud platform. Otherwise, you have to do this twice for the week 1 readings.",
        "question": "How many times would you have to delete the instance if you do it?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 132,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In this lecture, Alexey deleted his instance in Google Cloud. Do I have to do it?\nNope. Do not delete your instance in Google Cloud platform. Otherwise, you have to do this twice for the week 1 readings.",
        "question": "What could be the consequence of deleting your instance in week 1?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 132,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "System Resource Usage:\ntop or htop: Shows real-time information about system resource usage, including CPU, memory, and processes.\nfree -h: Displays information about system memory usage and availability.\ndf -h: Shows disk space usage of file systems.\ndu -h <directory>: Displays disk usage of a specific directory.\nRunning Processes:\nps aux: Lists all running processes along with detailed information.\nNetwork:\nifconfig or ip addr show: Shows network interface configuration.\nnetstat -tuln: Displays active network connections and listening ports.\nHardware Information:\nlscpu: Displays CPU information.\nlsblk: Lists block devices (disks and partitions).\nlshw: Lists hardware configuration.\nUser and Permissions:\nwho: Shows who is logged on and their activities.\nw: Displays information about currently logged-in users and their processes.\nPackage Management:\napt list --installed: Lists installed packages (for Ubuntu and Debian-based systems)",
        "question": "What command would you use to display real-time information about system resource usage?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 133,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "System Resource Usage:\ntop or htop: Shows real-time information about system resource usage, including CPU, memory, and processes.\nfree -h: Displays information about system memory usage and availability.\ndf -h: Shows disk space usage of file systems.\ndu -h <directory>: Displays disk usage of a specific directory.\nRunning Processes:\nps aux: Lists all running processes along with detailed information.\nNetwork:\nifconfig or ip addr show: Shows network interface configuration.\nnetstat -tuln: Displays active network connections and listening ports.\nHardware Information:\nlscpu: Displays CPU information.\nlsblk: Lists block devices (disks and partitions).\nlshw: Lists hardware configuration.\nUser and Permissions:\nwho: Shows who is logged on and their activities.\nw: Displays information about currently logged-in users and their processes.\nPackage Management:\napt list --installed: Lists installed packages (for Ubuntu and Debian-based systems)",
        "question": "Which command shows the disk space usage of file systems?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 133,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "System Resource Usage:\ntop or htop: Shows real-time information about system resource usage, including CPU, memory, and processes.\nfree -h: Displays information about system memory usage and availability.\ndf -h: Shows disk space usage of file systems.\ndu -h <directory>: Displays disk usage of a specific directory.\nRunning Processes:\nps aux: Lists all running processes along with detailed information.\nNetwork:\nifconfig or ip addr show: Shows network interface configuration.\nnetstat -tuln: Displays active network connections and listening ports.\nHardware Information:\nlscpu: Displays CPU information.\nlsblk: Lists block devices (disks and partitions).\nlshw: Lists hardware configuration.\nUser and Permissions:\nwho: Shows who is logged on and their activities.\nw: Displays information about currently logged-in users and their processes.\nPackage Management:\napt list --installed: Lists installed packages (for Ubuntu and Debian-based systems)",
        "question": "How can you display information about currently logged-in users and their processes?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 133,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "System Resource Usage:\ntop or htop: Shows real-time information about system resource usage, including CPU, memory, and processes.\nfree -h: Displays information about system memory usage and availability.\ndf -h: Shows disk space usage of file systems.\ndu -h <directory>: Displays disk usage of a specific directory.\nRunning Processes:\nps aux: Lists all running processes along with detailed information.\nNetwork:\nifconfig or ip addr show: Shows network interface configuration.\nnetstat -tuln: Displays active network connections and listening ports.\nHardware Information:\nlscpu: Displays CPU information.\nlsblk: Lists block devices (disks and partitions).\nlshw: Lists hardware configuration.\nUser and Permissions:\nwho: Shows who is logged on and their activities.\nw: Displays information about currently logged-in users and their processes.\nPackage Management:\napt list --installed: Lists installed packages (for Ubuntu and Debian-based systems)",
        "question": "What command provides detailed information about the hardware configuration of a system?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 133,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "System Resource Usage:\ntop or htop: Shows real-time information about system resource usage, including CPU, memory, and processes.\nfree -h: Displays information about system memory usage and availability.\ndf -h: Shows disk space usage of file systems.\ndu -h <directory>: Displays disk usage of a specific directory.\nRunning Processes:\nps aux: Lists all running processes along with detailed information.\nNetwork:\nifconfig or ip addr show: Shows network interface configuration.\nnetstat -tuln: Displays active network connections and listening ports.\nHardware Information:\nlscpu: Displays CPU information.\nlsblk: Lists block devices (disks and partitions).\nlshw: Lists hardware configuration.\nUser and Permissions:\nwho: Shows who is logged on and their activities.\nw: Displays information about currently logged-in users and their processes.\nPackage Management:\napt list --installed: Lists installed packages (for Ubuntu and Debian-based systems)",
        "question": "Which command lists all installed packages on Ubuntu and Debian-based systems?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 133,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "if you’ve got the error\n│ Error: Error updating Dataset \"projects/<your-project-id>/datasets/demo_dataset\": googleapi: Error 403: Billing has not been enabled for this project. Enable billing at https://console.cloud.google.com/billing. The default table expiration time must be less than 60 days, billingNotEnabled\nbut you’ve set your billing account indeed, then try to disable billing for the project and enable it again. It worked for ME!",
        "question": "What error message is encountered when billing is not enabled for a project?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 134,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "if you’ve got the error\n│ Error: Error updating Dataset \"projects/<your-project-id>/datasets/demo_dataset\": googleapi: Error 403: Billing has not been enabled for this project. Enable billing at https://console.cloud.google.com/billing. The default table expiration time must be less than 60 days, billingNotEnabled\nbut you’ve set your billing account indeed, then try to disable billing for the project and enable it again. It worked for ME!",
        "question": "Where can billing be enabled for a Google Cloud project?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 134,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "if you’ve got the error\n│ Error: Error updating Dataset \"projects/<your-project-id>/datasets/demo_dataset\": googleapi: Error 403: Billing has not been enabled for this project. Enable billing at https://console.cloud.google.com/billing. The default table expiration time must be less than 60 days, billingNotEnabled\nbut you’ve set your billing account indeed, then try to disable billing for the project and enable it again. It worked for ME!",
        "question": "What is the maximum default table expiration time mentioned in the error message?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 134,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "if you’ve got the error\n│ Error: Error updating Dataset \"projects/<your-project-id>/datasets/demo_dataset\": googleapi: Error 403: Billing has not been enabled for this project. Enable billing at https://console.cloud.google.com/billing. The default table expiration time must be less than 60 days, billingNotEnabled\nbut you’ve set your billing account indeed, then try to disable billing for the project and enable it again. It worked for ME!",
        "question": "What troubleshooting step is suggested if billing has already been set up?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 134,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "if you’ve got the error\n│ Error: Error updating Dataset \"projects/<your-project-id>/datasets/demo_dataset\": googleapi: Error 403: Billing has not been enabled for this project. Enable billing at https://console.cloud.google.com/billing. The default table expiration time must be less than 60 days, billingNotEnabled\nbut you’ve set your billing account indeed, then try to disable billing for the project and enable it again. It worked for ME!",
        "question": "What personal testimony is provided regarding the resolution of the billing issue?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 134,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "for windows if you having trouble install SDK try follow these steps on the link, if you getting this error:\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\nWARNING:\nCannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\nFor me:\nI reinstalled the sdk using unzip file “install.bat”,\nafter successfully checking gcloud version,\nrun gcloud init to set up project before\nyou run gcloud auth application-default login\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md\nGCP VM - I cannot get my Virtual Machine to start because GCP has no resources.\nClick on your VM\nCreate an image of your VM\nOn the page of the image, tell GCP to create a new VM instance via the image\nOn the settings page, change the location",
        "question": "What steps should be followed if there is trouble installing the SDK on Windows?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 135,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "for windows if you having trouble install SDK try follow these steps on the link, if you getting this error:\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\nWARNING:\nCannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\nFor me:\nI reinstalled the sdk using unzip file “install.bat”,\nafter successfully checking gcloud version,\nrun gcloud init to set up project before\nyou run gcloud auth application-default login\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md\nGCP VM - I cannot get my Virtual Machine to start because GCP has no resources.\nClick on your VM\nCreate an image of your VM\nOn the page of the image, tell GCP to create a new VM instance via the image\nOn the settings page, change the location",
        "question": "What warning might occur when using Application Default Credentials (ADC) related to quota projects?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 135,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "for windows if you having trouble install SDK try follow these steps on the link, if you getting this error:\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\nWARNING:\nCannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\nFor me:\nI reinstalled the sdk using unzip file “install.bat”,\nafter successfully checking gcloud version,\nrun gcloud init to set up project before\nyou run gcloud auth application-default login\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md\nGCP VM - I cannot get my Virtual Machine to start because GCP has no resources.\nClick on your VM\nCreate an image of your VM\nOn the page of the image, tell GCP to create a new VM instance via the image\nOn the settings page, change the location",
        "question": "What command needs to be run to add a quota project to Application Default Credentials?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 135,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "for windows if you having trouble install SDK try follow these steps on the link, if you getting this error:\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\nWARNING:\nCannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\nFor me:\nI reinstalled the sdk using unzip file “install.bat”,\nafter successfully checking gcloud version,\nrun gcloud init to set up project before\nyou run gcloud auth application-default login\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md\nGCP VM - I cannot get my Virtual Machine to start because GCP has no resources.\nClick on your VM\nCreate an image of your VM\nOn the page of the image, tell GCP to create a new VM instance via the image\nOn the settings page, change the location",
        "question": "What is the process to create a new VM instance from an existing VM image on GCP?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 135,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "for windows if you having trouble install SDK try follow these steps on the link, if you getting this error:\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\nWARNING:\nCannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\nFor me:\nI reinstalled the sdk using unzip file “install.bat”,\nafter successfully checking gcloud version,\nrun gcloud init to set up project before\nyou run gcloud auth application-default login\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md\nGCP VM - I cannot get my Virtual Machine to start because GCP has no resources.\nClick on your VM\nCreate an image of your VM\nOn the page of the image, tell GCP to create a new VM instance via the image\nOn the settings page, change the location",
        "question": "What should be done if a GCP Virtual Machine cannot start due to lack of resources?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 135,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The reason this video about the GCP VM exists is that many students had problems configuring their env. You can use your own env if it works for you.\nAnd the advantage of using your own environment is that if you are working in a Github repo where you can commit, you will be able to commit the changes that you do. In the VM the repo is cloned via HTTPS so it is not possible to directly commit, even if you are the owner of the repo.",
        "question": "Why was the video about the GCP VM created?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 136,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The reason this video about the GCP VM exists is that many students had problems configuring their env. You can use your own env if it works for you.\nAnd the advantage of using your own environment is that if you are working in a Github repo where you can commit, you will be able to commit the changes that you do. In the VM the repo is cloned via HTTPS so it is not possible to directly commit, even if you are the owner of the repo.",
        "question": "What is one advantage of using your own environment when working on a project?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 136,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The reason this video about the GCP VM exists is that many students had problems configuring their env. You can use your own env if it works for you.\nAnd the advantage of using your own environment is that if you are working in a Github repo where you can commit, you will be able to commit the changes that you do. In the VM the repo is cloned via HTTPS so it is not possible to directly commit, even if you are the owner of the repo.",
        "question": "How can you commit changes if you are using your own environment?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 136,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The reason this video about the GCP VM exists is that many students had problems configuring their env. You can use your own env if it works for you.\nAnd the advantage of using your own environment is that if you are working in a Github repo where you can commit, you will be able to commit the changes that you do. In the VM the repo is cloned via HTTPS so it is not possible to directly commit, even if you are the owner of the repo.",
        "question": "Why can't you commit changes directly from the VM?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 136,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The reason this video about the GCP VM exists is that many students had problems configuring their env. You can use your own env if it works for you.\nAnd the advantage of using your own environment is that if you are working in a Github repo where you can commit, you will be able to commit the changes that you do. In the VM the repo is cloned via HTTPS so it is not possible to directly commit, even if you are the owner of the repo.",
        "question": "What method is used to clone the repo in the VM?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 136,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I am trying to create a directory but it won't let me do it\nUser1@DESKTOP-PD6UM8A MINGW64 /\n$ mkdir .ssh\nmkdir: cannot create directory ‘.ssh’: Permission denied\nYou should do it in your home directory. Should be your home (~)\nLocal. But it seems you're trying to do it in the root folder (/). Should be your home (~)\nLink to Video 1.4.1",
        "question": "What command is being used to create a directory?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 137,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I am trying to create a directory but it won't let me do it\nUser1@DESKTOP-PD6UM8A MINGW64 /\n$ mkdir .ssh\nmkdir: cannot create directory ‘.ssh’: Permission denied\nYou should do it in your home directory. Should be your home (~)\nLocal. But it seems you're trying to do it in the root folder (/). Should be your home (~)\nLink to Video 1.4.1",
        "question": "What error message is displayed when the user tries to create the directory?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 137,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I am trying to create a directory but it won't let me do it\nUser1@DESKTOP-PD6UM8A MINGW64 /\n$ mkdir .ssh\nmkdir: cannot create directory ‘.ssh’: Permission denied\nYou should do it in your home directory. Should be your home (~)\nLocal. But it seems you're trying to do it in the root folder (/). Should be your home (~)\nLink to Video 1.4.1",
        "question": "Where should the user create the directory according to the advice given?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 137,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I am trying to create a directory but it won't let me do it\nUser1@DESKTOP-PD6UM8A MINGW64 /\n$ mkdir .ssh\nmkdir: cannot create directory ‘.ssh’: Permission denied\nYou should do it in your home directory. Should be your home (~)\nLocal. But it seems you're trying to do it in the root folder (/). Should be your home (~)\nLink to Video 1.4.1",
        "question": "What is indicated by the message 'Permission denied'?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 137,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I am trying to create a directory but it won't let me do it\nUser1@DESKTOP-PD6UM8A MINGW64 /\n$ mkdir .ssh\nmkdir: cannot create directory ‘.ssh’: Permission denied\nYou should do it in your home directory. Should be your home (~)\nLocal. But it seems you're trying to do it in the root folder (/). Should be your home (~)\nLink to Video 1.4.1",
        "question": "Which directory does the user appear to be attempting to create the directory in?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 137,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Failed to save '<file>': Unable to write file 'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open '/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')\nYou need to change the owner of the files you are trying to edit via VS Code. You can run the following command to change the ownership.\nssh\nsudo chown -R <user> <path to your directory>",
        "question": "What error does VS Code indicate when attempting to save a file?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 138,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Failed to save '<file>': Unable to write file 'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open '/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')\nYou need to change the owner of the files you are trying to edit via VS Code. You can run the following command to change the ownership.\nssh\nsudo chown -R <user> <path to your directory>",
        "question": "What command is suggested to change the owner of the files in the directory?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 138,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Failed to save '<file>': Unable to write file 'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open '/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')\nYou need to change the owner of the files you are trying to edit via VS Code. You can run the following command to change the ownership.\nssh\nsudo chown -R <user> <path to your directory>",
        "question": "What does the EACCES permission denied error mean in this context?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 138,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Failed to save '<file>': Unable to write file 'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open '/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')\nYou need to change the owner of the files you are trying to edit via VS Code. You can run the following command to change the ownership.\nssh\nsudo chown -R <user> <path to your directory>",
        "question": "Which directory path is mentioned in the permission error message?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 138,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Failed to save '<file>': Unable to write file 'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open '/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')\nYou need to change the owner of the files you are trying to edit via VS Code. You can run the following command to change the ownership.\nssh\nsudo chown -R <user> <path to your directory>",
        "question": "What role does the 'sudo' command play in changing file ownership?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 138,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Question: I connected to my VM perfectly fine last week (ssh) but when I tried again this week, the connection request keeps timing out.\n✅Answer: Start your VM. Once the VM is running, copy its External IP and paste that into your config file within the ~/.ssh folder.\ncd ~/.ssh\ncode config ← this opens the config file in VSCode",
        "question": "What issue is the user experiencing when trying to connect to their VM?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 139,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Question: I connected to my VM perfectly fine last week (ssh) but when I tried again this week, the connection request keeps timing out.\n✅Answer: Start your VM. Once the VM is running, copy its External IP and paste that into your config file within the ~/.ssh folder.\ncd ~/.ssh\ncode config ← this opens the config file in VSCode",
        "question": "What does the user need to do to resolve the connection timeout issue?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 139,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Question: I connected to my VM perfectly fine last week (ssh) but when I tried again this week, the connection request keeps timing out.\n✅Answer: Start your VM. Once the VM is running, copy its External IP and paste that into your config file within the ~/.ssh folder.\ncd ~/.ssh\ncode config ← this opens the config file in VSCode",
        "question": "Where is the config file located that the user needs to edit?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 139,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Question: I connected to my VM perfectly fine last week (ssh) but when I tried again this week, the connection request keeps timing out.\n✅Answer: Start your VM. Once the VM is running, copy its External IP and paste that into your config file within the ~/.ssh folder.\ncd ~/.ssh\ncode config ← this opens the config file in VSCode",
        "question": "Which command is used to open the config file in VSCode?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 139,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Question: I connected to my VM perfectly fine last week (ssh) but when I tried again this week, the connection request keeps timing out.\n✅Answer: Start your VM. Once the VM is running, copy its External IP and paste that into your config file within the ~/.ssh folder.\ncd ~/.ssh\ncode config ← this opens the config file in VSCode",
        "question": "What should the user do after starting their VM to reconnect through SSH?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 139,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "(reference: https://serverfault.com/questions/953290/google-compute-engine-ssh-connect-to-host-ip-port-22-operation-timed-out)Go to edit your VM.\nGo to section Automation\nAdd Startup script\n```\n#!/bin/bash\nsudo ufw allow ssh\n```\nStop and Start VM.",
        "question": "What should you do to edit your VM?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 140,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "(reference: https://serverfault.com/questions/953290/google-compute-engine-ssh-connect-to-host-ip-port-22-operation-timed-out)Go to edit your VM.\nGo to section Automation\nAdd Startup script\n```\n#!/bin/bash\nsudo ufw allow ssh\n```\nStop and Start VM.",
        "question": "Where do you navigate to add a startup script?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 140,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "(reference: https://serverfault.com/questions/953290/google-compute-engine-ssh-connect-to-host-ip-port-22-operation-timed-out)Go to edit your VM.\nGo to section Automation\nAdd Startup script\n```\n#!/bin/bash\nsudo ufw allow ssh\n```\nStop and Start VM.",
        "question": "What command is included in the startup script?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 140,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "(reference: https://serverfault.com/questions/953290/google-compute-engine-ssh-connect-to-host-ip-port-22-operation-timed-out)Go to edit your VM.\nGo to section Automation\nAdd Startup script\n```\n#!/bin/bash\nsudo ufw allow ssh\n```\nStop and Start VM.",
        "question": "What steps must be taken after adding the startup script?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 140,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "(reference: https://serverfault.com/questions/953290/google-compute-engine-ssh-connect-to-host-ip-port-22-operation-timed-out)Go to edit your VM.\nGo to section Automation\nAdd Startup script\n```\n#!/bin/bash\nsudo ufw allow ssh\n```\nStop and Start VM.",
        "question": "What does the command 'sudo ufw allow ssh' do?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 140,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can easily forward the ports of pgAdmin, postgres and Jupyter Notebook using the built-in tools in Ubuntu and without any additional client:\nFirst, in the VM machine, launch docker-compose up -d and jupyter notebook in the correct folder.\nFrom the local machine, execute: ssh -i ~/.ssh/gcp -L 5432:localhost:5432 username@external_ip_of_vm\nExecute the same command but with ports 8080 and 8888.\nNow you can access pgAdmin on local machine in browser typing localhost:8080\nFor Jupyter Notebook, type localhost:8888 in the browser of your local machine. If you have problems with the credentials, it is possible that you have to copy the link with the access token provided in the logs of the terminal of the VM machine when you launched the jupyter notebook command.\nTo forward both pgAdmin and postgres use, ssh -i ~/.ssh/gcp -L 5432:localhost:5432 -L 8080:localhost:8080 modito@35.197.218.128",
        "question": "What command is used to launch the necessary services in the VM machine?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 141,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can easily forward the ports of pgAdmin, postgres and Jupyter Notebook using the built-in tools in Ubuntu and without any additional client:\nFirst, in the VM machine, launch docker-compose up -d and jupyter notebook in the correct folder.\nFrom the local machine, execute: ssh -i ~/.ssh/gcp -L 5432:localhost:5432 username@external_ip_of_vm\nExecute the same command but with ports 8080 and 8888.\nNow you can access pgAdmin on local machine in browser typing localhost:8080\nFor Jupyter Notebook, type localhost:8888 in the browser of your local machine. If you have problems with the credentials, it is possible that you have to copy the link with the access token provided in the logs of the terminal of the VM machine when you launched the jupyter notebook command.\nTo forward both pgAdmin and postgres use, ssh -i ~/.ssh/gcp -L 5432:localhost:5432 -L 8080:localhost:8080 modito@35.197.218.128",
        "question": "How can you access pgAdmin from your local machine?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 141,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can easily forward the ports of pgAdmin, postgres and Jupyter Notebook using the built-in tools in Ubuntu and without any additional client:\nFirst, in the VM machine, launch docker-compose up -d and jupyter notebook in the correct folder.\nFrom the local machine, execute: ssh -i ~/.ssh/gcp -L 5432:localhost:5432 username@external_ip_of_vm\nExecute the same command but with ports 8080 and 8888.\nNow you can access pgAdmin on local machine in browser typing localhost:8080\nFor Jupyter Notebook, type localhost:8888 in the browser of your local machine. If you have problems with the credentials, it is possible that you have to copy the link with the access token provided in the logs of the terminal of the VM machine when you launched the jupyter notebook command.\nTo forward both pgAdmin and postgres use, ssh -i ~/.ssh/gcp -L 5432:localhost:5432 -L 8080:localhost:8080 modito@35.197.218.128",
        "question": "What ports need to be forwarded to access Jupyter Notebook locally?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 141,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can easily forward the ports of pgAdmin, postgres and Jupyter Notebook using the built-in tools in Ubuntu and without any additional client:\nFirst, in the VM machine, launch docker-compose up -d and jupyter notebook in the correct folder.\nFrom the local machine, execute: ssh -i ~/.ssh/gcp -L 5432:localhost:5432 username@external_ip_of_vm\nExecute the same command but with ports 8080 and 8888.\nNow you can access pgAdmin on local machine in browser typing localhost:8080\nFor Jupyter Notebook, type localhost:8888 in the browser of your local machine. If you have problems with the credentials, it is possible that you have to copy the link with the access token provided in the logs of the terminal of the VM machine when you launched the jupyter notebook command.\nTo forward both pgAdmin and postgres use, ssh -i ~/.ssh/gcp -L 5432:localhost:5432 -L 8080:localhost:8080 modito@35.197.218.128",
        "question": "What should you do if you encounter issues with Jupyter Notebook credentials?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 141,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can easily forward the ports of pgAdmin, postgres and Jupyter Notebook using the built-in tools in Ubuntu and without any additional client:\nFirst, in the VM machine, launch docker-compose up -d and jupyter notebook in the correct folder.\nFrom the local machine, execute: ssh -i ~/.ssh/gcp -L 5432:localhost:5432 username@external_ip_of_vm\nExecute the same command but with ports 8080 and 8888.\nNow you can access pgAdmin on local machine in browser typing localhost:8080\nFor Jupyter Notebook, type localhost:8888 in the browser of your local machine. If you have problems with the credentials, it is possible that you have to copy the link with the access token provided in the logs of the terminal of the VM machine when you launched the jupyter notebook command.\nTo forward both pgAdmin and postgres use, ssh -i ~/.ssh/gcp -L 5432:localhost:5432 -L 8080:localhost:8080 modito@35.197.218.128",
        "question": "What is the complete command to forward both pgAdmin and postgres?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 141,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are using MS VS Code and running gcloud in WSL2, when you first try to login to gcp via the gcloud cli gcloud auth application-default login, you will see a message like this, and nothing will happen\nAnd there might be a prompt to ask if you want to open it via browser, if you click on it, it will open up a page with error message\nSolution : you should instead hover on the long link, and ctrl + click the long link\n\nClick configure Trusted Domains here\n\nPopup will appear, pick first or second entry\nNext time you gcloud auth, the login page should popup via default browser without issues",
        "question": "What should you do if the gcloud CLI does not prompt for login when using MS VS Code and WSL2?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 142,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are using MS VS Code and running gcloud in WSL2, when you first try to login to gcp via the gcloud cli gcloud auth application-default login, you will see a message like this, and nothing will happen\nAnd there might be a prompt to ask if you want to open it via browser, if you click on it, it will open up a page with error message\nSolution : you should instead hover on the long link, and ctrl + click the long link\n\nClick configure Trusted Domains here\n\nPopup will appear, pick first or second entry\nNext time you gcloud auth, the login page should popup via default browser without issues",
        "question": "What error occurs when clicking the browser prompt after running gcloud auth application-default login?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 142,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are using MS VS Code and running gcloud in WSL2, when you first try to login to gcp via the gcloud cli gcloud auth application-default login, you will see a message like this, and nothing will happen\nAnd there might be a prompt to ask if you want to open it via browser, if you click on it, it will open up a page with error message\nSolution : you should instead hover on the long link, and ctrl + click the long link\n\nClick configure Trusted Domains here\n\nPopup will appear, pick first or second entry\nNext time you gcloud auth, the login page should popup via default browser without issues",
        "question": "How can you successfully open the login page for gcloud authentication?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 142,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are using MS VS Code and running gcloud in WSL2, when you first try to login to gcp via the gcloud cli gcloud auth application-default login, you will see a message like this, and nothing will happen\nAnd there might be a prompt to ask if you want to open it via browser, if you click on it, it will open up a page with error message\nSolution : you should instead hover on the long link, and ctrl + click the long link\n\nClick configure Trusted Domains here\n\nPopup will appear, pick first or second entry\nNext time you gcloud auth, the login page should popup via default browser without issues",
        "question": "What option should you select after clicking configure Trusted Domains?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 142,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are using MS VS Code and running gcloud in WSL2, when you first try to login to gcp via the gcloud cli gcloud auth application-default login, you will see a message like this, and nothing will happen\nAnd there might be a prompt to ask if you want to open it via browser, if you click on it, it will open up a page with error message\nSolution : you should instead hover on the long link, and ctrl + click the long link\n\nClick configure Trusted Domains here\n\nPopup will appear, pick first or second entry\nNext time you gcloud auth, the login page should popup via default browser without issues",
        "question": "What happens the next time you run gcloud auth after following the solution provided?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 142,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is an internet connectivity error, terraform is somehow not able to access the online registry. Check your VPN/Firewall settings (or just clear cookies or restart your network). Try terraform init again after this, it should work.",
        "question": "What is the nature of the error mentioned in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 143,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is an internet connectivity error, terraform is somehow not able to access the online registry. Check your VPN/Firewall settings (or just clear cookies or restart your network). Try terraform init again after this, it should work.",
        "question": "What might be the causes of the internet connectivity error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 143,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is an internet connectivity error, terraform is somehow not able to access the online registry. Check your VPN/Firewall settings (or just clear cookies or restart your network). Try terraform init again after this, it should work.",
        "question": "What steps can you take to resolve the error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 143,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is an internet connectivity error, terraform is somehow not able to access the online registry. Check your VPN/Firewall settings (or just clear cookies or restart your network). Try terraform init again after this, it should work.",
        "question": "What command should be tried again after making adjustments?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 143,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is an internet connectivity error, terraform is somehow not able to access the online registry. Check your VPN/Firewall settings (or just clear cookies or restart your network). Try terraform init again after this, it should work.",
        "question": "What specific settings should be checked to troubleshoot the issue?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 143,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.",
        "question": "What was the main issue experienced by the user?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 144,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.",
        "question": "Why is Google not accessible to the user?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 144,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.",
        "question": "What does the terminal program require for proxy settings?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 144,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.",
        "question": "What mode was opened in the Clash VPN app?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 144,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.",
        "question": "What should a user do if they encounter the same issue?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 144,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845",
        "question": "What are the prerequisites for configuring Terraform on Windows 10 Subsystem for Linux?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 145,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845",
        "question": "Which specific steps are involved in installing Terraform on Windows 10 WSL?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 145,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845",
        "question": "How can users verify that Terraform has been successfully installed on their system?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 145,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845",
        "question": "What are some common issues encountered during the configuration of Terraform on Windows 10 WSL?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 145,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845",
        "question": "Is there a difference in Terraform setup when using Windows 10 compared to other operating systems?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 145,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When running\nterraform apply\non wsl2 I've got this error:\n│ Error: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\n│ Response: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\nIT happens because there may be time desync on your machine which affects computing JWT\nTo fix this, run the command\nsudo hwclock -s\nwhich fixes your system time.\nReference",
        "question": "What command is used to fix the time desync issue while running terraform apply on wsl2?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 147,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When running\nterraform apply\non wsl2 I've got this error:\n│ Error: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\n│ Response: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\nIT happens because there may be time desync on your machine which affects computing JWT\nTo fix this, run the command\nsudo hwclock -s\nwhich fixes your system time.\nReference",
        "question": "What error message is displayed when there is a problem with the JWT token while running terraform apply?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 147,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When running\nterraform apply\non wsl2 I've got this error:\n│ Error: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\n│ Response: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\nIT happens because there may be time desync on your machine which affects computing JWT\nTo fix this, run the command\nsudo hwclock -s\nwhich fixes your system time.\nReference",
        "question": "What does the error description 'Invalid JWT: Token must be a short-lived token (60 minutes)' indicate?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 147,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When running\nterraform apply\non wsl2 I've got this error:\n│ Error: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\n│ Response: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\nIT happens because there may be time desync on your machine which affects computing JWT\nTo fix this, run the command\nsudo hwclock -s\nwhich fixes your system time.\nReference",
        "question": "Why does the time desync on your machine affect computing JWT?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 147,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When running\nterraform apply\non wsl2 I've got this error:\n│ Error: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\n│ Response: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\nIT happens because there may be time desync on your machine which affects computing JWT\nTo fix this, run the command\nsudo hwclock -s\nwhich fixes your system time.\nReference",
        "question": "What does the command 'sudo hwclock -s' do in the context of this issue?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 147,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "│ Error: googleapi: Error 403: Access denied., forbidden\nYour $GOOGLE_APPLICATION_CREDENTIALS might not be pointing to the correct file \nrun = export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json\nAnd then = gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS",
        "question": "What does the error message 'googleapi: Error 403: Access denied' indicate?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 148,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "│ Error: googleapi: Error 403: Access denied., forbidden\nYour $GOOGLE_APPLICATION_CREDENTIALS might not be pointing to the correct file \nrun = export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json\nAnd then = gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS",
        "question": "How can you resolve issues related to incorrect GOOGLE_APPLICATION_CREDENTIALS?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 148,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "│ Error: googleapi: Error 403: Access denied., forbidden\nYour $GOOGLE_APPLICATION_CREDENTIALS might not be pointing to the correct file \nrun = export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json\nAnd then = gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS",
        "question": "What command is used to set the GOOGLE_APPLICATION_CREDENTIALS environment variable?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 148,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "│ Error: googleapi: Error 403: Access denied., forbidden\nYour $GOOGLE_APPLICATION_CREDENTIALS might not be pointing to the correct file \nrun = export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json\nAnd then = gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS",
        "question": "What is the purpose of the command 'gcloud auth activate-service-account'?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 148,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "│ Error: googleapi: Error 403: Access denied., forbidden\nYour $GOOGLE_APPLICATION_CREDENTIALS might not be pointing to the correct file \nrun = export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json\nAnd then = gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS",
        "question": "What should the path in $GOOGLE_APPLICATION_CREDENTIALS point to?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 148,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "One service account is enough for all the services/resources you'll use in this course. After you get the file with your credentials and set your environment variable, you should be good to go.",
        "question": "What is the purpose of the service account mentioned in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 149,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "One service account is enough for all the services/resources you'll use in this course. After you get the file with your credentials and set your environment variable, you should be good to go.",
        "question": "How many service accounts are needed for the course?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 149,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "One service account is enough for all the services/resources you'll use in this course. After you get the file with your credentials and set your environment variable, you should be good to go.",
        "question": "What should you do after receiving the file with your credentials?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 149,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "One service account is enough for all the services/resources you'll use in this course. After you get the file with your credentials and set your environment variable, you should be good to go.",
        "question": "What needs to be set after obtaining the credentials?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 149,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "One service account is enough for all the services/resources you'll use in this course. After you get the file with your credentials and set your environment variable, you should be good to go.",
        "question": "What does 'you should be good to go' imply about the setup process?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 149,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip",
        "question": "What is the version of Terraform mentioned in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 150,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip",
        "question": "Which operating system is specified for the Terraform download?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 150,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip",
        "question": "What type of file is linked in the provided URL?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 150,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip",
        "question": "What is the architecture type of the Terraform version listed?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 150,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip",
        "question": "Where can the Terraform download be found?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 150,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command.",
        "question": "What command is mentioned in the text that results in an error when run outside the working directory?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 151,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command.",
        "question": "Why does running 'terraform init' outside the working directory cause an error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 151,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command.",
        "question": "What should you do before running the 'terraform init' command?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 151,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command.",
        "question": "What files must be present in the working directory for 'terraform init' to work correctly?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 151,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command.",
        "question": "What is the correct sequence of steps to run the 'terraform init' command?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 151,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The error:\nError: googleapi: Error 403: Access denied., forbidden\n│\nand\n│ Error: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.\nFor this solution make sure to run:\necho $GOOGLE_APPLICATION_CREDENTIALS\necho $?\nSolution:\nYou have to set again the GOOGLE_APPLICATION_CREDENTIALS as Alexey did in the environment set-up video in week1:\nexport GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json",
        "question": "What is the main error message mentioned in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 152,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The error:\nError: googleapi: Error 403: Access denied., forbidden\n│\nand\n│ Error: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.\nFor this solution make sure to run:\necho $GOOGLE_APPLICATION_CREDENTIALS\necho $?\nSolution:\nYou have to set again the GOOGLE_APPLICATION_CREDENTIALS as Alexey did in the environment set-up video in week1:\nexport GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json",
        "question": "What command is suggested to check the status of GOOGLE_APPLICATION_CREDENTIALS?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 152,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The error:\nError: googleapi: Error 403: Access denied., forbidden\n│\nand\n│ Error: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.\nFor this solution make sure to run:\necho $GOOGLE_APPLICATION_CREDENTIALS\necho $?\nSolution:\nYou have to set again the GOOGLE_APPLICATION_CREDENTIALS as Alexey did in the environment set-up video in week1:\nexport GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json",
        "question": "What error occurs when there are insufficient authentication scopes?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 152,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The error:\nError: googleapi: Error 403: Access denied., forbidden\n│\nand\n│ Error: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.\nFor this solution make sure to run:\necho $GOOGLE_APPLICATION_CREDENTIALS\necho $?\nSolution:\nYou have to set again the GOOGLE_APPLICATION_CREDENTIALS as Alexey did in the environment set-up video in week1:\nexport GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json",
        "question": "What environment variable needs to be set according to the solution?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 152,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The error:\nError: googleapi: Error 403: Access denied., forbidden\n│\nand\n│ Error: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.\nFor this solution make sure to run:\necho $GOOGLE_APPLICATION_CREDENTIALS\necho $?\nSolution:\nYou have to set again the GOOGLE_APPLICATION_CREDENTIALS as Alexey did in the environment set-up video in week1:\nexport GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json",
        "question": "Where can the instructions for setting up GOOGLE_APPLICATION_CREDENTIALS be found?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 152,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The error:\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\nThe solution:\nYou have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.",
        "question": "What does the error message indicate about the permissions of the service account?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 153,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The error:\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\nThe solution:\nYou have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.",
        "question": "What specific access is denied to the service account in the error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 153,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The error:\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\nThe solution:\nYou have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.",
        "question": "How can you resolve the error related to the 'storage.buckets.create' permission?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 153,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The error:\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\nThe solution:\nYou have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.",
        "question": "What should be declared as the Project ID in order to avoid the error?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 153,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The error:\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\nThe solution:\nYou have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.",
        "question": "Where can you find the Project name that should not be used as the Project ID?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 153,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "provider \"google\" {\nproject     = var.projectId\ncredentials = file(\"${var.gcpkey}\")\n#region      = var.region\nzone = var.zone\n}",
        "question": "What is the purpose of the 'provider' block in the given text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 154,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "provider \"google\" {\nproject     = var.projectId\ncredentials = file(\"${var.gcpkey}\")\n#region      = var.region\nzone = var.zone\n}",
        "question": "How is the project ID specified in the configuration?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 154,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "provider \"google\" {\nproject     = var.projectId\ncredentials = file(\"${var.gcpkey}\")\n#region      = var.region\nzone = var.zone\n}",
        "question": "What format is used to provide the credentials in the configuration?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 154,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "provider \"google\" {\nproject     = var.projectId\ncredentials = file(\"${var.gcpkey}\")\n#region      = var.region\nzone = var.zone\n}",
        "question": "Which variable is used to define the zone in the configuration?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 154,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "provider \"google\" {\nproject     = var.projectId\ncredentials = file(\"${var.gcpkey}\")\n#region      = var.region\nzone = var.zone\n}",
        "question": "Is there a line referencing the region, and if so, how is it commented out?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 154,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For the HW1 I encountered this issue. The solution is\nSELECT * FROM zones AS z WHERE z.\"Zone\" = 'Astoria Zone';\nI think columns which start with uppercase need to go between “Column”. I ran into a lot of issues like this and “ ” made it work out.\nAddition to the above point, for me, there is no ‘Astoria Zone’, only ‘Astoria’ is existing in the dataset.\nSELECT * FROM zones AS z WHERE z.\"Zone\" = 'Astoria’;",
        "question": "What issue did the author encounter in HW1?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 155,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For the HW1 I encountered this issue. The solution is\nSELECT * FROM zones AS z WHERE z.\"Zone\" = 'Astoria Zone';\nI think columns which start with uppercase need to go between “Column”. I ran into a lot of issues like this and “ ” made it work out.\nAddition to the above point, for me, there is no ‘Astoria Zone’, only ‘Astoria’ is existing in the dataset.\nSELECT * FROM zones AS z WHERE z.\"Zone\" = 'Astoria’;",
        "question": "Why does the author believe columns starting with uppercase need special treatment?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 155,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For the HW1 I encountered this issue. The solution is\nSELECT * FROM zones AS z WHERE z.\"Zone\" = 'Astoria Zone';\nI think columns which start with uppercase need to go between “Column”. I ran into a lot of issues like this and “ ” made it work out.\nAddition to the above point, for me, there is no ‘Astoria Zone’, only ‘Astoria’ is existing in the dataset.\nSELECT * FROM zones AS z WHERE z.\"Zone\" = 'Astoria’;",
        "question": "What SQL query does the author suggest to resolve the issue?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 155,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For the HW1 I encountered this issue. The solution is\nSELECT * FROM zones AS z WHERE z.\"Zone\" = 'Astoria Zone';\nI think columns which start with uppercase need to go between “Column”. I ran into a lot of issues like this and “ ” made it work out.\nAddition to the above point, for me, there is no ‘Astoria Zone’, only ‘Astoria’ is existing in the dataset.\nSELECT * FROM zones AS z WHERE z.\"Zone\" = 'Astoria’;",
        "question": "What discrepancy did the author find in the dataset regarding the 'Astoria Zone'?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 155,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For the HW1 I encountered this issue. The solution is\nSELECT * FROM zones AS z WHERE z.\"Zone\" = 'Astoria Zone';\nI think columns which start with uppercase need to go between “Column”. I ran into a lot of issues like this and “ ” made it work out.\nAddition to the above point, for me, there is no ‘Astoria Zone’, only ‘Astoria’ is existing in the dataset.\nSELECT * FROM zones AS z WHERE z.\"Zone\" = 'Astoria’;",
        "question": "What is the correct SQL query to retrieve data for 'Astoria' according to the author?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 155,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is inconvenient to use quotation marks all the time, so it is better to put the data to the database all in lowercase, so in Pandas after\ndf = pd.read_csv(‘taxi+_zone_lookup.csv’)\nAdd the row:\ndf.columns = df.columns.str.lower()",
        "question": "Why is it suggested to use lowercase for data in the database?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 156,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is inconvenient to use quotation marks all the time, so it is better to put the data to the database all in lowercase, so in Pandas after\ndf = pd.read_csv(‘taxi+_zone_lookup.csv’)\nAdd the row:\ndf.columns = df.columns.str.lower()",
        "question": "What command is used to read a CSV file in Pandas?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 156,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is inconvenient to use quotation marks all the time, so it is better to put the data to the database all in lowercase, so in Pandas after\ndf = pd.read_csv(‘taxi+_zone_lookup.csv’)\nAdd the row:\ndf.columns = df.columns.str.lower()",
        "question": "How can you convert column names to lowercase in a Pandas DataFrame?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 156,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is inconvenient to use quotation marks all the time, so it is better to put the data to the database all in lowercase, so in Pandas after\ndf = pd.read_csv(‘taxi+_zone_lookup.csv’)\nAdd the row:\ndf.columns = df.columns.str.lower()",
        "question": "What is the purpose of the line 'df.columns = df.columns.str.lower()'?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 156,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is inconvenient to use quotation marks all the time, so it is better to put the data to the database all in lowercase, so in Pandas after\ndf = pd.read_csv(‘taxi+_zone_lookup.csv’)\nAdd the row:\ndf.columns = df.columns.str.lower()",
        "question": "What might be the inconvenience of using quotation marks all the time?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 156,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution (for mac users): os.system(f\"curl {url} --output {csv_name}\")",
        "question": "What is the purpose of the command in the solution provided?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 157,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution (for mac users): os.system(f\"curl {url} --output {csv_name}\")",
        "question": "Which operating system is the solution intended for?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 157,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution (for mac users): os.system(f\"curl {url} --output {csv_name}\")",
        "question": "What command is used to download a file in the solution?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 157,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution (for mac users): os.system(f\"curl {url} --output {csv_name}\")",
        "question": "What do 'url' and 'csv_name' represent in the given code?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 157,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution (for mac users): os.system(f\"curl {url} --output {csv_name}\")",
        "question": "How can the provided solution be modified for use on non-Mac operating systems?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 157,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To resolve this, ensure that your config file is in C/User/Username/.ssh/config",
        "question": "What is the correct path for the config file mentioned in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 158,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To resolve this, ensure that your config file is in C/User/Username/.ssh/config",
        "question": "Which directory should the config file be located in?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 158,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To resolve this, ensure that your config file is in C/User/Username/.ssh/config",
        "question": "What type of file is referenced in the text?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 158,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To resolve this, ensure that your config file is in C/User/Username/.ssh/config",
        "question": "What is the role of the config file in the context provided?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 158,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To resolve this, ensure that your config file is in C/User/Username/.ssh/config",
        "question": "What should one verify to resolve the issue mentioned?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 158,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you use Anaconda (recommended for the course), it comes with pip, so the issues is probably that the anaconda’s Python is not on the PATH.\nAdding it to the PATH is different for each operation system.\nFor Linux and MacOS:\nOpen a terminal.\nFind the path to your Anaconda installation. This is typically `~/anaconda3` or `~/opt/anaconda3`.\nAdd Anaconda to your PATH with the command: `export PATH=\"/path/to/anaconda3/bin:$PATH\"`.\nTo make this change permanent, add the command to your `.bashrc` (Linux) or `.bash_profile` (MacOS) file.\nOn Windows, python and pip are in different locations (python is in the anaconda root, and pip is in Scripts). With GitBash:\nLocate your Anaconda installation. The default path is usually `C:\\Users\\[YourUsername]\\Anaconda3`.\nDetermine the correct path format for Git Bash. Paths in Git Bash follow the Unix-style, so convert the Windows path to a Unix-style path. For example, `C:\\Users\\[YourUsername]\\Anaconda3` becomes `/c/Users/[YourUsername]/Anaconda3`.\nAdd Anaconda to your PATH with the command: `export PATH=\"/c/Users/[YourUsername]/Anaconda3/:/c/Users/[YourUsername]/Anaconda3/Scripts/$PATH\"`.\nTo make this change permanent, add the command to your `.bashrc` file in your home directory.\nRefresh your environment with the command: `source ~/.bashrc`.\nFor Windows (without Git Bash):\nRight-click on 'This PC' or 'My Computer' and select 'Properties'.\nClick on 'Advanced system settings'.\nIn the System Properties window, click on 'Environment Variables'.\nIn the Environment Variables window, select the 'Path' variable in the 'System variables' section and click 'Edit'.\nIn the Edit Environment Variable window, click 'New' and add the path to your Anaconda installation (typically `C:\\Users\\[YourUsername]\\Anaconda3` and C:\\Users\\[YourUsername]\\Anaconda3\\Scripts`).\nClick 'OK' in all windows to apply the changes.\nAfter adding Anaconda to the PATH, you should be able to use `pip` from the command line. Remember to restart your terminal (or command prompt in Windows) to apply these changes.",
        "question": "What is the recommended way to set up Python for this course?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 159,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you use Anaconda (recommended for the course), it comes with pip, so the issues is probably that the anaconda’s Python is not on the PATH.\nAdding it to the PATH is different for each operation system.\nFor Linux and MacOS:\nOpen a terminal.\nFind the path to your Anaconda installation. This is typically `~/anaconda3` or `~/opt/anaconda3`.\nAdd Anaconda to your PATH with the command: `export PATH=\"/path/to/anaconda3/bin:$PATH\"`.\nTo make this change permanent, add the command to your `.bashrc` (Linux) or `.bash_profile` (MacOS) file.\nOn Windows, python and pip are in different locations (python is in the anaconda root, and pip is in Scripts). With GitBash:\nLocate your Anaconda installation. The default path is usually `C:\\Users\\[YourUsername]\\Anaconda3`.\nDetermine the correct path format for Git Bash. Paths in Git Bash follow the Unix-style, so convert the Windows path to a Unix-style path. For example, `C:\\Users\\[YourUsername]\\Anaconda3` becomes `/c/Users/[YourUsername]/Anaconda3`.\nAdd Anaconda to your PATH with the command: `export PATH=\"/c/Users/[YourUsername]/Anaconda3/:/c/Users/[YourUsername]/Anaconda3/Scripts/$PATH\"`.\nTo make this change permanent, add the command to your `.bashrc` file in your home directory.\nRefresh your environment with the command: `source ~/.bashrc`.\nFor Windows (without Git Bash):\nRight-click on 'This PC' or 'My Computer' and select 'Properties'.\nClick on 'Advanced system settings'.\nIn the System Properties window, click on 'Environment Variables'.\nIn the Environment Variables window, select the 'Path' variable in the 'System variables' section and click 'Edit'.\nIn the Edit Environment Variable window, click 'New' and add the path to your Anaconda installation (typically `C:\\Users\\[YourUsername]\\Anaconda3` and C:\\Users\\[YourUsername]\\Anaconda3\\Scripts`).\nClick 'OK' in all windows to apply the changes.\nAfter adding Anaconda to the PATH, you should be able to use `pip` from the command line. Remember to restart your terminal (or command prompt in Windows) to apply these changes.",
        "question": "How can you add Anaconda to the PATH on Linux or MacOS?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 159,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you use Anaconda (recommended for the course), it comes with pip, so the issues is probably that the anaconda’s Python is not on the PATH.\nAdding it to the PATH is different for each operation system.\nFor Linux and MacOS:\nOpen a terminal.\nFind the path to your Anaconda installation. This is typically `~/anaconda3` or `~/opt/anaconda3`.\nAdd Anaconda to your PATH with the command: `export PATH=\"/path/to/anaconda3/bin:$PATH\"`.\nTo make this change permanent, add the command to your `.bashrc` (Linux) or `.bash_profile` (MacOS) file.\nOn Windows, python and pip are in different locations (python is in the anaconda root, and pip is in Scripts). With GitBash:\nLocate your Anaconda installation. The default path is usually `C:\\Users\\[YourUsername]\\Anaconda3`.\nDetermine the correct path format for Git Bash. Paths in Git Bash follow the Unix-style, so convert the Windows path to a Unix-style path. For example, `C:\\Users\\[YourUsername]\\Anaconda3` becomes `/c/Users/[YourUsername]/Anaconda3`.\nAdd Anaconda to your PATH with the command: `export PATH=\"/c/Users/[YourUsername]/Anaconda3/:/c/Users/[YourUsername]/Anaconda3/Scripts/$PATH\"`.\nTo make this change permanent, add the command to your `.bashrc` file in your home directory.\nRefresh your environment with the command: `source ~/.bashrc`.\nFor Windows (without Git Bash):\nRight-click on 'This PC' or 'My Computer' and select 'Properties'.\nClick on 'Advanced system settings'.\nIn the System Properties window, click on 'Environment Variables'.\nIn the Environment Variables window, select the 'Path' variable in the 'System variables' section and click 'Edit'.\nIn the Edit Environment Variable window, click 'New' and add the path to your Anaconda installation (typically `C:\\Users\\[YourUsername]\\Anaconda3` and C:\\Users\\[YourUsername]\\Anaconda3\\Scripts`).\nClick 'OK' in all windows to apply the changes.\nAfter adding Anaconda to the PATH, you should be able to use `pip` from the command line. Remember to restart your terminal (or command prompt in Windows) to apply these changes.",
        "question": "What command is used to make the PATH change permanent in Linux and MacOS?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 159,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you use Anaconda (recommended for the course), it comes with pip, so the issues is probably that the anaconda’s Python is not on the PATH.\nAdding it to the PATH is different for each operation system.\nFor Linux and MacOS:\nOpen a terminal.\nFind the path to your Anaconda installation. This is typically `~/anaconda3` or `~/opt/anaconda3`.\nAdd Anaconda to your PATH with the command: `export PATH=\"/path/to/anaconda3/bin:$PATH\"`.\nTo make this change permanent, add the command to your `.bashrc` (Linux) or `.bash_profile` (MacOS) file.\nOn Windows, python and pip are in different locations (python is in the anaconda root, and pip is in Scripts). With GitBash:\nLocate your Anaconda installation. The default path is usually `C:\\Users\\[YourUsername]\\Anaconda3`.\nDetermine the correct path format for Git Bash. Paths in Git Bash follow the Unix-style, so convert the Windows path to a Unix-style path. For example, `C:\\Users\\[YourUsername]\\Anaconda3` becomes `/c/Users/[YourUsername]/Anaconda3`.\nAdd Anaconda to your PATH with the command: `export PATH=\"/c/Users/[YourUsername]/Anaconda3/:/c/Users/[YourUsername]/Anaconda3/Scripts/$PATH\"`.\nTo make this change permanent, add the command to your `.bashrc` file in your home directory.\nRefresh your environment with the command: `source ~/.bashrc`.\nFor Windows (without Git Bash):\nRight-click on 'This PC' or 'My Computer' and select 'Properties'.\nClick on 'Advanced system settings'.\nIn the System Properties window, click on 'Environment Variables'.\nIn the Environment Variables window, select the 'Path' variable in the 'System variables' section and click 'Edit'.\nIn the Edit Environment Variable window, click 'New' and add the path to your Anaconda installation (typically `C:\\Users\\[YourUsername]\\Anaconda3` and C:\\Users\\[YourUsername]\\Anaconda3\\Scripts`).\nClick 'OK' in all windows to apply the changes.\nAfter adding Anaconda to the PATH, you should be able to use `pip` from the command line. Remember to restart your terminal (or command prompt in Windows) to apply these changes.",
        "question": "What steps should be followed to add Anaconda to the PATH on Windows without Git Bash?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 159,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you use Anaconda (recommended for the course), it comes with pip, so the issues is probably that the anaconda’s Python is not on the PATH.\nAdding it to the PATH is different for each operation system.\nFor Linux and MacOS:\nOpen a terminal.\nFind the path to your Anaconda installation. This is typically `~/anaconda3` or `~/opt/anaconda3`.\nAdd Anaconda to your PATH with the command: `export PATH=\"/path/to/anaconda3/bin:$PATH\"`.\nTo make this change permanent, add the command to your `.bashrc` (Linux) or `.bash_profile` (MacOS) file.\nOn Windows, python and pip are in different locations (python is in the anaconda root, and pip is in Scripts). With GitBash:\nLocate your Anaconda installation. The default path is usually `C:\\Users\\[YourUsername]\\Anaconda3`.\nDetermine the correct path format for Git Bash. Paths in Git Bash follow the Unix-style, so convert the Windows path to a Unix-style path. For example, `C:\\Users\\[YourUsername]\\Anaconda3` becomes `/c/Users/[YourUsername]/Anaconda3`.\nAdd Anaconda to your PATH with the command: `export PATH=\"/c/Users/[YourUsername]/Anaconda3/:/c/Users/[YourUsername]/Anaconda3/Scripts/$PATH\"`.\nTo make this change permanent, add the command to your `.bashrc` file in your home directory.\nRefresh your environment with the command: `source ~/.bashrc`.\nFor Windows (without Git Bash):\nRight-click on 'This PC' or 'My Computer' and select 'Properties'.\nClick on 'Advanced system settings'.\nIn the System Properties window, click on 'Environment Variables'.\nIn the Environment Variables window, select the 'Path' variable in the 'System variables' section and click 'Edit'.\nIn the Edit Environment Variable window, click 'New' and add the path to your Anaconda installation (typically `C:\\Users\\[YourUsername]\\Anaconda3` and C:\\Users\\[YourUsername]\\Anaconda3\\Scripts`).\nClick 'OK' in all windows to apply the changes.\nAfter adding Anaconda to the PATH, you should be able to use `pip` from the command line. Remember to restart your terminal (or command prompt in Windows) to apply these changes.",
        "question": "Why is it necessary to restart the terminal or command prompt after adding Anaconda to the PATH?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 159,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Resolution: You need to stop the services which is using the port.\nRun the following:\n```\nsudo kill -9 `sudo lsof -t -i:<port>`\n```\n<port> being 8080 in this case. This will free up the port for use.\n~ Abhijit Chakraborty\nError: error response from daemon: cannot stop container: 1afaf8f7d52277318b71eef8f7a7f238c777045e769dd832426219d6c4b8dfb4: permission denied\nResolution: In my case, I had to stop docker and restart the service to get it running properly\nUse the following command:\n```\nsudo systemctl restart docker.socket docker.service\n```\n~ Abhijit Chakraborty\nError: cannot import module psycopg2\nResolution: Run the following command in linux:\n```\nsudo apt-get install libpq-dev\npip install psycopg2\n```\n~ Abhijit Chakraborty\nError: docker build Error checking context: 'can't stat '<path-to-file>'\nResolution: This happens due to insufficient permission for docker to access a certain file within the directory which hosts the Dockerfile.\n1. You can create a .dockerignore file and add the directory/file which you want Dockerfile to ignore while build.\n2. If the above does not work, then put the dockerfile and corresponding script, `\t1.py` in our case to a subfolder. and run `docker build ...`\nfrom inside the new folder.\n~ Abhijit Chakraborty",
        "question": "What command is used to free up a port that is currently in use?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 160,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Resolution: You need to stop the services which is using the port.\nRun the following:\n```\nsudo kill -9 `sudo lsof -t -i:<port>`\n```\n<port> being 8080 in this case. This will free up the port for use.\n~ Abhijit Chakraborty\nError: error response from daemon: cannot stop container: 1afaf8f7d52277318b71eef8f7a7f238c777045e769dd832426219d6c4b8dfb4: permission denied\nResolution: In my case, I had to stop docker and restart the service to get it running properly\nUse the following command:\n```\nsudo systemctl restart docker.socket docker.service\n```\n~ Abhijit Chakraborty\nError: cannot import module psycopg2\nResolution: Run the following command in linux:\n```\nsudo apt-get install libpq-dev\npip install psycopg2\n```\n~ Abhijit Chakraborty\nError: docker build Error checking context: 'can't stat '<path-to-file>'\nResolution: This happens due to insufficient permission for docker to access a certain file within the directory which hosts the Dockerfile.\n1. You can create a .dockerignore file and add the directory/file which you want Dockerfile to ignore while build.\n2. If the above does not work, then put the dockerfile and corresponding script, `\t1.py` in our case to a subfolder. and run `docker build ...`\nfrom inside the new folder.\n~ Abhijit Chakraborty",
        "question": "What steps should be taken if there is a permission denied error when stopping a Docker container?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 160,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Resolution: You need to stop the services which is using the port.\nRun the following:\n```\nsudo kill -9 `sudo lsof -t -i:<port>`\n```\n<port> being 8080 in this case. This will free up the port for use.\n~ Abhijit Chakraborty\nError: error response from daemon: cannot stop container: 1afaf8f7d52277318b71eef8f7a7f238c777045e769dd832426219d6c4b8dfb4: permission denied\nResolution: In my case, I had to stop docker and restart the service to get it running properly\nUse the following command:\n```\nsudo systemctl restart docker.socket docker.service\n```\n~ Abhijit Chakraborty\nError: cannot import module psycopg2\nResolution: Run the following command in linux:\n```\nsudo apt-get install libpq-dev\npip install psycopg2\n```\n~ Abhijit Chakraborty\nError: docker build Error checking context: 'can't stat '<path-to-file>'\nResolution: This happens due to insufficient permission for docker to access a certain file within the directory which hosts the Dockerfile.\n1. You can create a .dockerignore file and add the directory/file which you want Dockerfile to ignore while build.\n2. If the above does not work, then put the dockerfile and corresponding script, `\t1.py` in our case to a subfolder. and run `docker build ...`\nfrom inside the new folder.\n~ Abhijit Chakraborty",
        "question": "Which command is recommended to install the psycopg2 module in Linux?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 160,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Resolution: You need to stop the services which is using the port.\nRun the following:\n```\nsudo kill -9 `sudo lsof -t -i:<port>`\n```\n<port> being 8080 in this case. This will free up the port for use.\n~ Abhijit Chakraborty\nError: error response from daemon: cannot stop container: 1afaf8f7d52277318b71eef8f7a7f238c777045e769dd832426219d6c4b8dfb4: permission denied\nResolution: In my case, I had to stop docker and restart the service to get it running properly\nUse the following command:\n```\nsudo systemctl restart docker.socket docker.service\n```\n~ Abhijit Chakraborty\nError: cannot import module psycopg2\nResolution: Run the following command in linux:\n```\nsudo apt-get install libpq-dev\npip install psycopg2\n```\n~ Abhijit Chakraborty\nError: docker build Error checking context: 'can't stat '<path-to-file>'\nResolution: This happens due to insufficient permission for docker to access a certain file within the directory which hosts the Dockerfile.\n1. You can create a .dockerignore file and add the directory/file which you want Dockerfile to ignore while build.\n2. If the above does not work, then put the dockerfile and corresponding script, `\t1.py` in our case to a subfolder. and run `docker build ...`\nfrom inside the new folder.\n~ Abhijit Chakraborty",
        "question": "What causes the error related to checking context during a Docker build?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 160,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Resolution: You need to stop the services which is using the port.\nRun the following:\n```\nsudo kill -9 `sudo lsof -t -i:<port>`\n```\n<port> being 8080 in this case. This will free up the port for use.\n~ Abhijit Chakraborty\nError: error response from daemon: cannot stop container: 1afaf8f7d52277318b71eef8f7a7f238c777045e769dd832426219d6c4b8dfb4: permission denied\nResolution: In my case, I had to stop docker and restart the service to get it running properly\nUse the following command:\n```\nsudo systemctl restart docker.socket docker.service\n```\n~ Abhijit Chakraborty\nError: cannot import module psycopg2\nResolution: Run the following command in linux:\n```\nsudo apt-get install libpq-dev\npip install psycopg2\n```\n~ Abhijit Chakraborty\nError: docker build Error checking context: 'can't stat '<path-to-file>'\nResolution: This happens due to insufficient permission for docker to access a certain file within the directory which hosts the Dockerfile.\n1. You can create a .dockerignore file and add the directory/file which you want Dockerfile to ignore while build.\n2. If the above does not work, then put the dockerfile and corresponding script, `\t1.py` in our case to a subfolder. and run `docker build ...`\nfrom inside the new folder.\n~ Abhijit Chakraborty",
        "question": "What are two solutions provided for insufficient permissions when building a Docker image?",
        "section": "Module 1: Docker and Terraform",
        "document_id": 160,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To get a pip-friendly requirements.txt file file from Anaconda use\nconda install pip then `pip list –format=freeze > requirements.txt`.\n`conda list -d > requirements.txt` will not work and `pip freeze > requirements.txt` may give odd pathing.",
        "question": "What command is used to install pip in Anaconda?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 161,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To get a pip-friendly requirements.txt file file from Anaconda use\nconda install pip then `pip list –format=freeze > requirements.txt`.\n`conda list -d > requirements.txt` will not work and `pip freeze > requirements.txt` may give odd pathing.",
        "question": "How can you create a pip-friendly requirements.txt file using Anaconda?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 161,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To get a pip-friendly requirements.txt file file from Anaconda use\nconda install pip then `pip list –format=freeze > requirements.txt`.\n`conda list -d > requirements.txt` will not work and `pip freeze > requirements.txt` may give odd pathing.",
        "question": "Why might 'conda list -d > requirements.txt' not work for generating a requirements file?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 161,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To get a pip-friendly requirements.txt file file from Anaconda use\nconda install pip then `pip list –format=freeze > requirements.txt`.\n`conda list -d > requirements.txt` will not work and `pip freeze > requirements.txt` may give odd pathing.",
        "question": "What issue can occur when using 'pip freeze > requirements.txt'?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 161,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To get a pip-friendly requirements.txt file file from Anaconda use\nconda install pip then `pip list –format=freeze > requirements.txt`.\n`conda list -d > requirements.txt` will not work and `pip freeze > requirements.txt` may give odd pathing.",
        "question": "What does the command 'pip list --format=freeze > requirements.txt' accomplish?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 161,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Prefect: https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing\nAirflow: https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing",
        "question": "What is the main purpose of Prefect?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 162,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Prefect: https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing\nAirflow: https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing",
        "question": "How does Airflow handle task scheduling?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 162,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Prefect: https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing\nAirflow: https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing",
        "question": "What are the key features of Prefect mentioned in the document?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 162,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Prefect: https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing\nAirflow: https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing",
        "question": "In what scenarios would one choose Airflow over Prefect?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 162,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Prefect: https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing\nAirflow: https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing",
        "question": "Are there any limitations associated with using either Prefect or Airflow?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 162,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue : Docker containers exit instantly with code 132, upon docker compose up\nMage documentation has it listing the cause as \"older architecture\" .\nThis might be a hardware issue, so unless you have another computer, you can't solve it without purchasing a new one, so the next best solution is a VM.\nThis is from a student running on a VirtualBox VM, Ubuntu 22.04.3 LTS, Docker version 25.0.2. So not having the context on how the vbox was spin up with (CPU, RAM, network, etc), it’s really inconclusive at this time.",
        "question": "What does the exit code 132 indicate when Docker containers exit instantly?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 163,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue : Docker containers exit instantly with code 132, upon docker compose up\nMage documentation has it listing the cause as \"older architecture\" .\nThis might be a hardware issue, so unless you have another computer, you can't solve it without purchasing a new one, so the next best solution is a VM.\nThis is from a student running on a VirtualBox VM, Ubuntu 22.04.3 LTS, Docker version 25.0.2. So not having the context on how the vbox was spin up with (CPU, RAM, network, etc), it’s really inconclusive at this time.",
        "question": "What is the possible cause of Docker containers exiting as referenced in the Mage documentation?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 163,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue : Docker containers exit instantly with code 132, upon docker compose up\nMage documentation has it listing the cause as \"older architecture\" .\nThis might be a hardware issue, so unless you have another computer, you can't solve it without purchasing a new one, so the next best solution is a VM.\nThis is from a student running on a VirtualBox VM, Ubuntu 22.04.3 LTS, Docker version 25.0.2. So not having the context on how the vbox was spin up with (CPU, RAM, network, etc), it’s really inconclusive at this time.",
        "question": "What alternative solution is suggested if the issue is due to older architecture?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 163,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue : Docker containers exit instantly with code 132, upon docker compose up\nMage documentation has it listing the cause as \"older architecture\" .\nThis might be a hardware issue, so unless you have another computer, you can't solve it without purchasing a new one, so the next best solution is a VM.\nThis is from a student running on a VirtualBox VM, Ubuntu 22.04.3 LTS, Docker version 25.0.2. So not having the context on how the vbox was spin up with (CPU, RAM, network, etc), it’s really inconclusive at this time.",
        "question": "What system is the student using to run Docker, and what are its specifications?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 163,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue : Docker containers exit instantly with code 132, upon docker compose up\nMage documentation has it listing the cause as \"older architecture\" .\nThis might be a hardware issue, so unless you have another computer, you can't solve it without purchasing a new one, so the next best solution is a VM.\nThis is from a student running on a VirtualBox VM, Ubuntu 22.04.3 LTS, Docker version 25.0.2. So not having the context on how the vbox was spin up with (CPU, RAM, network, etc), it’s really inconclusive at this time.",
        "question": "Why is the context of the VirtualBox setup important for diagnosing the issue?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 163,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This issue was occurring with Windows WSL 2\nFor me this was because WSL 2 was not dedicating enough cpu cores to Docker.The load seems to take up at least one cpu core so I recommend dedicating at least two.\nOpen Bash and run the following code:\n$ cd ~\n$ ls -la\nLook for the .wsl config file:\n-rw-r--r-- 1 ~1049089       31 Jan 25 12:54  .wslconfig\nUsing a text editing tool of your choice edit or create your .wslconfig file:\n$ nano .wslconfig\nPaste the following into the new file/ edit the existing file in this format and save:\n*** Note - for memory– this is the RAM on your machine you can dedicate to Docker, your situation may be different than mine ***\n[wsl2]\nprocessors=<Number of Processors - at least 2!> example: 4\nmemory=<memory> example:4GB\nExample:\nOnce you do that run:\n$ wsl --shutdown\nThis shuts down WSL\nThen Restart Docker Desktop - You should now be able to load the .csv.gz file without the error into a pandas dataframe",
        "question": "What issue was occurring with Windows WSL 2?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 164,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This issue was occurring with Windows WSL 2\nFor me this was because WSL 2 was not dedicating enough cpu cores to Docker.The load seems to take up at least one cpu core so I recommend dedicating at least two.\nOpen Bash and run the following code:\n$ cd ~\n$ ls -la\nLook for the .wsl config file:\n-rw-r--r-- 1 ~1049089       31 Jan 25 12:54  .wslconfig\nUsing a text editing tool of your choice edit or create your .wslconfig file:\n$ nano .wslconfig\nPaste the following into the new file/ edit the existing file in this format and save:\n*** Note - for memory– this is the RAM on your machine you can dedicate to Docker, your situation may be different than mine ***\n[wsl2]\nprocessors=<Number of Processors - at least 2!> example: 4\nmemory=<memory> example:4GB\nExample:\nOnce you do that run:\n$ wsl --shutdown\nThis shuts down WSL\nThen Restart Docker Desktop - You should now be able to load the .csv.gz file without the error into a pandas dataframe",
        "question": "Why is it recommended to dedicate at least two CPU cores to Docker?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 164,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This issue was occurring with Windows WSL 2\nFor me this was because WSL 2 was not dedicating enough cpu cores to Docker.The load seems to take up at least one cpu core so I recommend dedicating at least two.\nOpen Bash and run the following code:\n$ cd ~\n$ ls -la\nLook for the .wsl config file:\n-rw-r--r-- 1 ~1049089       31 Jan 25 12:54  .wslconfig\nUsing a text editing tool of your choice edit or create your .wslconfig file:\n$ nano .wslconfig\nPaste the following into the new file/ edit the existing file in this format and save:\n*** Note - for memory– this is the RAM on your machine you can dedicate to Docker, your situation may be different than mine ***\n[wsl2]\nprocessors=<Number of Processors - at least 2!> example: 4\nmemory=<memory> example:4GB\nExample:\nOnce you do that run:\n$ wsl --shutdown\nThis shuts down WSL\nThen Restart Docker Desktop - You should now be able to load the .csv.gz file without the error into a pandas dataframe",
        "question": "How can you locate the .wslconfig file in your system?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 164,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This issue was occurring with Windows WSL 2\nFor me this was because WSL 2 was not dedicating enough cpu cores to Docker.The load seems to take up at least one cpu core so I recommend dedicating at least two.\nOpen Bash and run the following code:\n$ cd ~\n$ ls -la\nLook for the .wsl config file:\n-rw-r--r-- 1 ~1049089       31 Jan 25 12:54  .wslconfig\nUsing a text editing tool of your choice edit or create your .wslconfig file:\n$ nano .wslconfig\nPaste the following into the new file/ edit the existing file in this format and save:\n*** Note - for memory– this is the RAM on your machine you can dedicate to Docker, your situation may be different than mine ***\n[wsl2]\nprocessors=<Number of Processors - at least 2!> example: 4\nmemory=<memory> example:4GB\nExample:\nOnce you do that run:\n$ wsl --shutdown\nThis shuts down WSL\nThen Restart Docker Desktop - You should now be able to load the .csv.gz file without the error into a pandas dataframe",
        "question": "What commands are used to edit the .wslconfig file?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 164,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This issue was occurring with Windows WSL 2\nFor me this was because WSL 2 was not dedicating enough cpu cores to Docker.The load seems to take up at least one cpu core so I recommend dedicating at least two.\nOpen Bash and run the following code:\n$ cd ~\n$ ls -la\nLook for the .wsl config file:\n-rw-r--r-- 1 ~1049089       31 Jan 25 12:54  .wslconfig\nUsing a text editing tool of your choice edit or create your .wslconfig file:\n$ nano .wslconfig\nPaste the following into the new file/ edit the existing file in this format and save:\n*** Note - for memory– this is the RAM on your machine you can dedicate to Docker, your situation may be different than mine ***\n[wsl2]\nprocessors=<Number of Processors - at least 2!> example: 4\nmemory=<memory> example:4GB\nExample:\nOnce you do that run:\n$ wsl --shutdown\nThis shuts down WSL\nThen Restart Docker Desktop - You should now be able to load the .csv.gz file without the error into a pandas dataframe",
        "question": "What steps should you take after modifying the .wslconfig file to resolve the loading error?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 164,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The issue and solution on the link:\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1706817366764269?thread_ts=1706815324.993529&cid=C01FABYF2RG",
        "question": "What is the main issue discussed in the provided link?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 165,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The issue and solution on the link:\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1706817366764269?thread_ts=1706815324.993529&cid=C01FABYF2RG",
        "question": "What solution is proposed in the content of the link?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 165,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The issue and solution on the link:\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1706817366764269?thread_ts=1706815324.993529&cid=C01FABYF2RG",
        "question": "How does the thread relate to data discussions?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 165,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The issue and solution on the link:\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1706817366764269?thread_ts=1706815324.993529&cid=C01FABYF2RG",
        "question": "What specific timestamp is referenced for the thread in the link?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 165,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The issue and solution on the link:\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1706817366764269?thread_ts=1706815324.993529&cid=C01FABYF2RG",
        "question": "Who is the author or participant involved in the conversation linked?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 165,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check that the POSTGRES_PORT variable in the io_config.yml  file is set to port 5432, which is the default postgres port. The POSTGRES_PORT variable is the mage container port, not the host port. Hence, there’s no need to set the POSTGRES_PORT to 5431 just because you already have a conflicting postgres installation in your host machine.",
        "question": "What should the POSTGRES_PORT variable be set to in the io_config.yml file?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 166,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check that the POSTGRES_PORT variable in the io_config.yml  file is set to port 5432, which is the default postgres port. The POSTGRES_PORT variable is the mage container port, not the host port. Hence, there’s no need to set the POSTGRES_PORT to 5431 just because you already have a conflicting postgres installation in your host machine.",
        "question": "What does the POSTGRES_PORT variable represent?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 166,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check that the POSTGRES_PORT variable in the io_config.yml  file is set to port 5432, which is the default postgres port. The POSTGRES_PORT variable is the mage container port, not the host port. Hence, there’s no need to set the POSTGRES_PORT to 5431 just because you already have a conflicting postgres installation in your host machine.",
        "question": "Is it necessary to change the POSTGRES_PORT if there is a conflicting PostgreSQL installation on the host machine?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 166,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check that the POSTGRES_PORT variable in the io_config.yml  file is set to port 5432, which is the default postgres port. The POSTGRES_PORT variable is the mage container port, not the host port. Hence, there’s no need to set the POSTGRES_PORT to 5431 just because you already have a conflicting postgres installation in your host machine.",
        "question": "What port does PostgreSQL typically use by default?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 166,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check that the POSTGRES_PORT variable in the io_config.yml  file is set to port 5432, which is the default postgres port. The POSTGRES_PORT variable is the mage container port, not the host port. Hence, there’s no need to set the POSTGRES_PORT to 5431 just because you already have a conflicting postgres installation in your host machine.",
        "question": "Where can the POSTGRES_PORT variable be found?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 166,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You forgot to select ‘dev’ profile in the dropdown menu next to where you select ‘PostgreSQL’ in the connection drop down.",
        "question": "What do you need to select in the dropdown menu?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 167,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You forgot to select ‘dev’ profile in the dropdown menu next to where you select ‘PostgreSQL’ in the connection drop down.",
        "question": "Which database is mentioned in the connection dropdown?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 167,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You forgot to select ‘dev’ profile in the dropdown menu next to where you select ‘PostgreSQL’ in the connection drop down.",
        "question": "What profile should be selected to avoid the issue?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 167,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You forgot to select ‘dev’ profile in the dropdown menu next to where you select ‘PostgreSQL’ in the connection drop down.",
        "question": "Where is the 'dev' profile located?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 167,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You forgot to select ‘dev’ profile in the dropdown menu next to where you select ‘PostgreSQL’ in the connection drop down.",
        "question": "What happens if the 'dev' profile is not selected?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 167,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are getting this error. Update your mage io_config.yaml file, and specify a timeout value set to 600 like this.\nMake sure to save your changes.\nMAGE - 2.2.4 Testing BigQuery connection using SQL 404 error:\nNotFound: 404 Not found: Dataset ny-rides-diegogutierrez:None was not found in location northamerica-northeast1\nIf you get this error even with all roles/permissions given to the service account check if you have ticked the box where it says “Use raw SQL”, just like the image below.",
        "question": "What should you update in the mage io_config.yaml file to avoid the error?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 168,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are getting this error. Update your mage io_config.yaml file, and specify a timeout value set to 600 like this.\nMake sure to save your changes.\nMAGE - 2.2.4 Testing BigQuery connection using SQL 404 error:\nNotFound: 404 Not found: Dataset ny-rides-diegogutierrez:None was not found in location northamerica-northeast1\nIf you get this error even with all roles/permissions given to the service account check if you have ticked the box where it says “Use raw SQL”, just like the image below.",
        "question": "What timeout value should be set in the mage io_config.yaml file?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 168,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are getting this error. Update your mage io_config.yaml file, and specify a timeout value set to 600 like this.\nMake sure to save your changes.\nMAGE - 2.2.4 Testing BigQuery connection using SQL 404 error:\nNotFound: 404 Not found: Dataset ny-rides-diegogutierrez:None was not found in location northamerica-northeast1\nIf you get this error even with all roles/permissions given to the service account check if you have ticked the box where it says “Use raw SQL”, just like the image below.",
        "question": "What error is associated with the BigQuery connection mentioned in the text?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 168,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are getting this error. Update your mage io_config.yaml file, and specify a timeout value set to 600 like this.\nMake sure to save your changes.\nMAGE - 2.2.4 Testing BigQuery connection using SQL 404 error:\nNotFound: 404 Not found: Dataset ny-rides-diegogutierrez:None was not found in location northamerica-northeast1\nIf you get this error even with all roles/permissions given to the service account check if you have ticked the box where it says “Use raw SQL”, just like the image below.",
        "question": "What additional step should you check if you encounter a NotFound error despite having the required permissions?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 168,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are getting this error. Update your mage io_config.yaml file, and specify a timeout value set to 600 like this.\nMake sure to save your changes.\nMAGE - 2.2.4 Testing BigQuery connection using SQL 404 error:\nNotFound: 404 Not found: Dataset ny-rides-diegogutierrez:None was not found in location northamerica-northeast1\nIf you get this error even with all roles/permissions given to the service account check if you have ticked the box where it says “Use raw SQL”, just like the image below.",
        "question": "What location is specified in the error message regarding the dataset not being found?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 168,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution: https://stackoverflow.com/questions/48056381/google-client-invalid-jwt-token-must-be-a-short-lived-token",
        "question": "What is the link provided in the text?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 169,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution: https://stackoverflow.com/questions/48056381/google-client-invalid-jwt-token-must-be-a-short-lived-token",
        "question": "What issue is addressed in the linked Stack Overflow question?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 169,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution: https://stackoverflow.com/questions/48056381/google-client-invalid-jwt-token-must-be-a-short-lived-token",
        "question": "What type of token is mentioned in the solution?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 169,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution: https://stackoverflow.com/questions/48056381/google-client-invalid-jwt-token-must-be-a-short-lived-token",
        "question": "Where can one find a solution to the invalid JWT token issue?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 169,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution: https://stackoverflow.com/questions/48056381/google-client-invalid-jwt-token-must-be-a-short-lived-token",
        "question": "What is the significance of a short-lived token in this context?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 169,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Origin of Solution (Mage Slack-Channel): https://mageai.slack.com/archives/C03HTTWFEKE/p1706543947795599\nProblem: This error can often be seen after solving the error mentioned in 2.2.4. The error can be found in Mage version 0.9.61 and is a side-effect of the update of the code for data-loader blocks.\nNote: Mage 0.9.62 has been released, as of Feb 5 2024. Please recheck. Solution below may be obsolete\nSolution: Using a “fixed” version of the docker container\nPull updated docker image from docker-hub\nmageai/mageaidocker pull:alpha\nUpdate docker-compose.yaml\nversion: '3'\nservices:\nmagic:\nimage: mageai/mageai:alpha  <--- instead of “latest”-tag\ndocker-compose up\nThe original Error is still present, but the SQL-query will return the desired result:\n--------------------------------------------------------------------------------------",
        "question": "What is the origin of the solution mentioned in the text?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 170,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Origin of Solution (Mage Slack-Channel): https://mageai.slack.com/archives/C03HTTWFEKE/p1706543947795599\nProblem: This error can often be seen after solving the error mentioned in 2.2.4. The error can be found in Mage version 0.9.61 and is a side-effect of the update of the code for data-loader blocks.\nNote: Mage 0.9.62 has been released, as of Feb 5 2024. Please recheck. Solution below may be obsolete\nSolution: Using a “fixed” version of the docker container\nPull updated docker image from docker-hub\nmageai/mageaidocker pull:alpha\nUpdate docker-compose.yaml\nversion: '3'\nservices:\nmagic:\nimage: mageai/mageai:alpha  <--- instead of “latest”-tag\ndocker-compose up\nThe original Error is still present, but the SQL-query will return the desired result:\n--------------------------------------------------------------------------------------",
        "question": "Which version of Mage is associated with the error described?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 170,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Origin of Solution (Mage Slack-Channel): https://mageai.slack.com/archives/C03HTTWFEKE/p1706543947795599\nProblem: This error can often be seen after solving the error mentioned in 2.2.4. The error can be found in Mage version 0.9.61 and is a side-effect of the update of the code for data-loader blocks.\nNote: Mage 0.9.62 has been released, as of Feb 5 2024. Please recheck. Solution below may be obsolete\nSolution: Using a “fixed” version of the docker container\nPull updated docker image from docker-hub\nmageai/mageaidocker pull:alpha\nUpdate docker-compose.yaml\nversion: '3'\nservices:\nmagic:\nimage: mageai/mageai:alpha  <--- instead of “latest”-tag\ndocker-compose up\nThe original Error is still present, but the SQL-query will return the desired result:\n--------------------------------------------------------------------------------------",
        "question": "What was released on February 5, 2024, regarding Mage?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 170,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Origin of Solution (Mage Slack-Channel): https://mageai.slack.com/archives/C03HTTWFEKE/p1706543947795599\nProblem: This error can often be seen after solving the error mentioned in 2.2.4. The error can be found in Mage version 0.9.61 and is a side-effect of the update of the code for data-loader blocks.\nNote: Mage 0.9.62 has been released, as of Feb 5 2024. Please recheck. Solution below may be obsolete\nSolution: Using a “fixed” version of the docker container\nPull updated docker image from docker-hub\nmageai/mageaidocker pull:alpha\nUpdate docker-compose.yaml\nversion: '3'\nservices:\nmagic:\nimage: mageai/mageai:alpha  <--- instead of “latest”-tag\ndocker-compose up\nThe original Error is still present, but the SQL-query will return the desired result:\n--------------------------------------------------------------------------------------",
        "question": "What is the suggested solution for the error in Mage version 0.9.61?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 170,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Origin of Solution (Mage Slack-Channel): https://mageai.slack.com/archives/C03HTTWFEKE/p1706543947795599\nProblem: This error can often be seen after solving the error mentioned in 2.2.4. The error can be found in Mage version 0.9.61 and is a side-effect of the update of the code for data-loader blocks.\nNote: Mage 0.9.62 has been released, as of Feb 5 2024. Please recheck. Solution below may be obsolete\nSolution: Using a “fixed” version of the docker container\nPull updated docker image from docker-hub\nmageai/mageaidocker pull:alpha\nUpdate docker-compose.yaml\nversion: '3'\nservices:\nmagic:\nimage: mageai/mageai:alpha  <--- instead of “latest”-tag\ndocker-compose up\nThe original Error is still present, but the SQL-query will return the desired result:\n--------------------------------------------------------------------------------------",
        "question": "How should the docker image be updated according to the solution?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 170,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Add\nif not path.parent.is_dir():\npath.parent.mkdir(parents=True)\npath = Path(path).as_posix()\nsee:\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1675774214591809?thread_ts=1675768839.028879&cid=C01FABYF2RG",
        "question": "What does the code snippet check for regarding the directory path?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 171,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Add\nif not path.parent.is_dir():\npath.parent.mkdir(parents=True)\npath = Path(path).as_posix()\nsee:\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1675774214591809?thread_ts=1675768839.028879&cid=C01FABYF2RG",
        "question": "What action is taken if the parent directory does not exist?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 171,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Add\nif not path.parent.is_dir():\npath.parent.mkdir(parents=True)\npath = Path(path).as_posix()\nsee:\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1675774214591809?thread_ts=1675768839.028879&cid=C01FABYF2RG",
        "question": "How is the path converted in the code?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 171,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Add\nif not path.parent.is_dir():\npath.parent.mkdir(parents=True)\npath = Path(path).as_posix()\nsee:\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1675774214591809?thread_ts=1675768839.028879&cid=C01FABYF2RG",
        "question": "What is the purpose of the 'parents=True' argument in the mkdir method?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 171,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Add\nif not path.parent.is_dir():\npath.parent.mkdir(parents=True)\npath = Path(path).as_posix()\nsee:\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1675774214591809?thread_ts=1675768839.028879&cid=C01FABYF2RG",
        "question": "Where can additional context about this code be found?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 171,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The video DE Zoomcamp 2.2.7 is missing  the actual deployment of Mage using Terraform to GCP. The steps for the deployment were not covered in the video.\nI successfully deployed it and wanted to share some key points:\nIn variables.tf, set the project_id default value to your GCP project ID.\nEnable the Cloud Filestore API:\nVisit the Google Cloud Console.to\nNavigate to \"APIs & Services\" > \"Library.\"\nSearch for \"Cloud Filestore API.\"\nClick on the API and enable it.\nTo perform the deployment:\nterraform init\nterraform apply\nPlease note that during the terraform apply step, Terraform will prompt you to enter the PostgreSQL password. After that, it will ask for confirmation to proceed with the deployment. Review the changes, type 'yes' when prompted, and press Enter.",
        "question": "What video is mentioned as missing the deployment steps for Mage using Terraform to GCP?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 172,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The video DE Zoomcamp 2.2.7 is missing  the actual deployment of Mage using Terraform to GCP. The steps for the deployment were not covered in the video.\nI successfully deployed it and wanted to share some key points:\nIn variables.tf, set the project_id default value to your GCP project ID.\nEnable the Cloud Filestore API:\nVisit the Google Cloud Console.to\nNavigate to \"APIs & Services\" > \"Library.\"\nSearch for \"Cloud Filestore API.\"\nClick on the API and enable it.\nTo perform the deployment:\nterraform init\nterraform apply\nPlease note that during the terraform apply step, Terraform will prompt you to enter the PostgreSQL password. After that, it will ask for confirmation to proceed with the deployment. Review the changes, type 'yes' when prompted, and press Enter.",
        "question": "What file needs to have the project_id default value set to your GCP project ID?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 172,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The video DE Zoomcamp 2.2.7 is missing  the actual deployment of Mage using Terraform to GCP. The steps for the deployment were not covered in the video.\nI successfully deployed it and wanted to share some key points:\nIn variables.tf, set the project_id default value to your GCP project ID.\nEnable the Cloud Filestore API:\nVisit the Google Cloud Console.to\nNavigate to \"APIs & Services\" > \"Library.\"\nSearch for \"Cloud Filestore API.\"\nClick on the API and enable it.\nTo perform the deployment:\nterraform init\nterraform apply\nPlease note that during the terraform apply step, Terraform will prompt you to enter the PostgreSQL password. After that, it will ask for confirmation to proceed with the deployment. Review the changes, type 'yes' when prompted, and press Enter.",
        "question": "What is the first step to enable the Cloud Filestore API?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 172,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The video DE Zoomcamp 2.2.7 is missing  the actual deployment of Mage using Terraform to GCP. The steps for the deployment were not covered in the video.\nI successfully deployed it and wanted to share some key points:\nIn variables.tf, set the project_id default value to your GCP project ID.\nEnable the Cloud Filestore API:\nVisit the Google Cloud Console.to\nNavigate to \"APIs & Services\" > \"Library.\"\nSearch for \"Cloud Filestore API.\"\nClick on the API and enable it.\nTo perform the deployment:\nterraform init\nterraform apply\nPlease note that during the terraform apply step, Terraform will prompt you to enter the PostgreSQL password. After that, it will ask for confirmation to proceed with the deployment. Review the changes, type 'yes' when prompted, and press Enter.",
        "question": "What command is used to initialize Terraform for deployment?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 172,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The video DE Zoomcamp 2.2.7 is missing  the actual deployment of Mage using Terraform to GCP. The steps for the deployment were not covered in the video.\nI successfully deployed it and wanted to share some key points:\nIn variables.tf, set the project_id default value to your GCP project ID.\nEnable the Cloud Filestore API:\nVisit the Google Cloud Console.to\nNavigate to \"APIs & Services\" > \"Library.\"\nSearch for \"Cloud Filestore API.\"\nClick on the API and enable it.\nTo perform the deployment:\nterraform init\nterraform apply\nPlease note that during the terraform apply step, Terraform will prompt you to enter the PostgreSQL password. After that, it will ask for confirmation to proceed with the deployment. Review the changes, type 'yes' when prompted, and press Enter.",
        "question": "What must you do when prompted during the terraform apply step regarding the PostgreSQL password?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 172,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you want to rune multiple docker containers from different directories. Then make sure to change the port mappings in the docker-compose.yml file.\nports:\n- 8088:6789\nThe 8088 port in above case is hostport, where mage will run on your local machine. You can customize this as long as the port is available. If you are running on VM, make sure to forward the port too. You need to keep the container port to 6789 as this is the port where mage is running.\nGCP - 2.2.7d Deploying Mage to Google Cloud\nWhile terraforming all the resources inside a VM created in GCS the following error is shown.\nError log:\nmodule.lb-http.google_compute_backend_service.default[\"default\"]: Creating...\n╷\n│ Error: Error creating GlobalAddress: googleapi: Error 403: Request had insufficient authentication scopes.\n│ Details:\n│ [\n│   {\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n│     \"domain\": \"googleapis.com\",\n│     \"metadatas\": {\n│       \"method\": \"compute.beta.GlobalAddressesService.Insert\",\n│       \"service\": \"compute.googleapis.com\"\n│     },\n│     \"reason\": \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\n│   }\n│ ]\n│\n│ More details:\n│ Reason: insufficientPermissions, Message: Insufficient Permission\nThis error might happen when you are using a VM inside GCS. To use the Google APIs from a GCP virtual machine you need to add the cloud platform scope (\"https://www.googleapis.com/auth/cloud-platform\") to your VM when it is created.\nSince ours is already created you can just stop it and change the permissions. You can do it in the console, just go to \"EDIT\", g99o all the way down until you find \"Cloud API access scopes\". There you can \"Allow full access to all Cloud APIs\". I did this and all went smoothly generating all the resources needed. Hope it helps if you encounter this same error.\nResources: https://stackoverflow.com/questions/35928534/403-request-had-insufficient-authentication-scopes-during-gcloud-container-clu",
        "question": "What should you change in the docker-compose.yml file to run multiple Docker containers from different directories?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 173,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you want to rune multiple docker containers from different directories. Then make sure to change the port mappings in the docker-compose.yml file.\nports:\n- 8088:6789\nThe 8088 port in above case is hostport, where mage will run on your local machine. You can customize this as long as the port is available. If you are running on VM, make sure to forward the port too. You need to keep the container port to 6789 as this is the port where mage is running.\nGCP - 2.2.7d Deploying Mage to Google Cloud\nWhile terraforming all the resources inside a VM created in GCS the following error is shown.\nError log:\nmodule.lb-http.google_compute_backend_service.default[\"default\"]: Creating...\n╷\n│ Error: Error creating GlobalAddress: googleapi: Error 403: Request had insufficient authentication scopes.\n│ Details:\n│ [\n│   {\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n│     \"domain\": \"googleapis.com\",\n│     \"metadatas\": {\n│       \"method\": \"compute.beta.GlobalAddressesService.Insert\",\n│       \"service\": \"compute.googleapis.com\"\n│     },\n│     \"reason\": \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\n│   }\n│ ]\n│\n│ More details:\n│ Reason: insufficientPermissions, Message: Insufficient Permission\nThis error might happen when you are using a VM inside GCS. To use the Google APIs from a GCP virtual machine you need to add the cloud platform scope (\"https://www.googleapis.com/auth/cloud-platform\") to your VM when it is created.\nSince ours is already created you can just stop it and change the permissions. You can do it in the console, just go to \"EDIT\", g99o all the way down until you find \"Cloud API access scopes\". There you can \"Allow full access to all Cloud APIs\". I did this and all went smoothly generating all the resources needed. Hope it helps if you encounter this same error.\nResources: https://stackoverflow.com/questions/35928534/403-request-had-insufficient-authentication-scopes-during-gcloud-container-clu",
        "question": "What does the port mapping '8088:6789' signify in the Docker configuration?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 173,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you want to rune multiple docker containers from different directories. Then make sure to change the port mappings in the docker-compose.yml file.\nports:\n- 8088:6789\nThe 8088 port in above case is hostport, where mage will run on your local machine. You can customize this as long as the port is available. If you are running on VM, make sure to forward the port too. You need to keep the container port to 6789 as this is the port where mage is running.\nGCP - 2.2.7d Deploying Mage to Google Cloud\nWhile terraforming all the resources inside a VM created in GCS the following error is shown.\nError log:\nmodule.lb-http.google_compute_backend_service.default[\"default\"]: Creating...\n╷\n│ Error: Error creating GlobalAddress: googleapi: Error 403: Request had insufficient authentication scopes.\n│ Details:\n│ [\n│   {\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n│     \"domain\": \"googleapis.com\",\n│     \"metadatas\": {\n│       \"method\": \"compute.beta.GlobalAddressesService.Insert\",\n│       \"service\": \"compute.googleapis.com\"\n│     },\n│     \"reason\": \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\n│   }\n│ ]\n│\n│ More details:\n│ Reason: insufficientPermissions, Message: Insufficient Permission\nThis error might happen when you are using a VM inside GCS. To use the Google APIs from a GCP virtual machine you need to add the cloud platform scope (\"https://www.googleapis.com/auth/cloud-platform\") to your VM when it is created.\nSince ours is already created you can just stop it and change the permissions. You can do it in the console, just go to \"EDIT\", g99o all the way down until you find \"Cloud API access scopes\". There you can \"Allow full access to all Cloud APIs\". I did this and all went smoothly generating all the resources needed. Hope it helps if you encounter this same error.\nResources: https://stackoverflow.com/questions/35928534/403-request-had-insufficient-authentication-scopes-during-gcloud-container-clu",
        "question": "What error might occur when deploying Mage in Google Cloud and what does it indicate?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 173,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you want to rune multiple docker containers from different directories. Then make sure to change the port mappings in the docker-compose.yml file.\nports:\n- 8088:6789\nThe 8088 port in above case is hostport, where mage will run on your local machine. You can customize this as long as the port is available. If you are running on VM, make sure to forward the port too. You need to keep the container port to 6789 as this is the port where mage is running.\nGCP - 2.2.7d Deploying Mage to Google Cloud\nWhile terraforming all the resources inside a VM created in GCS the following error is shown.\nError log:\nmodule.lb-http.google_compute_backend_service.default[\"default\"]: Creating...\n╷\n│ Error: Error creating GlobalAddress: googleapi: Error 403: Request had insufficient authentication scopes.\n│ Details:\n│ [\n│   {\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n│     \"domain\": \"googleapis.com\",\n│     \"metadatas\": {\n│       \"method\": \"compute.beta.GlobalAddressesService.Insert\",\n│       \"service\": \"compute.googleapis.com\"\n│     },\n│     \"reason\": \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\n│   }\n│ ]\n│\n│ More details:\n│ Reason: insufficientPermissions, Message: Insufficient Permission\nThis error might happen when you are using a VM inside GCS. To use the Google APIs from a GCP virtual machine you need to add the cloud platform scope (\"https://www.googleapis.com/auth/cloud-platform\") to your VM when it is created.\nSince ours is already created you can just stop it and change the permissions. You can do it in the console, just go to \"EDIT\", g99o all the way down until you find \"Cloud API access scopes\". There you can \"Allow full access to all Cloud APIs\". I did this and all went smoothly generating all the resources needed. Hope it helps if you encounter this same error.\nResources: https://stackoverflow.com/questions/35928534/403-request-had-insufficient-authentication-scopes-during-gcloud-container-clu",
        "question": "How can you resolve the 'insufficient authentication scopes' error when using a GCP virtual machine?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 173,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you want to rune multiple docker containers from different directories. Then make sure to change the port mappings in the docker-compose.yml file.\nports:\n- 8088:6789\nThe 8088 port in above case is hostport, where mage will run on your local machine. You can customize this as long as the port is available. If you are running on VM, make sure to forward the port too. You need to keep the container port to 6789 as this is the port where mage is running.\nGCP - 2.2.7d Deploying Mage to Google Cloud\nWhile terraforming all the resources inside a VM created in GCS the following error is shown.\nError log:\nmodule.lb-http.google_compute_backend_service.default[\"default\"]: Creating...\n╷\n│ Error: Error creating GlobalAddress: googleapi: Error 403: Request had insufficient authentication scopes.\n│ Details:\n│ [\n│   {\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n│     \"domain\": \"googleapis.com\",\n│     \"metadatas\": {\n│       \"method\": \"compute.beta.GlobalAddressesService.Insert\",\n│       \"service\": \"compute.googleapis.com\"\n│     },\n│     \"reason\": \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\n│   }\n│ ]\n│\n│ More details:\n│ Reason: insufficientPermissions, Message: Insufficient Permission\nThis error might happen when you are using a VM inside GCS. To use the Google APIs from a GCP virtual machine you need to add the cloud platform scope (\"https://www.googleapis.com/auth/cloud-platform\") to your VM when it is created.\nSince ours is already created you can just stop it and change the permissions. You can do it in the console, just go to \"EDIT\", g99o all the way down until you find \"Cloud API access scopes\". There you can \"Allow full access to all Cloud APIs\". I did this and all went smoothly generating all the resources needed. Hope it helps if you encounter this same error.\nResources: https://stackoverflow.com/questions/35928534/403-request-had-insufficient-authentication-scopes-during-gcloud-container-clu",
        "question": "What steps are necessary to change the Cloud API access scopes for a GCP VM that has already been created?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 173,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are on the free trial account on GCP you will face this issue when trying to deploy the infrastructures with terraform. This service is not available for this kind of account.\nThe solution I found was to delete the load_balancer.tf file and to comment or delete the rows that differentiate it on the main.tf file. After this just do terraform destroy to delete any infrastructure created on the fail attempts and re-run the terraform apply.\nCode on main.tf to comment/delete:\nLine 166, 167, 168",
        "question": "What issue do users face when deploying infrastructures with Terraform on a free trial account on GCP?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 174,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are on the free trial account on GCP you will face this issue when trying to deploy the infrastructures with terraform. This service is not available for this kind of account.\nThe solution I found was to delete the load_balancer.tf file and to comment or delete the rows that differentiate it on the main.tf file. After this just do terraform destroy to delete any infrastructure created on the fail attempts and re-run the terraform apply.\nCode on main.tf to comment/delete:\nLine 166, 167, 168",
        "question": "What is the solution suggested for the issue encountered on the GCP free trial account?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 174,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are on the free trial account on GCP you will face this issue when trying to deploy the infrastructures with terraform. This service is not available for this kind of account.\nThe solution I found was to delete the load_balancer.tf file and to comment or delete the rows that differentiate it on the main.tf file. After this just do terraform destroy to delete any infrastructure created on the fail attempts and re-run the terraform apply.\nCode on main.tf to comment/delete:\nLine 166, 167, 168",
        "question": "Which file needs to be deleted to resolve the issue when using Terraform on GCP?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 174,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are on the free trial account on GCP you will face this issue when trying to deploy the infrastructures with terraform. This service is not available for this kind of account.\nThe solution I found was to delete the load_balancer.tf file and to comment or delete the rows that differentiate it on the main.tf file. After this just do terraform destroy to delete any infrastructure created on the fail attempts and re-run the terraform apply.\nCode on main.tf to comment/delete:\nLine 166, 167, 168",
        "question": "What specific lines in the main.tf file need to be commented or deleted?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 174,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are on the free trial account on GCP you will face this issue when trying to deploy the infrastructures with terraform. This service is not available for this kind of account.\nThe solution I found was to delete the load_balancer.tf file and to comment or delete the rows that differentiate it on the main.tf file. After this just do terraform destroy to delete any infrastructure created on the fail attempts and re-run the terraform apply.\nCode on main.tf to comment/delete:\nLine 166, 167, 168",
        "question": "What command should be run after deleting the load_balancer.tf file and modifying main.tf?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 174,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you get the following error\nYou have to edit variables.tf on the gcp folder, set your project-id and region and zones properly. Then, run terraform apply again.\nYou can find correct regions/zones here: https://cloud.google.com/compute/docs/regions-zones\nDeploying MAGE to GCP  with Terraform via the VM (2.2.7)\nFYI - It can take up to 20 minutes to deploy the MAGE Terraform files if you are using a GCP Virtual Machine. It is normal, so don’t interrupt the process or think it’s taking too long. If you have, make sure you run a terraform destroy before trying again as you will have likely partially created resources which will cause errors next time you run `terraform apply`.\n`terraform destroy` may not completely delete partial resources - go to Google Cloud Console and use the search bar at the top to search for the ‘app.name’ you declared in your variables.tf file; this will list all resources with that name - make sure you delete them all before running `terraform apply` again.\nWhy are my GCP free credits going so fast? MAGE .tf files - Terraform Destroy not destroying all Resources\nI checked my GCP billing last night & the MAGE Terraform IaC didn't destroy a GCP Resource called Filestore as ‘mage-data-prep- it has been costing £5.01 of my free credits each day  I now have £151 left - Alexey has assured me that This amount WILL BE SUFFICIENT funds to finish the course. Note to anyone who had issues deploying the MAGE terraform code: check your billing account to see what you're being charged for (main menu - billing) (even if it's your free credits) and run a search for 'mage-data-prep' in the top bar just to be sure that your resources have been destroyed - if any come up delete them.",
        "question": "What steps should you take if you encounter an error while deploying MAGE to GCP?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 175,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you get the following error\nYou have to edit variables.tf on the gcp folder, set your project-id and region and zones properly. Then, run terraform apply again.\nYou can find correct regions/zones here: https://cloud.google.com/compute/docs/regions-zones\nDeploying MAGE to GCP  with Terraform via the VM (2.2.7)\nFYI - It can take up to 20 minutes to deploy the MAGE Terraform files if you are using a GCP Virtual Machine. It is normal, so don’t interrupt the process or think it’s taking too long. If you have, make sure you run a terraform destroy before trying again as you will have likely partially created resources which will cause errors next time you run `terraform apply`.\n`terraform destroy` may not completely delete partial resources - go to Google Cloud Console and use the search bar at the top to search for the ‘app.name’ you declared in your variables.tf file; this will list all resources with that name - make sure you delete them all before running `terraform apply` again.\nWhy are my GCP free credits going so fast? MAGE .tf files - Terraform Destroy not destroying all Resources\nI checked my GCP billing last night & the MAGE Terraform IaC didn't destroy a GCP Resource called Filestore as ‘mage-data-prep- it has been costing £5.01 of my free credits each day  I now have £151 left - Alexey has assured me that This amount WILL BE SUFFICIENT funds to finish the course. Note to anyone who had issues deploying the MAGE terraform code: check your billing account to see what you're being charged for (main menu - billing) (even if it's your free credits) and run a search for 'mage-data-prep' in the top bar just to be sure that your resources have been destroyed - if any come up delete them.",
        "question": "How long can the deployment of MAGE Terraform files take on a GCP Virtual Machine?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 175,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you get the following error\nYou have to edit variables.tf on the gcp folder, set your project-id and region and zones properly. Then, run terraform apply again.\nYou can find correct regions/zones here: https://cloud.google.com/compute/docs/regions-zones\nDeploying MAGE to GCP  with Terraform via the VM (2.2.7)\nFYI - It can take up to 20 minutes to deploy the MAGE Terraform files if you are using a GCP Virtual Machine. It is normal, so don’t interrupt the process or think it’s taking too long. If you have, make sure you run a terraform destroy before trying again as you will have likely partially created resources which will cause errors next time you run `terraform apply`.\n`terraform destroy` may not completely delete partial resources - go to Google Cloud Console and use the search bar at the top to search for the ‘app.name’ you declared in your variables.tf file; this will list all resources with that name - make sure you delete them all before running `terraform apply` again.\nWhy are my GCP free credits going so fast? MAGE .tf files - Terraform Destroy not destroying all Resources\nI checked my GCP billing last night & the MAGE Terraform IaC didn't destroy a GCP Resource called Filestore as ‘mage-data-prep- it has been costing £5.01 of my free credits each day  I now have £151 left - Alexey has assured me that This amount WILL BE SUFFICIENT funds to finish the course. Note to anyone who had issues deploying the MAGE terraform code: check your billing account to see what you're being charged for (main menu - billing) (even if it's your free credits) and run a search for 'mage-data-prep' in the top bar just to be sure that your resources have been destroyed - if any come up delete them.",
        "question": "What should you do before trying to run 'terraform apply' again after an interruption?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 175,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you get the following error\nYou have to edit variables.tf on the gcp folder, set your project-id and region and zones properly. Then, run terraform apply again.\nYou can find correct regions/zones here: https://cloud.google.com/compute/docs/regions-zones\nDeploying MAGE to GCP  with Terraform via the VM (2.2.7)\nFYI - It can take up to 20 minutes to deploy the MAGE Terraform files if you are using a GCP Virtual Machine. It is normal, so don’t interrupt the process or think it’s taking too long. If you have, make sure you run a terraform destroy before trying again as you will have likely partially created resources which will cause errors next time you run `terraform apply`.\n`terraform destroy` may not completely delete partial resources - go to Google Cloud Console and use the search bar at the top to search for the ‘app.name’ you declared in your variables.tf file; this will list all resources with that name - make sure you delete them all before running `terraform apply` again.\nWhy are my GCP free credits going so fast? MAGE .tf files - Terraform Destroy not destroying all Resources\nI checked my GCP billing last night & the MAGE Terraform IaC didn't destroy a GCP Resource called Filestore as ‘mage-data-prep- it has been costing £5.01 of my free credits each day  I now have £151 left - Alexey has assured me that This amount WILL BE SUFFICIENT funds to finish the course. Note to anyone who had issues deploying the MAGE terraform code: check your billing account to see what you're being charged for (main menu - billing) (even if it's your free credits) and run a search for 'mage-data-prep' in the top bar just to be sure that your resources have been destroyed - if any come up delete them.",
        "question": "How can you ensure that all resources declared in your variables.tf file have been deleted?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 175,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you get the following error\nYou have to edit variables.tf on the gcp folder, set your project-id and region and zones properly. Then, run terraform apply again.\nYou can find correct regions/zones here: https://cloud.google.com/compute/docs/regions-zones\nDeploying MAGE to GCP  with Terraform via the VM (2.2.7)\nFYI - It can take up to 20 minutes to deploy the MAGE Terraform files if you are using a GCP Virtual Machine. It is normal, so don’t interrupt the process or think it’s taking too long. If you have, make sure you run a terraform destroy before trying again as you will have likely partially created resources which will cause errors next time you run `terraform apply`.\n`terraform destroy` may not completely delete partial resources - go to Google Cloud Console and use the search bar at the top to search for the ‘app.name’ you declared in your variables.tf file; this will list all resources with that name - make sure you delete them all before running `terraform apply` again.\nWhy are my GCP free credits going so fast? MAGE .tf files - Terraform Destroy not destroying all Resources\nI checked my GCP billing last night & the MAGE Terraform IaC didn't destroy a GCP Resource called Filestore as ‘mage-data-prep- it has been costing £5.01 of my free credits each day  I now have £151 left - Alexey has assured me that This amount WILL BE SUFFICIENT funds to finish the course. Note to anyone who had issues deploying the MAGE terraform code: check your billing account to see what you're being charged for (main menu - billing) (even if it's your free credits) and run a search for 'mage-data-prep' in the top bar just to be sure that your resources have been destroyed - if any come up delete them.",
        "question": "What could be the reason for rapid depletion of GCP free credits while using MAGE Terraform files?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 175,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "```\n│ Error: Error creating Connector: googleapi: Error 403: Permission 'vpcaccess.connectors.create' denied on resource '//vpcaccess.googleapis.com/projects/<ommit>/locations/us-west1' (or it may not exist).\n│ Details:\n│ [\n│   {\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n│     \"domain\": \"vpcaccess.googleapis.com\",\n│     \"metadata\": {\n│       \"permission\": \"vpcaccess.connectors.create\",\n│       \"resource\": \"projects/<ommit>/locations/us-west1\"\n│     },\n│     \"reason\": \"IAM_PERMISSION_DENIED\"\n│   }\n│ ]\n│\n│   with google_vpc_access_connector.connector,\n│   on fs.tf line 19, in resource \"google_vpc_access_connector\" \"connector\":\n│   19: resource \"google_vpc_access_connector\" \"connector\" {\n│\n```\nSolution: Add Serverless VPC Access Admin to Service Account.\nLine 148",
        "question": "What was the error message encountered when trying to create the Connector?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 176,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "```\n│ Error: Error creating Connector: googleapi: Error 403: Permission 'vpcaccess.connectors.create' denied on resource '//vpcaccess.googleapis.com/projects/<ommit>/locations/us-west1' (or it may not exist).\n│ Details:\n│ [\n│   {\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n│     \"domain\": \"vpcaccess.googleapis.com\",\n│     \"metadata\": {\n│       \"permission\": \"vpcaccess.connectors.create\",\n│       \"resource\": \"projects/<ommit>/locations/us-west1\"\n│     },\n│     \"reason\": \"IAM_PERMISSION_DENIED\"\n│   }\n│ ]\n│\n│   with google_vpc_access_connector.connector,\n│   on fs.tf line 19, in resource \"google_vpc_access_connector\" \"connector\":\n│   19: resource \"google_vpc_access_connector\" \"connector\" {\n│\n```\nSolution: Add Serverless VPC Access Admin to Service Account.\nLine 148",
        "question": "What permission is denied as indicated in the error details?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 176,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "```\n│ Error: Error creating Connector: googleapi: Error 403: Permission 'vpcaccess.connectors.create' denied on resource '//vpcaccess.googleapis.com/projects/<ommit>/locations/us-west1' (or it may not exist).\n│ Details:\n│ [\n│   {\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n│     \"domain\": \"vpcaccess.googleapis.com\",\n│     \"metadata\": {\n│       \"permission\": \"vpcaccess.connectors.create\",\n│       \"resource\": \"projects/<ommit>/locations/us-west1\"\n│     },\n│     \"reason\": \"IAM_PERMISSION_DENIED\"\n│   }\n│ ]\n│\n│   with google_vpc_access_connector.connector,\n│   on fs.tf line 19, in resource \"google_vpc_access_connector\" \"connector\":\n│   19: resource \"google_vpc_access_connector\" \"connector\" {\n│\n```\nSolution: Add Serverless VPC Access Admin to Service Account.\nLine 148",
        "question": "Which resource is associated with the error in the provided text?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 176,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "```\n│ Error: Error creating Connector: googleapi: Error 403: Permission 'vpcaccess.connectors.create' denied on resource '//vpcaccess.googleapis.com/projects/<ommit>/locations/us-west1' (or it may not exist).\n│ Details:\n│ [\n│   {\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n│     \"domain\": \"vpcaccess.googleapis.com\",\n│     \"metadata\": {\n│       \"permission\": \"vpcaccess.connectors.create\",\n│       \"resource\": \"projects/<ommit>/locations/us-west1\"\n│     },\n│     \"reason\": \"IAM_PERMISSION_DENIED\"\n│   }\n│ ]\n│\n│   with google_vpc_access_connector.connector,\n│   on fs.tf line 19, in resource \"google_vpc_access_connector\" \"connector\":\n│   19: resource \"google_vpc_access_connector\" \"connector\" {\n│\n```\nSolution: Add Serverless VPC Access Admin to Service Account.\nLine 148",
        "question": "What is the recommended solution to resolve the permission issue?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 176,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "```\n│ Error: Error creating Connector: googleapi: Error 403: Permission 'vpcaccess.connectors.create' denied on resource '//vpcaccess.googleapis.com/projects/<ommit>/locations/us-west1' (or it may not exist).\n│ Details:\n│ [\n│   {\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n│     \"domain\": \"vpcaccess.googleapis.com\",\n│     \"metadata\": {\n│       \"permission\": \"vpcaccess.connectors.create\",\n│       \"resource\": \"projects/<ommit>/locations/us-west1\"\n│     },\n│     \"reason\": \"IAM_PERMISSION_DENIED\"\n│   }\n│ ]\n│\n│   with google_vpc_access_connector.connector,\n│   on fs.tf line 19, in resource \"google_vpc_access_connector\" \"connector\":\n│   19: resource \"google_vpc_access_connector\" \"connector\" {\n│\n```\nSolution: Add Serverless VPC Access Admin to Service Account.\nLine 148",
        "question": "In which file and at what line number does the resource creation error occur?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 176,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Git won’t push an empty folder to GitHub, so if you put a file in that folder and then push, then you should be good to go.\nOr - in your code- make the folder if it doesn’t exist using Pathlib as shown here: https://stackoverflow.com/a/273227/4590385.\nFor some reason, when using github storage, the relative path for writing locally no longer works. Try using two separate paths, one full path for the local write, and the original relative path for GCS bucket upload.",
        "question": "Why won't Git push an empty folder to GitHub?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 177,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Git won’t push an empty folder to GitHub, so if you put a file in that folder and then push, then you should be good to go.\nOr - in your code- make the folder if it doesn’t exist using Pathlib as shown here: https://stackoverflow.com/a/273227/4590385.\nFor some reason, when using github storage, the relative path for writing locally no longer works. Try using two separate paths, one full path for the local write, and the original relative path for GCS bucket upload.",
        "question": "What should you do to ensure a folder is pushed to GitHub?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 177,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Git won’t push an empty folder to GitHub, so if you put a file in that folder and then push, then you should be good to go.\nOr - in your code- make the folder if it doesn’t exist using Pathlib as shown here: https://stackoverflow.com/a/273227/4590385.\nFor some reason, when using github storage, the relative path for writing locally no longer works. Try using two separate paths, one full path for the local write, and the original relative path for GCS bucket upload.",
        "question": "How can you create a folder if it doesn’t exist in your code?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 177,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Git won’t push an empty folder to GitHub, so if you put a file in that folder and then push, then you should be good to go.\nOr - in your code- make the folder if it doesn’t exist using Pathlib as shown here: https://stackoverflow.com/a/273227/4590385.\nFor some reason, when using github storage, the relative path for writing locally no longer works. Try using two separate paths, one full path for the local write, and the original relative path for GCS bucket upload.",
        "question": "What issue arises when using GitHub storage with relative paths?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 177,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Git won’t push an empty folder to GitHub, so if you put a file in that folder and then push, then you should be good to go.\nOr - in your code- make the folder if it doesn’t exist using Pathlib as shown here: https://stackoverflow.com/a/273227/4590385.\nFor some reason, when using github storage, the relative path for writing locally no longer works. Try using two separate paths, one full path for the local write, and the original relative path for GCS bucket upload.",
        "question": "What is the recommended method for handling paths when uploading to a GCS bucket?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 177,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The green dataset contains lpep_pickup_datetime while the yellow contains tpep_pickup_datetime. Modify the script(s) depending on  the dataset as required.",
        "question": "What is the difference between lpep_pickup_datetime and tpep_pickup_datetime?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 178,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The green dataset contains lpep_pickup_datetime while the yellow contains tpep_pickup_datetime. Modify the script(s) depending on  the dataset as required.",
        "question": "Which dataset contains lpep_pickup_datetime?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 178,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The green dataset contains lpep_pickup_datetime while the yellow contains tpep_pickup_datetime. Modify the script(s) depending on  the dataset as required.",
        "question": "What should be modified in the script(s) depending on the dataset?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 178,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The green dataset contains lpep_pickup_datetime while the yellow contains tpep_pickup_datetime. Modify the script(s) depending on  the dataset as required.",
        "question": "What color is the dataset that contains tpep_pickup_datetime?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 178,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The green dataset contains lpep_pickup_datetime while the yellow contains tpep_pickup_datetime. Modify the script(s) depending on  the dataset as required.",
        "question": "Why is it necessary to modify the script(s) based on the dataset?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 178,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "pd.read_csv\ndf_iter = pd.read_csv(dataset_url, iterator=True, chunksize=100000)\nThe data needs to be appended to the parquet file using the fastparquet engine\ndf.to_parquet(path, compression=\"gzip\", engine='fastparquet', append=True)",
        "question": "What function is used to read a CSV file in the provided text?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 179,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "pd.read_csv\ndf_iter = pd.read_csv(dataset_url, iterator=True, chunksize=100000)\nThe data needs to be appended to the parquet file using the fastparquet engine\ndf.to_parquet(path, compression=\"gzip\", engine='fastparquet', append=True)",
        "question": "How is the data read from the CSV file in chunks?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 179,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "pd.read_csv\ndf_iter = pd.read_csv(dataset_url, iterator=True, chunksize=100000)\nThe data needs to be appended to the parquet file using the fastparquet engine\ndf.to_parquet(path, compression=\"gzip\", engine='fastparquet', append=True)",
        "question": "What parameters are used in the pd.read_csv function in the text?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 179,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "pd.read_csv\ndf_iter = pd.read_csv(dataset_url, iterator=True, chunksize=100000)\nThe data needs to be appended to the parquet file using the fastparquet engine\ndf.to_parquet(path, compression=\"gzip\", engine='fastparquet', append=True)",
        "question": "Which engine is specified for appending the data to the parquet file?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 179,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "pd.read_csv\ndf_iter = pd.read_csv(dataset_url, iterator=True, chunksize=100000)\nThe data needs to be appended to the parquet file using the fastparquet engine\ndf.to_parquet(path, compression=\"gzip\", engine='fastparquet', append=True)",
        "question": "What compression method is used when converting the DataFrame to a parquet file?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 179,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "denied: requested access to the resource is denied\nThis can happen when you\nHaven't logged in properly to Docker Desktop (use docker login -u \"myusername\")\nHave used the wrong username when pushing to docker images. Use the same one as your username and as the one you build on\ndocker image build -t <myusername>/<imagename>:<tag>\ndocker image push <myusername>/<imagename>:<tag>",
        "question": "What does it mean when access to a resource is denied?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 180,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "denied: requested access to the resource is denied\nThis can happen when you\nHaven't logged in properly to Docker Desktop (use docker login -u \"myusername\")\nHave used the wrong username when pushing to docker images. Use the same one as your username and as the one you build on\ndocker image build -t <myusername>/<imagename>:<tag>\ndocker image push <myusername>/<imagename>:<tag>",
        "question": "What command should you use to log in properly to Docker Desktop?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 180,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "denied: requested access to the resource is denied\nThis can happen when you\nHaven't logged in properly to Docker Desktop (use docker login -u \"myusername\")\nHave used the wrong username when pushing to docker images. Use the same one as your username and as the one you build on\ndocker image build -t <myusername>/<imagename>:<tag>\ndocker image push <myusername>/<imagename>:<tag>",
        "question": "What should you ensure when pushing to Docker images?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 180,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "denied: requested access to the resource is denied\nThis can happen when you\nHaven't logged in properly to Docker Desktop (use docker login -u \"myusername\")\nHave used the wrong username when pushing to docker images. Use the same one as your username and as the one you build on\ndocker image build -t <myusername>/<imagename>:<tag>\ndocker image push <myusername>/<imagename>:<tag>",
        "question": "How do you properly build a Docker image with your username?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 180,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "denied: requested access to the resource is denied\nThis can happen when you\nHaven't logged in properly to Docker Desktop (use docker login -u \"myusername\")\nHave used the wrong username when pushing to docker images. Use the same one as your username and as the one you build on\ndocker image build -t <myusername>/<imagename>:<tag>\ndocker image push <myusername>/<imagename>:<tag>",
        "question": "What is the correct syntax for pushing a Docker image to the repository?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 180,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "16:21:35.607 | INFO    | Flow run 'singing-malkoha' - Executing 'write_bq-b366772c-0' immediately...\nKilled\nSolution:  You probably are running out of memory on your VM and need to add more.  For example, if you have 8 gigs of RAM on your VM, you may want to expand that to 16 gigs.",
        "question": "What does the log entry 'Executing 'write_bq-b366772c-0' immediately...' indicate?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 181,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "16:21:35.607 | INFO    | Flow run 'singing-malkoha' - Executing 'write_bq-b366772c-0' immediately...\nKilled\nSolution:  You probably are running out of memory on your VM and need to add more.  For example, if you have 8 gigs of RAM on your VM, you may want to expand that to 16 gigs.",
        "question": "What might be the cause of the process being killed?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 181,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "16:21:35.607 | INFO    | Flow run 'singing-malkoha' - Executing 'write_bq-b366772c-0' immediately...\nKilled\nSolution:  You probably are running out of memory on your VM and need to add more.  For example, if you have 8 gigs of RAM on your VM, you may want to expand that to 16 gigs.",
        "question": "How much RAM is suggested for the VM to prevent the issue mentioned?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 181,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "16:21:35.607 | INFO    | Flow run 'singing-malkoha' - Executing 'write_bq-b366772c-0' immediately...\nKilled\nSolution:  You probably are running out of memory on your VM and need to add more.  For example, if you have 8 gigs of RAM on your VM, you may want to expand that to 16 gigs.",
        "question": "What is the original amount of RAM mentioned for the VM?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 181,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "16:21:35.607 | INFO    | Flow run 'singing-malkoha' - Executing 'write_bq-b366772c-0' immediately...\nKilled\nSolution:  You probably are running out of memory on your VM and need to add more.  For example, if you have 8 gigs of RAM on your VM, you may want to expand that to 16 gigs.",
        "question": "What is a potential solution to the problem described in the text?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 181,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After playing around with prefect for a while this can happen.\nSsh to your VM and run sudo du -h --block-size=G | sort -n -r | head -n 30 to see which directory needs the most space.\nMost likely it will be …/.prefect/storage, where your cached flows are stored. You can delete older flows from there. You also have to delete the corresponding flow in the UI, otherwise it will throw you an error, when you try to run your next flow.\nSSL Certificate Verify: (I got it when trying to run flows on MAC): urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]\npip install certifi\n/Applications/Python\\ {ver}/Install\\ Certificates.command\nor\nrunning the “Install Certificate.command” inside of the python{ver} folder",
        "question": "What command can be used to check which directory needs the most space after using prefect?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 182,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After playing around with prefect for a while this can happen.\nSsh to your VM and run sudo du -h --block-size=G | sort -n -r | head -n 30 to see which directory needs the most space.\nMost likely it will be …/.prefect/storage, where your cached flows are stored. You can delete older flows from there. You also have to delete the corresponding flow in the UI, otherwise it will throw you an error, when you try to run your next flow.\nSSL Certificate Verify: (I got it when trying to run flows on MAC): urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]\npip install certifi\n/Applications/Python\\ {ver}/Install\\ Certificates.command\nor\nrunning the “Install Certificate.command” inside of the python{ver} folder",
        "question": "Where are cached flows typically stored in relation to prefect?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 182,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After playing around with prefect for a while this can happen.\nSsh to your VM and run sudo du -h --block-size=G | sort -n -r | head -n 30 to see which directory needs the most space.\nMost likely it will be …/.prefect/storage, where your cached flows are stored. You can delete older flows from there. You also have to delete the corresponding flow in the UI, otherwise it will throw you an error, when you try to run your next flow.\nSSL Certificate Verify: (I got it when trying to run flows on MAC): urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]\npip install certifi\n/Applications/Python\\ {ver}/Install\\ Certificates.command\nor\nrunning the “Install Certificate.command” inside of the python{ver} folder",
        "question": "What should you do after deleting older flows from the .prefect/storage directory?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 182,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After playing around with prefect for a while this can happen.\nSsh to your VM and run sudo du -h --block-size=G | sort -n -r | head -n 30 to see which directory needs the most space.\nMost likely it will be …/.prefect/storage, where your cached flows are stored. You can delete older flows from there. You also have to delete the corresponding flow in the UI, otherwise it will throw you an error, when you try to run your next flow.\nSSL Certificate Verify: (I got it when trying to run flows on MAC): urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]\npip install certifi\n/Applications/Python\\ {ver}/Install\\ Certificates.command\nor\nrunning the “Install Certificate.command” inside of the python{ver} folder",
        "question": "What error might occur if you do not delete the corresponding flow in the UI?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 182,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After playing around with prefect for a while this can happen.\nSsh to your VM and run sudo du -h --block-size=G | sort -n -r | head -n 30 to see which directory needs the most space.\nMost likely it will be …/.prefect/storage, where your cached flows are stored. You can delete older flows from there. You also have to delete the corresponding flow in the UI, otherwise it will throw you an error, when you try to run your next flow.\nSSL Certificate Verify: (I got it when trying to run flows on MAC): urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]\npip install certifi\n/Applications/Python\\ {ver}/Install\\ Certificates.command\nor\nrunning the “Install Certificate.command” inside of the python{ver} folder",
        "question": "What steps can be taken to resolve the SSL certificate verification error when running flows on a MAC?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 182,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It means your container consumed all available RAM allocated to it. It can happen in particular when working on Question#3 in the homework as the dataset is relatively large and containers eat a lot of memory in general.\nI would recommend restarting your computer and only starting the necessary processes to run the container. If that doesn’t work, allocate more resources to docker. If also that doesn’t work because your workstation is a potato, you can use an online compute environment service like GitPod, which is free under under 50 hours / month of use.",
        "question": "What does it mean when a container has consumed all available RAM?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 183,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It means your container consumed all available RAM allocated to it. It can happen in particular when working on Question#3 in the homework as the dataset is relatively large and containers eat a lot of memory in general.\nI would recommend restarting your computer and only starting the necessary processes to run the container. If that doesn’t work, allocate more resources to docker. If also that doesn’t work because your workstation is a potato, you can use an online compute environment service like GitPod, which is free under under 50 hours / month of use.",
        "question": "Why might a container use a lot of memory when working on Question#3 in the homework?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 183,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It means your container consumed all available RAM allocated to it. It can happen in particular when working on Question#3 in the homework as the dataset is relatively large and containers eat a lot of memory in general.\nI would recommend restarting your computer and only starting the necessary processes to run the container. If that doesn’t work, allocate more resources to docker. If also that doesn’t work because your workstation is a potato, you can use an online compute environment service like GitPod, which is free under under 50 hours / month of use.",
        "question": "What are the recommended steps if the container is using too much memory?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 183,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It means your container consumed all available RAM allocated to it. It can happen in particular when working on Question#3 in the homework as the dataset is relatively large and containers eat a lot of memory in general.\nI would recommend restarting your computer and only starting the necessary processes to run the container. If that doesn’t work, allocate more resources to docker. If also that doesn’t work because your workstation is a potato, you can use an online compute environment service like GitPod, which is free under under 50 hours / month of use.",
        "question": "What is an alternative to using a local workstation if it does not have enough resources?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 183,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It means your container consumed all available RAM allocated to it. It can happen in particular when working on Question#3 in the homework as the dataset is relatively large and containers eat a lot of memory in general.\nI would recommend restarting your computer and only starting the necessary processes to run the container. If that doesn’t work, allocate more resources to docker. If also that doesn’t work because your workstation is a potato, you can use an online compute environment service like GitPod, which is free under under 50 hours / month of use.",
        "question": "How many hours per month is GitPod free to use?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 183,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In Q3 there was a task to run the etl script from web to GCS. The problem was, it wasn’t really an ETL straight from web to GCS, but it was actually a web to local storage to local memory to GCS over network ETL. Yellow data is about 100 MB each per month compressed and ~700 MB after uncompressed on memory\nThis leads to a problem where i either got a network type error because my not so good 3rd world internet or i got my WSL2 crashed/hanged because out of memory error and/or 100% resource usage hang.\nSolution:\nif you have a lot of time at hand, try compressing it to parquet and writing it to GCS with the timeout argument set to a really high number (the default os 60 seconds)\nthe yellow taxi data for feb 2019 is about 100MB as parquet file\ngcp_cloud_storage_bucket_block.upload_from_path(\nfrom_path=f\"{path}\",\nto_path=path,\ntimeout=600\n)",
        "question": "What was the main task in Q3 regarding the ETL script?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 184,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In Q3 there was a task to run the etl script from web to GCS. The problem was, it wasn’t really an ETL straight from web to GCS, but it was actually a web to local storage to local memory to GCS over network ETL. Yellow data is about 100 MB each per month compressed and ~700 MB after uncompressed on memory\nThis leads to a problem where i either got a network type error because my not so good 3rd world internet or i got my WSL2 crashed/hanged because out of memory error and/or 100% resource usage hang.\nSolution:\nif you have a lot of time at hand, try compressing it to parquet and writing it to GCS with the timeout argument set to a really high number (the default os 60 seconds)\nthe yellow taxi data for feb 2019 is about 100MB as parquet file\ngcp_cloud_storage_bucket_block.upload_from_path(\nfrom_path=f\"{path}\",\nto_path=path,\ntimeout=600\n)",
        "question": "What problem did the author encounter when trying to run the ETL script?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 184,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In Q3 there was a task to run the etl script from web to GCS. The problem was, it wasn’t really an ETL straight from web to GCS, but it was actually a web to local storage to local memory to GCS over network ETL. Yellow data is about 100 MB each per month compressed and ~700 MB after uncompressed on memory\nThis leads to a problem where i either got a network type error because my not so good 3rd world internet or i got my WSL2 crashed/hanged because out of memory error and/or 100% resource usage hang.\nSolution:\nif you have a lot of time at hand, try compressing it to parquet and writing it to GCS with the timeout argument set to a really high number (the default os 60 seconds)\nthe yellow taxi data for feb 2019 is about 100MB as parquet file\ngcp_cloud_storage_bucket_block.upload_from_path(\nfrom_path=f\"{path}\",\nto_path=path,\ntimeout=600\n)",
        "question": "What is the size of the yellow taxi data when compressed and uncompressed?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 184,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In Q3 there was a task to run the etl script from web to GCS. The problem was, it wasn’t really an ETL straight from web to GCS, but it was actually a web to local storage to local memory to GCS over network ETL. Yellow data is about 100 MB each per month compressed and ~700 MB after uncompressed on memory\nThis leads to a problem where i either got a network type error because my not so good 3rd world internet or i got my WSL2 crashed/hanged because out of memory error and/or 100% resource usage hang.\nSolution:\nif you have a lot of time at hand, try compressing it to parquet and writing it to GCS with the timeout argument set to a really high number (the default os 60 seconds)\nthe yellow taxi data for feb 2019 is about 100MB as parquet file\ngcp_cloud_storage_bucket_block.upload_from_path(\nfrom_path=f\"{path}\",\nto_path=path,\ntimeout=600\n)",
        "question": "What solution is suggested for handling the data transfer to GCS?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 184,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In Q3 there was a task to run the etl script from web to GCS. The problem was, it wasn’t really an ETL straight from web to GCS, but it was actually a web to local storage to local memory to GCS over network ETL. Yellow data is about 100 MB each per month compressed and ~700 MB after uncompressed on memory\nThis leads to a problem where i either got a network type error because my not so good 3rd world internet or i got my WSL2 crashed/hanged because out of memory error and/or 100% resource usage hang.\nSolution:\nif you have a lot of time at hand, try compressing it to parquet and writing it to GCS with the timeout argument set to a really high number (the default os 60 seconds)\nthe yellow taxi data for feb 2019 is about 100MB as parquet file\ngcp_cloud_storage_bucket_block.upload_from_path(\nfrom_path=f\"{path}\",\nto_path=path,\ntimeout=600\n)",
        "question": "What is the recommended timeout setting when uploading the parquet file to GCS?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 184,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error occurs when you try to re-run the export block, of the transformed green_taxi data to PostgreSQL.\nWhat you’ll need to do is to drop the table using SQL in Mage (screenshot below).\nYou should be able to re-run the block successfully after dropping the table.",
        "question": "What error occurs when trying to re-run the export block of the transformed green_taxi data?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 185,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error occurs when you try to re-run the export block, of the transformed green_taxi data to PostgreSQL.\nWhat you’ll need to do is to drop the table using SQL in Mage (screenshot below).\nYou should be able to re-run the block successfully after dropping the table.",
        "question": "What step is necessary to resolve the error when re-running the export block?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 185,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error occurs when you try to re-run the export block, of the transformed green_taxi data to PostgreSQL.\nWhat you’ll need to do is to drop the table using SQL in Mage (screenshot below).\nYou should be able to re-run the block successfully after dropping the table.",
        "question": "Where do you need to drop the table to fix the error?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 185,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error occurs when you try to re-run the export block, of the transformed green_taxi data to PostgreSQL.\nWhat you’ll need to do is to drop the table using SQL in Mage (screenshot below).\nYou should be able to re-run the block successfully after dropping the table.",
        "question": "What database is mentioned in relation to the green_taxi data export?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 185,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error occurs when you try to re-run the export block, of the transformed green_taxi data to PostgreSQL.\nWhat you’ll need to do is to drop the table using SQL in Mage (screenshot below).\nYou should be able to re-run the block successfully after dropping the table.",
        "question": "What will happen after you drop the table and re-run the export block?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 185,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame.\nUse the data.loc[] = value syntax instead of df[] = value to ensure that the new column is being assigned to the original dataframe instead of a copy of a dataframe or a series.",
        "question": "What does SettingWithCopyWarning indicate when working with a DataFrame?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 186,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame.\nUse the data.loc[] = value syntax instead of df[] = value to ensure that the new column is being assigned to the original dataframe instead of a copy of a dataframe or a series.",
        "question": "What is the recommended syntax to assign a new value in a DataFrame?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 186,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame.\nUse the data.loc[] = value syntax instead of df[] = value to ensure that the new column is being assigned to the original dataframe instead of a copy of a dataframe or a series.",
        "question": "Why is it important to use data.loc[] = value instead of df[] = value?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 186,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame.\nUse the data.loc[] = value syntax instead of df[] = value to ensure that the new column is being assigned to the original dataframe instead of a copy of a dataframe or a series.",
        "question": "What could happen if you assign values to a copy of a DataFrame instead of the original?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 186,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame.\nUse the data.loc[] = value syntax instead of df[] = value to ensure that the new column is being assigned to the original dataframe instead of a copy of a dataframe or a series.",
        "question": "In what context does SettingWithCopyWarning typically occur?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 186,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "CSV Files are very big in nyc data, so we instead of using Pandas/Python kernel , we can try Pyspark Kernel\nDocumentation of Mage for using pyspark kernel: https://docs.mage.ai/integrations/spark-pyspark\n?",
        "question": "What are CSV files in the context of NYC data?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 187,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "CSV Files are very big in nyc data, so we instead of using Pandas/Python kernel , we can try Pyspark Kernel\nDocumentation of Mage for using pyspark kernel: https://docs.mage.ai/integrations/spark-pyspark\n?",
        "question": "Why is Pyspark Kernel suggested instead of Pandas/Python kernel for handling large CSV files?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 187,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "CSV Files are very big in nyc data, so we instead of using Pandas/Python kernel , we can try Pyspark Kernel\nDocumentation of Mage for using pyspark kernel: https://docs.mage.ai/integrations/spark-pyspark\n?",
        "question": "Where can one find the documentation for using Pyspark Kernel with Mage?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 187,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "CSV Files are very big in nyc data, so we instead of using Pandas/Python kernel , we can try Pyspark Kernel\nDocumentation of Mage for using pyspark kernel: https://docs.mage.ai/integrations/spark-pyspark\n?",
        "question": "What are the potential benefits of using Pyspark Kernel for large datasets?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 187,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "CSV Files are very big in nyc data, so we instead of using Pandas/Python kernel , we can try Pyspark Kernel\nDocumentation of Mage for using pyspark kernel: https://docs.mage.ai/integrations/spark-pyspark\n?",
        "question": "How does the size of CSV files impact the choice of data processing tools?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 187,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "So we will first delete the connection between blocks then we can remove the connection.",
        "question": "What is the first step mentioned in the process?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 188,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "So we will first delete the connection between blocks then we can remove the connection.",
        "question": "What are we removing connections from?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 188,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "So we will first delete the connection between blocks then we can remove the connection.",
        "question": "Why is it necessary to delete connections between blocks?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 188,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "So we will first delete the connection between blocks then we can remove the connection.",
        "question": "What is the sequence of actions suggested in the text?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 188,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "So we will first delete the connection between blocks then we can remove the connection.",
        "question": "What does 'remove the connection' refer to?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 188,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "While Editing the Pipeline Name It throws permission denied error.\n(Work around)In that case proceed with the work and save later on revisit it will let you edit.",
        "question": "What error occurs when trying to edit the pipeline name?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 189,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "While Editing the Pipeline Name It throws permission denied error.\n(Work around)In that case proceed with the work and save later on revisit it will let you edit.",
        "question": "What is the suggested workaround if a permission denied error is encountered?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 189,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "While Editing the Pipeline Name It throws permission denied error.\n(Work around)In that case proceed with the work and save later on revisit it will let you edit.",
        "question": "What should you do after saving your work due to the permission denied error?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 189,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "While Editing the Pipeline Name It throws permission denied error.\n(Work around)In that case proceed with the work and save later on revisit it will let you edit.",
        "question": "Is it possible to edit the pipeline name later after receiving an error?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 189,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "While Editing the Pipeline Name It throws permission denied error.\n(Work around)In that case proceed with the work and save later on revisit it will let you edit.",
        "question": "What specific steps should be taken if a user faces a permission denied error while editing?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 189,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "All mage files are in your /home/src/folder where you saved your credentials.json so you should be able to access them locally. You will see a folder for ‘Pipelines’,  'data loaders', 'data transformers' & 'data exporters' - inside these will be the .py or .sql files for the blocks you created in your pipeline.\nRight click & ‘download’ the pipeline itself to your local machine (which gives you metadata, pycache and other files)\nAs above, download each .py/.sql file that corresponds to each block you created for the pipeline. You'll find these under 'data loaders', 'data transformers' 'data exporters'\nMove the downloaded files to your GitHub repo folder & commit your changes.",
        "question": "Where are all the mage files located?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 191,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "All mage files are in your /home/src/folder where you saved your credentials.json so you should be able to access them locally. You will see a folder for ‘Pipelines’,  'data loaders', 'data transformers' & 'data exporters' - inside these will be the .py or .sql files for the blocks you created in your pipeline.\nRight click & ‘download’ the pipeline itself to your local machine (which gives you metadata, pycache and other files)\nAs above, download each .py/.sql file that corresponds to each block you created for the pipeline. You'll find these under 'data loaders', 'data transformers' 'data exporters'\nMove the downloaded files to your GitHub repo folder & commit your changes.",
        "question": "What should you do to download the pipeline metadata?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 191,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "All mage files are in your /home/src/folder where you saved your credentials.json so you should be able to access them locally. You will see a folder for ‘Pipelines’,  'data loaders', 'data transformers' & 'data exporters' - inside these will be the .py or .sql files for the blocks you created in your pipeline.\nRight click & ‘download’ the pipeline itself to your local machine (which gives you metadata, pycache and other files)\nAs above, download each .py/.sql file that corresponds to each block you created for the pipeline. You'll find these under 'data loaders', 'data transformers' 'data exporters'\nMove the downloaded files to your GitHub repo folder & commit your changes.",
        "question": "What types of files can be found in the folders for 'data loaders', 'data transformers', and 'data exporters'?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 191,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "All mage files are in your /home/src/folder where you saved your credentials.json so you should be able to access them locally. You will see a folder for ‘Pipelines’,  'data loaders', 'data transformers' & 'data exporters' - inside these will be the .py or .sql files for the blocks you created in your pipeline.\nRight click & ‘download’ the pipeline itself to your local machine (which gives you metadata, pycache and other files)\nAs above, download each .py/.sql file that corresponds to each block you created for the pipeline. You'll find these under 'data loaders', 'data transformers' 'data exporters'\nMove the downloaded files to your GitHub repo folder & commit your changes.",
        "question": "What is the next step after downloading the .py/.sql files for your pipeline blocks?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 191,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "All mage files are in your /home/src/folder where you saved your credentials.json so you should be able to access them locally. You will see a folder for ‘Pipelines’,  'data loaders', 'data transformers' & 'data exporters' - inside these will be the .py or .sql files for the blocks you created in your pipeline.\nRight click & ‘download’ the pipeline itself to your local machine (which gives you metadata, pycache and other files)\nAs above, download each .py/.sql file that corresponds to each block you created for the pipeline. You'll find these under 'data loaders', 'data transformers' 'data exporters'\nMove the downloaded files to your GitHub repo folder & commit your changes.",
        "question": "What folder should you move the downloaded files to?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 191,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\nMove the contents of the .gitignore file in your main .gitignore.\nUse the terminal to cd into the Mage folder and:\nrun “git remote remove origin” to de-couple the Mage repo,\nrun “rm -rf .git” to delete local git files,\nrun “git add .” to add the current folder as changes to stage, commit and push.",
        "question": "What steps are required to include Mage files in the personal copy of the Data Engineering Zoomcamp repo?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 192,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\nMove the contents of the .gitignore file in your main .gitignore.\nUse the terminal to cd into the Mage folder and:\nrun “git remote remove origin” to de-couple the Mage repo,\nrun “rm -rf .git” to delete local git files,\nrun “git add .” to add the current folder as changes to stage, commit and push.",
        "question": "Why might including Mage files in the DE Zoomcamp repo not work by default?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 192,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\nMove the contents of the .gitignore file in your main .gitignore.\nUse the terminal to cd into the Mage folder and:\nrun “git remote remove origin” to de-couple the Mage repo,\nrun “rm -rf .git” to delete local git files,\nrun “git add .” to add the current folder as changes to stage, commit and push.",
        "question": "What command is used to de-couple the Mage repository from its original remote?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 192,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\nMove the contents of the .gitignore file in your main .gitignore.\nUse the terminal to cd into the Mage folder and:\nrun “git remote remove origin” to de-couple the Mage repo,\nrun “rm -rf .git” to delete local git files,\nrun “git add .” to add the current folder as changes to stage, commit and push.",
        "question": "What does the command 'rm -rf .git' accomplish in this process?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 192,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\nMove the contents of the .gitignore file in your main .gitignore.\nUse the terminal to cd into the Mage folder and:\nrun “git remote remove origin” to de-couple the Mage repo,\nrun “rm -rf .git” to delete local git files,\nrun “git add .” to add the current folder as changes to stage, commit and push.",
        "question": "What are the final commands needed to stage, commit, and push the Mage files after removing the Git repository?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 192,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When try to add three assertions:\nvendor_id is one of the existing values in the column (currently)\npassenger_count is greater than 0\ntrip_distance is greater than 0\nto test_output, I got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). Below is my code:\ndata_filter = (data['passenger_count'] > 0) and (data['trip_distance'] > 0)\nAfter looking for solutions at Stackoverflow, I found great discussion about it. So I changed my code into:\ndata_filter = (data['passenger_count'] > 0) & (data['trip_distance'] > 0)",
        "question": "What are the three assertions being added to test_output?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 193,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When try to add three assertions:\nvendor_id is one of the existing values in the column (currently)\npassenger_count is greater than 0\ntrip_distance is greater than 0\nto test_output, I got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). Below is my code:\ndata_filter = (data['passenger_count'] > 0) and (data['trip_distance'] > 0)\nAfter looking for solutions at Stackoverflow, I found great discussion about it. So I changed my code into:\ndata_filter = (data['passenger_count'] > 0) & (data['trip_distance'] > 0)",
        "question": "What error did the author encounter while adding assertions?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 193,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When try to add three assertions:\nvendor_id is one of the existing values in the column (currently)\npassenger_count is greater than 0\ntrip_distance is greater than 0\nto test_output, I got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). Below is my code:\ndata_filter = (data['passenger_count'] > 0) and (data['trip_distance'] > 0)\nAfter looking for solutions at Stackoverflow, I found great discussion about it. So I changed my code into:\ndata_filter = (data['passenger_count'] > 0) & (data['trip_distance'] > 0)",
        "question": "What is the ambiguous issue with the truth value of a Series?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 193,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When try to add three assertions:\nvendor_id is one of the existing values in the column (currently)\npassenger_count is greater than 0\ntrip_distance is greater than 0\nto test_output, I got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). Below is my code:\ndata_filter = (data['passenger_count'] > 0) and (data['trip_distance'] > 0)\nAfter looking for solutions at Stackoverflow, I found great discussion about it. So I changed my code into:\ndata_filter = (data['passenger_count'] > 0) & (data['trip_distance'] > 0)",
        "question": "How did the author resolve the ValueError they encountered?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 193,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When try to add three assertions:\nvendor_id is one of the existing values in the column (currently)\npassenger_count is greater than 0\ntrip_distance is greater than 0\nto test_output, I got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). Below is my code:\ndata_filter = (data['passenger_count'] > 0) and (data['trip_distance'] > 0)\nAfter looking for solutions at Stackoverflow, I found great discussion about it. So I changed my code into:\ndata_filter = (data['passenger_count'] > 0) & (data['trip_distance'] > 0)",
        "question": "What logical operator is used to combine conditions in the corrected code?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 193,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happened when I just booted up my PC, continuing from the progress I was doing from yesterday.\nAfter cd-ing into your directory, and running docker compose up , the web interface for the Mage shows, but the files that I had yesterday was gone.\nIf your files are gone, go ahead and close the web interface, and properly shutting down the mage docker compose by doing Ctrl + C once. Try running it again. This worked for me more than once (yes the issue persisted with my PC twice)\nAlso, you should check if you’re in the correct repository before doing docker compose up . This was discussed in the Slack #course-data-engineering channel",
        "question": "What steps should be taken when files are missing after running docker compose up?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 194,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happened when I just booted up my PC, continuing from the progress I was doing from yesterday.\nAfter cd-ing into your directory, and running docker compose up , the web interface for the Mage shows, but the files that I had yesterday was gone.\nIf your files are gone, go ahead and close the web interface, and properly shutting down the mage docker compose by doing Ctrl + C once. Try running it again. This worked for me more than once (yes the issue persisted with my PC twice)\nAlso, you should check if you’re in the correct repository before doing docker compose up . This was discussed in the Slack #course-data-engineering channel",
        "question": "What command is used to properly shut down the mage docker compose?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 194,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happened when I just booted up my PC, continuing from the progress I was doing from yesterday.\nAfter cd-ing into your directory, and running docker compose up , the web interface for the Mage shows, but the files that I had yesterday was gone.\nIf your files are gone, go ahead and close the web interface, and properly shutting down the mage docker compose by doing Ctrl + C once. Try running it again. This worked for me more than once (yes the issue persisted with my PC twice)\nAlso, you should check if you’re in the correct repository before doing docker compose up . This was discussed in the Slack #course-data-engineering channel",
        "question": "What is the importance of checking the repository before running docker compose up?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 194,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happened when I just booted up my PC, continuing from the progress I was doing from yesterday.\nAfter cd-ing into your directory, and running docker compose up , the web interface for the Mage shows, but the files that I had yesterday was gone.\nIf your files are gone, go ahead and close the web interface, and properly shutting down the mage docker compose by doing Ctrl + C once. Try running it again. This worked for me more than once (yes the issue persisted with my PC twice)\nAlso, you should check if you’re in the correct repository before doing docker compose up . This was discussed in the Slack #course-data-engineering channel",
        "question": "Where was the issue of missing files discussed?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 194,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happened when I just booted up my PC, continuing from the progress I was doing from yesterday.\nAfter cd-ing into your directory, and running docker compose up , the web interface for the Mage shows, but the files that I had yesterday was gone.\nIf your files are gone, go ahead and close the web interface, and properly shutting down the mage docker compose by doing Ctrl + C once. Try running it again. This worked for me more than once (yes the issue persisted with my PC twice)\nAlso, you should check if you’re in the correct repository before doing docker compose up . This was discussed in the Slack #course-data-engineering channel",
        "question": "What should you do if the web interface for the Mage shows but your files are gone?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 194,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The above errors due to “ at the trailing side and it need to be modified with ‘ quotes at both ends\nKrishna Anand",
        "question": "What type of error is mentioned in the text?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 195,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The above errors due to “ at the trailing side and it need to be modified with ‘ quotes at both ends\nKrishna Anand",
        "question": "What modification is needed for the quotes?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 195,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The above errors due to “ at the trailing side and it need to be modified with ‘ quotes at both ends\nKrishna Anand",
        "question": "Who is the author of the text?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 195,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The above errors due to “ at the trailing side and it need to be modified with ‘ quotes at both ends\nKrishna Anand",
        "question": "What is meant by 'trailing side' in this context?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 195,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The above errors due to “ at the trailing side and it need to be modified with ‘ quotes at both ends\nKrishna Anand",
        "question": "Why is it important to use ' quotes at both ends?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 195,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: The following error occurs when attempting to export data from Mage to a GCS bucket using pyarrow suggesting Mage doesn’t have the necessary permissions to access the specified GCP credentials .json file.\nArrowException: Unknown error: google::cloud::Status(UNKNOWN: Permanent error GetBucketMetadata: Could not create a OAuth2 access token to authenticate the request. The request was not sent, as such an access token is required to complete the request successfully. Learn more about Google Cloud authentication at https://cloud.google.com/docs/authentication. The underlying error message was: Cannot open credentials file /home/src/...\nSolution: Inside the Mage app:\nCreate a credentials folder (e.g. gcp-creds) within the magic-zoomcamp folder\nIn the credentials folder create a .json key file (e.g. mage-gcp-creds.json)\nCopy/paste GCP service account credentials into the .json key file and save\nUpdate code to point to this file. E.g.\nenviron['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/magic-zoomcamp/gcp-creds/mage-gcp-creds.json'",
        "question": "What error occurs when exporting data from Mage to a GCS bucket using pyarrow?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 196,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: The following error occurs when attempting to export data from Mage to a GCS bucket using pyarrow suggesting Mage doesn’t have the necessary permissions to access the specified GCP credentials .json file.\nArrowException: Unknown error: google::cloud::Status(UNKNOWN: Permanent error GetBucketMetadata: Could not create a OAuth2 access token to authenticate the request. The request was not sent, as such an access token is required to complete the request successfully. Learn more about Google Cloud authentication at https://cloud.google.com/docs/authentication. The underlying error message was: Cannot open credentials file /home/src/...\nSolution: Inside the Mage app:\nCreate a credentials folder (e.g. gcp-creds) within the magic-zoomcamp folder\nIn the credentials folder create a .json key file (e.g. mage-gcp-creds.json)\nCopy/paste GCP service account credentials into the .json key file and save\nUpdate code to point to this file. E.g.\nenviron['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/magic-zoomcamp/gcp-creds/mage-gcp-creds.json'",
        "question": "What indicates that Mage lacks the necessary permissions to access the GCP credentials file?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 196,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: The following error occurs when attempting to export data from Mage to a GCS bucket using pyarrow suggesting Mage doesn’t have the necessary permissions to access the specified GCP credentials .json file.\nArrowException: Unknown error: google::cloud::Status(UNKNOWN: Permanent error GetBucketMetadata: Could not create a OAuth2 access token to authenticate the request. The request was not sent, as such an access token is required to complete the request successfully. Learn more about Google Cloud authentication at https://cloud.google.com/docs/authentication. The underlying error message was: Cannot open credentials file /home/src/...\nSolution: Inside the Mage app:\nCreate a credentials folder (e.g. gcp-creds) within the magic-zoomcamp folder\nIn the credentials folder create a .json key file (e.g. mage-gcp-creds.json)\nCopy/paste GCP service account credentials into the .json key file and save\nUpdate code to point to this file. E.g.\nenviron['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/magic-zoomcamp/gcp-creds/mage-gcp-creds.json'",
        "question": "What steps are outlined in the solution to resolve the permission issue in Mage?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 196,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: The following error occurs when attempting to export data from Mage to a GCS bucket using pyarrow suggesting Mage doesn’t have the necessary permissions to access the specified GCP credentials .json file.\nArrowException: Unknown error: google::cloud::Status(UNKNOWN: Permanent error GetBucketMetadata: Could not create a OAuth2 access token to authenticate the request. The request was not sent, as such an access token is required to complete the request successfully. Learn more about Google Cloud authentication at https://cloud.google.com/docs/authentication. The underlying error message was: Cannot open credentials file /home/src/...\nSolution: Inside the Mage app:\nCreate a credentials folder (e.g. gcp-creds) within the magic-zoomcamp folder\nIn the credentials folder create a .json key file (e.g. mage-gcp-creds.json)\nCopy/paste GCP service account credentials into the .json key file and save\nUpdate code to point to this file. E.g.\nenviron['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/magic-zoomcamp/gcp-creds/mage-gcp-creds.json'",
        "question": "Where should the credentials folder be created according to the solution?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 196,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: The following error occurs when attempting to export data from Mage to a GCS bucket using pyarrow suggesting Mage doesn’t have the necessary permissions to access the specified GCP credentials .json file.\nArrowException: Unknown error: google::cloud::Status(UNKNOWN: Permanent error GetBucketMetadata: Could not create a OAuth2 access token to authenticate the request. The request was not sent, as such an access token is required to complete the request successfully. Learn more about Google Cloud authentication at https://cloud.google.com/docs/authentication. The underlying error message was: Cannot open credentials file /home/src/...\nSolution: Inside the Mage app:\nCreate a credentials folder (e.g. gcp-creds) within the magic-zoomcamp folder\nIn the credentials folder create a .json key file (e.g. mage-gcp-creds.json)\nCopy/paste GCP service account credentials into the .json key file and save\nUpdate code to point to this file. E.g.\nenviron['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/magic-zoomcamp/gcp-creds/mage-gcp-creds.json'",
        "question": "What should the environment variable 'GOOGLE_APPLICATION_CREDENTIALS' point to for successful authentication?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 196,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Oserror: google::cloud::status(unavailable: retry policy exhausted getbucketmetadata: could not create a OAuth2 access token to authenticate the request. the request was not sent, as such an access token is required to complete the request successfully. learn more about google cloud authentication at https://cloud.google.com/docs/authentication. the underlying error message was: performwork() - curl error [6]=couldn't resolve host name)",
        "question": "What type of error is being reported in the message?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 197,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Oserror: google::cloud::status(unavailable: retry policy exhausted getbucketmetadata: could not create a OAuth2 access token to authenticate the request. the request was not sent, as such an access token is required to complete the request successfully. learn more about google cloud authentication at https://cloud.google.com/docs/authentication. the underlying error message was: performwork() - curl error [6]=couldn't resolve host name)",
        "question": "What is required to complete the request successfully according to the message?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 197,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Oserror: google::cloud::status(unavailable: retry policy exhausted getbucketmetadata: could not create a OAuth2 access token to authenticate the request. the request was not sent, as such an access token is required to complete the request successfully. learn more about google cloud authentication at https://cloud.google.com/docs/authentication. the underlying error message was: performwork() - curl error [6]=couldn't resolve host name)",
        "question": "Where can one learn more about Google Cloud authentication?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 197,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Oserror: google::cloud::status(unavailable: retry policy exhausted getbucketmetadata: could not create a OAuth2 access token to authenticate the request. the request was not sent, as such an access token is required to complete the request successfully. learn more about google cloud authentication at https://cloud.google.com/docs/authentication. the underlying error message was: performwork() - curl error [6]=couldn't resolve host name)",
        "question": "What does the underlying error message indicate regarding network issues?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 197,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Oserror: google::cloud::status(unavailable: retry policy exhausted getbucketmetadata: could not create a OAuth2 access token to authenticate the request. the request was not sent, as such an access token is required to complete the request successfully. learn more about google cloud authentication at https://cloud.google.com/docs/authentication. the underlying error message was: performwork() - curl error [6]=couldn't resolve host name)",
        "question": "What specific failure is mentioned related to the OAuth2 access token?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 197,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: The following error occurs when attempting to export data from Mage to a GCS bucket. Assigned service account doesn’t have the necessary permissions access Google Cloud Storage Bucket\nPermissionError: [Errno 13] google::cloud::Status(PERMISSION_DENIED: Permanent error GetBucketMetadata:... .iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist). error_info={reason=forbidden, domain=global, metadata={http_status_code=403}}). Detail: [errno 13] Permission denied\nSolution: Add Cloud Storage Admin role to the service account:\nGo to project in Google Cloud Console>IAM & Admin>IAM\nClick Edit principal (pencil symbol) to the right of the service account you are using\nClick + ADD ANOTHER ROLE\nSelect Cloud Storage>Storage Admin\nClick Save",
        "question": "What error occurs when attempting to export data from Mage to a GCS bucket?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 198,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: The following error occurs when attempting to export data from Mage to a GCS bucket. Assigned service account doesn’t have the necessary permissions access Google Cloud Storage Bucket\nPermissionError: [Errno 13] google::cloud::Status(PERMISSION_DENIED: Permanent error GetBucketMetadata:... .iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist). error_info={reason=forbidden, domain=global, metadata={http_status_code=403}}). Detail: [errno 13] Permission denied\nSolution: Add Cloud Storage Admin role to the service account:\nGo to project in Google Cloud Console>IAM & Admin>IAM\nClick Edit principal (pencil symbol) to the right of the service account you are using\nClick + ADD ANOTHER ROLE\nSelect Cloud Storage>Storage Admin\nClick Save",
        "question": "What permission is denied for the service account when accessing the Google Cloud Storage bucket?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 198,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: The following error occurs when attempting to export data from Mage to a GCS bucket. Assigned service account doesn’t have the necessary permissions access Google Cloud Storage Bucket\nPermissionError: [Errno 13] google::cloud::Status(PERMISSION_DENIED: Permanent error GetBucketMetadata:... .iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist). error_info={reason=forbidden, domain=global, metadata={http_status_code=403}}). Detail: [errno 13] Permission denied\nSolution: Add Cloud Storage Admin role to the service account:\nGo to project in Google Cloud Console>IAM & Admin>IAM\nClick Edit principal (pencil symbol) to the right of the service account you are using\nClick + ADD ANOTHER ROLE\nSelect Cloud Storage>Storage Admin\nClick Save",
        "question": "What steps should be taken to resolve the PermissionError encountered?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 198,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: The following error occurs when attempting to export data from Mage to a GCS bucket. Assigned service account doesn’t have the necessary permissions access Google Cloud Storage Bucket\nPermissionError: [Errno 13] google::cloud::Status(PERMISSION_DENIED: Permanent error GetBucketMetadata:... .iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist). error_info={reason=forbidden, domain=global, metadata={http_status_code=403}}). Detail: [errno 13] Permission denied\nSolution: Add Cloud Storage Admin role to the service account:\nGo to project in Google Cloud Console>IAM & Admin>IAM\nClick Edit principal (pencil symbol) to the right of the service account you are using\nClick + ADD ANOTHER ROLE\nSelect Cloud Storage>Storage Admin\nClick Save",
        "question": "Which role needs to be added to the service account to gain access to the storage bucket?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 198,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: The following error occurs when attempting to export data from Mage to a GCS bucket. Assigned service account doesn’t have the necessary permissions access Google Cloud Storage Bucket\nPermissionError: [Errno 13] google::cloud::Status(PERMISSION_DENIED: Permanent error GetBucketMetadata:... .iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist). error_info={reason=forbidden, domain=global, metadata={http_status_code=403}}). Detail: [errno 13] Permission denied\nSolution: Add Cloud Storage Admin role to the service account:\nGo to project in Google Cloud Console>IAM & Admin>IAM\nClick Edit principal (pencil symbol) to the right of the service account you are using\nClick + ADD ANOTHER ROLE\nSelect Cloud Storage>Storage Admin\nClick Save",
        "question": "Where in the Google Cloud Console can you edit the service account's permissions?",
        "section": "Module 2: Workflow Orchestration",
        "document_id": 198,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "1. Make sure your pyspark script is ready to be send to Dataproc cluster\n2. Create a Dataproc Cluster in GCP Console\n3. Make sure to edit the service account and add new role - Dataproc Editor\n4. Copy the python script ./notebooks/pyspark_script.py and place it under GCS bucket path\n5. Make sure gcloud cli is installed either in Mage manually or  via your Dockerfile and docker-compose files. This is needed to let Mage access google Dataproc and the script it needs to execute. Refer - Installing the latest gcloud CLI\n6. Use the Bigquery/Dataproc script mentioned here - https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/code/cloud.md . Use Mage to trigger the query",
        "question": "What is the first step to prepare a pyspark script for Dataproc cluster?",
        "section": "Module 3: Data Warehousing",
        "document_id": 199,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "1. Make sure your pyspark script is ready to be send to Dataproc cluster\n2. Create a Dataproc Cluster in GCP Console\n3. Make sure to edit the service account and add new role - Dataproc Editor\n4. Copy the python script ./notebooks/pyspark_script.py and place it under GCS bucket path\n5. Make sure gcloud cli is installed either in Mage manually or  via your Dockerfile and docker-compose files. This is needed to let Mage access google Dataproc and the script it needs to execute. Refer - Installing the latest gcloud CLI\n6. Use the Bigquery/Dataproc script mentioned here - https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/code/cloud.md . Use Mage to trigger the query",
        "question": "How do you create a Dataproc Cluster in the GCP Console?",
        "section": "Module 3: Data Warehousing",
        "document_id": 199,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "1. Make sure your pyspark script is ready to be send to Dataproc cluster\n2. Create a Dataproc Cluster in GCP Console\n3. Make sure to edit the service account and add new role - Dataproc Editor\n4. Copy the python script ./notebooks/pyspark_script.py and place it under GCS bucket path\n5. Make sure gcloud cli is installed either in Mage manually or  via your Dockerfile and docker-compose files. This is needed to let Mage access google Dataproc and the script it needs to execute. Refer - Installing the latest gcloud CLI\n6. Use the Bigquery/Dataproc script mentioned here - https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/code/cloud.md . Use Mage to trigger the query",
        "question": "What role needs to be added to the service account for Dataproc?",
        "section": "Module 3: Data Warehousing",
        "document_id": 199,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "1. Make sure your pyspark script is ready to be send to Dataproc cluster\n2. Create a Dataproc Cluster in GCP Console\n3. Make sure to edit the service account and add new role - Dataproc Editor\n4. Copy the python script ./notebooks/pyspark_script.py and place it under GCS bucket path\n5. Make sure gcloud cli is installed either in Mage manually or  via your Dockerfile and docker-compose files. This is needed to let Mage access google Dataproc and the script it needs to execute. Refer - Installing the latest gcloud CLI\n6. Use the Bigquery/Dataproc script mentioned here - https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/code/cloud.md . Use Mage to trigger the query",
        "question": "Where should the Python script be placed after copying it?",
        "section": "Module 3: Data Warehousing",
        "document_id": 199,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "1. Make sure your pyspark script is ready to be send to Dataproc cluster\n2. Create a Dataproc Cluster in GCP Console\n3. Make sure to edit the service account and add new role - Dataproc Editor\n4. Copy the python script ./notebooks/pyspark_script.py and place it under GCS bucket path\n5. Make sure gcloud cli is installed either in Mage manually or  via your Dockerfile and docker-compose files. This is needed to let Mage access google Dataproc and the script it needs to execute. Refer - Installing the latest gcloud CLI\n6. Use the Bigquery/Dataproc script mentioned here - https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/code/cloud.md . Use Mage to trigger the query",
        "question": "What is required to allow Mage to access Google Dataproc and execute the script?",
        "section": "Module 3: Data Warehousing",
        "document_id": 199,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "A:\n1 solution) Add -Y flag, so that apt-get automatically agrees to install additional packages\n2) Use python ZipFile package, which is included in all modern python distributions",
        "question": "What does the -Y flag do when used with apt-get?",
        "section": "Module 3: Data Warehousing",
        "document_id": 200,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "A:\n1 solution) Add -Y flag, so that apt-get automatically agrees to install additional packages\n2) Use python ZipFile package, which is included in all modern python distributions",
        "question": "Which Python package is mentioned for working with ZIP files?",
        "section": "Module 3: Data Warehousing",
        "document_id": 200,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "A:\n1 solution) Add -Y flag, so that apt-get automatically agrees to install additional packages\n2) Use python ZipFile package, which is included in all modern python distributions",
        "question": "Is the ZipFile package included in all modern Python distributions?",
        "section": "Module 3: Data Warehousing",
        "document_id": 200,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "A:\n1 solution) Add -Y flag, so that apt-get automatically agrees to install additional packages\n2) Use python ZipFile package, which is included in all modern python distributions",
        "question": "What is the first solution suggested for installing additional packages with apt-get?",
        "section": "Module 3: Data Warehousing",
        "document_id": 200,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "A:\n1 solution) Add -Y flag, so that apt-get automatically agrees to install additional packages\n2) Use python ZipFile package, which is included in all modern python distributions",
        "question": "Are there any prerequisites mentioned for using the ZipFile package in Python?",
        "section": "Module 3: Data Warehousing",
        "document_id": 200,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure to use Nullable dataTypes, such as Int64 when appliable.",
        "question": "What are Nullable dataTypes?",
        "section": "Module 3: Data Warehousing",
        "document_id": 201,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure to use Nullable dataTypes, such as Int64 when appliable.",
        "question": "When should Int64 be used?",
        "section": "Module 3: Data Warehousing",
        "document_id": 201,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure to use Nullable dataTypes, such as Int64 when appliable.",
        "question": "Why is it important to use Nullable dataTypes?",
        "section": "Module 3: Data Warehousing",
        "document_id": 201,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure to use Nullable dataTypes, such as Int64 when appliable.",
        "question": "What is the difference between Nullable and non-nullable dataTypes?",
        "section": "Module 3: Data Warehousing",
        "document_id": 201,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure to use Nullable dataTypes, such as Int64 when appliable.",
        "question": "Can you provide examples of when to apply Nullable dataTypes?",
        "section": "Module 3: Data Warehousing",
        "document_id": 201,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ultimately, when trying to ingest data into a BigQuery table, all files within a given directory must have the same schema.\nWhen dealing for example with the FHV Datasets from 2019, however (see image below), one can see that the files for '2019-05', and 2019-06, have the columns \"PUlocationID\" and \"DOlocationID\" as Integers, while for the period of '2019-01' through '2019-04', the same column is defined as FLOAT.\nSo while importing these files as parquet to BigQuery, the first one will be used to define the schema of the table, while all files following that will be used to append data on the existing table. Which means, they must all follow the very same schema of the file that created the table.\nSo, in order to prevent errors like that, make sure to enforce the data types for the columns on the DataFrame before you serialize/upload them to BigQuery. Like this:\npd.read_csv(\"path_or_url\").astype({\n\t\"col1_name\": \"datatype\",\t\n\t\"col2_name\": \"datatype\",\t\n\t...\t\t\t\t\t\n\t\"colN_name\": \"datatype\" \t\n})",
        "question": "What is a requirement for files within a directory when ingesting data into a BigQuery table?",
        "section": "Module 3: Data Warehousing",
        "document_id": 202,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ultimately, when trying to ingest data into a BigQuery table, all files within a given directory must have the same schema.\nWhen dealing for example with the FHV Datasets from 2019, however (see image below), one can see that the files for '2019-05', and 2019-06, have the columns \"PUlocationID\" and \"DOlocationID\" as Integers, while for the period of '2019-01' through '2019-04', the same column is defined as FLOAT.\nSo while importing these files as parquet to BigQuery, the first one will be used to define the schema of the table, while all files following that will be used to append data on the existing table. Which means, they must all follow the very same schema of the file that created the table.\nSo, in order to prevent errors like that, make sure to enforce the data types for the columns on the DataFrame before you serialize/upload them to BigQuery. Like this:\npd.read_csv(\"path_or_url\").astype({\n\t\"col1_name\": \"datatype\",\t\n\t\"col2_name\": \"datatype\",\t\n\t...\t\t\t\t\t\n\t\"colN_name\": \"datatype\" \t\n})",
        "question": "How are the columns 'PUlocationID' and 'DOlocationID' defined in the FHV Datasets from 2019?",
        "section": "Module 3: Data Warehousing",
        "document_id": 202,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ultimately, when trying to ingest data into a BigQuery table, all files within a given directory must have the same schema.\nWhen dealing for example with the FHV Datasets from 2019, however (see image below), one can see that the files for '2019-05', and 2019-06, have the columns \"PUlocationID\" and \"DOlocationID\" as Integers, while for the period of '2019-01' through '2019-04', the same column is defined as FLOAT.\nSo while importing these files as parquet to BigQuery, the first one will be used to define the schema of the table, while all files following that will be used to append data on the existing table. Which means, they must all follow the very same schema of the file that created the table.\nSo, in order to prevent errors like that, make sure to enforce the data types for the columns on the DataFrame before you serialize/upload them to BigQuery. Like this:\npd.read_csv(\"path_or_url\").astype({\n\t\"col1_name\": \"datatype\",\t\n\t\"col2_name\": \"datatype\",\t\n\t...\t\t\t\t\t\n\t\"colN_name\": \"datatype\" \t\n})",
        "question": "What happens to the schema of the table when importing files into BigQuery?",
        "section": "Module 3: Data Warehousing",
        "document_id": 202,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ultimately, when trying to ingest data into a BigQuery table, all files within a given directory must have the same schema.\nWhen dealing for example with the FHV Datasets from 2019, however (see image below), one can see that the files for '2019-05', and 2019-06, have the columns \"PUlocationID\" and \"DOlocationID\" as Integers, while for the period of '2019-01' through '2019-04', the same column is defined as FLOAT.\nSo while importing these files as parquet to BigQuery, the first one will be used to define the schema of the table, while all files following that will be used to append data on the existing table. Which means, they must all follow the very same schema of the file that created the table.\nSo, in order to prevent errors like that, make sure to enforce the data types for the columns on the DataFrame before you serialize/upload them to BigQuery. Like this:\npd.read_csv(\"path_or_url\").astype({\n\t\"col1_name\": \"datatype\",\t\n\t\"col2_name\": \"datatype\",\t\n\t...\t\t\t\t\t\n\t\"colN_name\": \"datatype\" \t\n})",
        "question": "How can one prevent errors related to schema differences when uploading files to BigQuery?",
        "section": "Module 3: Data Warehousing",
        "document_id": 202,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ultimately, when trying to ingest data into a BigQuery table, all files within a given directory must have the same schema.\nWhen dealing for example with the FHV Datasets from 2019, however (see image below), one can see that the files for '2019-05', and 2019-06, have the columns \"PUlocationID\" and \"DOlocationID\" as Integers, while for the period of '2019-01' through '2019-04', the same column is defined as FLOAT.\nSo while importing these files as parquet to BigQuery, the first one will be used to define the schema of the table, while all files following that will be used to append data on the existing table. Which means, they must all follow the very same schema of the file that created the table.\nSo, in order to prevent errors like that, make sure to enforce the data types for the columns on the DataFrame before you serialize/upload them to BigQuery. Like this:\npd.read_csv(\"path_or_url\").astype({\n\t\"col1_name\": \"datatype\",\t\n\t\"col2_name\": \"datatype\",\t\n\t...\t\t\t\t\t\n\t\"colN_name\": \"datatype\" \t\n})",
        "question": "What function is suggested to enforce data types for columns in a DataFrame before serialization to BigQuery?",
        "section": "Module 3: Data Warehousing",
        "document_id": 202,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you receive the error gzip.BadGzipFile: Not a gzipped file (b'\\n\\n'), this is because you have specified the wrong URL to the FHV dataset. Make sure to use https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\nEmphasising the ‘/releases/download’ part of the URL.",
        "question": "What error is indicated when the file is not recognized as a gzipped file?",
        "section": "Module 3: Data Warehousing",
        "document_id": 203,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you receive the error gzip.BadGzipFile: Not a gzipped file (b'\\n\\n'), this is because you have specified the wrong URL to the FHV dataset. Make sure to use https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\nEmphasising the ‘/releases/download’ part of the URL.",
        "question": "What could cause the error gzip.BadGzipFile: Not a gzipped file?",
        "section": "Module 3: Data Warehousing",
        "document_id": 203,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you receive the error gzip.BadGzipFile: Not a gzipped file (b'\\n\\n'), this is because you have specified the wrong URL to the FHV dataset. Make sure to use https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\nEmphasising the ‘/releases/download’ part of the URL.",
        "question": "What is the correct URL format to access the FHV dataset?",
        "section": "Module 3: Data Warehousing",
        "document_id": 203,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you receive the error gzip.BadGzipFile: Not a gzipped file (b'\\n\\n'), this is because you have specified the wrong URL to the FHV dataset. Make sure to use https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\nEmphasising the ‘/releases/download’ part of the URL.",
        "question": "Why is the '/releases/download' part of the URL emphasized?",
        "section": "Module 3: Data Warehousing",
        "document_id": 203,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you receive the error gzip.BadGzipFile: Not a gzipped file (b'\\n\\n'), this is because you have specified the wrong URL to the FHV dataset. Make sure to use https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\nEmphasising the ‘/releases/download’ part of the URL.",
        "question": "What should you check if you encounter a bad gzip file error when accessing the FHV dataset?",
        "section": "Module 3: Data Warehousing",
        "document_id": 203,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Krishna Anand",
        "question": "Who is Krishna Anand?",
        "section": "Module 3: Data Warehousing",
        "document_id": 204,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Krishna Anand",
        "question": "What significance does Krishna Anand have?",
        "section": "Module 3: Data Warehousing",
        "document_id": 204,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Krishna Anand",
        "question": "In what context is Krishna Anand mentioned?",
        "section": "Module 3: Data Warehousing",
        "document_id": 204,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Krishna Anand",
        "question": "What are the key contributions of Krishna Anand?",
        "section": "Module 3: Data Warehousing",
        "document_id": 204,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Krishna Anand",
        "question": "What themes are associated with Krishna Anand?",
        "section": "Module 3: Data Warehousing",
        "document_id": 204,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check the Schema\nYou might have a wrong formatting\nTry to upload the CSV.GZ files without formatting or going through pandas via wget\nSee this Slack conversation for helpful tips",
        "question": "What should you check if you encounter issues with file formatting?",
        "section": "Module 3: Data Warehousing",
        "document_id": 205,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check the Schema\nYou might have a wrong formatting\nTry to upload the CSV.GZ files without formatting or going through pandas via wget\nSee this Slack conversation for helpful tips",
        "question": "What type of files should be uploaded without formatting?",
        "section": "Module 3: Data Warehousing",
        "document_id": 205,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check the Schema\nYou might have a wrong formatting\nTry to upload the CSV.GZ files without formatting or going through pandas via wget\nSee this Slack conversation for helpful tips",
        "question": "Why is it recommended to avoid using pandas when uploading CSV.GZ files?",
        "section": "Module 3: Data Warehousing",
        "document_id": 205,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check the Schema\nYou might have a wrong formatting\nTry to upload the CSV.GZ files without formatting or going through pandas via wget\nSee this Slack conversation for helpful tips",
        "question": "Where can you find helpful tips regarding file upload issues?",
        "section": "Module 3: Data Warehousing",
        "document_id": 205,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check the Schema\nYou might have a wrong formatting\nTry to upload the CSV.GZ files without formatting or going through pandas via wget\nSee this Slack conversation for helpful tips",
        "question": "What method is suggested for uploading CSV.GZ files?",
        "section": "Module 3: Data Warehousing",
        "document_id": 205,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Run the following command to check if “BigQuery Command Line Tool” is installed or not: gcloud components list\nYou can also use bq.cmd instead of bq to make it work.",
        "question": "What command is used to check if the BigQuery Command Line Tool is installed?",
        "section": "Module 3: Data Warehousing",
        "document_id": 206,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Run the following command to check if “BigQuery Command Line Tool” is installed or not: gcloud components list\nYou can also use bq.cmd instead of bq to make it work.",
        "question": "What is an alternative command to use instead of bq?",
        "section": "Module 3: Data Warehousing",
        "document_id": 206,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Run the following command to check if “BigQuery Command Line Tool” is installed or not: gcloud components list\nYou can also use bq.cmd instead of bq to make it work.",
        "question": "Which tool is referred to in the text for command line operations?",
        "section": "Module 3: Data Warehousing",
        "document_id": 206,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Run the following command to check if “BigQuery Command Line Tool” is installed or not: gcloud components list\nYou can also use bq.cmd instead of bq to make it work.",
        "question": "How can you verify the installation of the BigQuery Command Line Tool?",
        "section": "Module 3: Data Warehousing",
        "document_id": 206,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Run the following command to check if “BigQuery Command Line Tool” is installed or not: gcloud components list\nYou can also use bq.cmd instead of bq to make it work.",
        "question": "What does the command 'gcloud components list' do?",
        "section": "Module 3: Data Warehousing",
        "document_id": 206,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use big queries carefully,\nI created by bigquery dataset on an account where my free trial was exhausted, and got a bill of $80.\nUse big query in free credits and destroy all the datasets after creation.\nCheck your Billing daily! Especially if you’ve spinned up a VM.",
        "question": "What should you do to avoid incurring charges when using BigQuery?",
        "section": "Module 3: Data Warehousing",
        "document_id": 207,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use big queries carefully,\nI created by bigquery dataset on an account where my free trial was exhausted, and got a bill of $80.\nUse big query in free credits and destroy all the datasets after creation.\nCheck your Billing daily! Especially if you’ve spinned up a VM.",
        "question": "What happened when the author created a BigQuery dataset after their free trial ended?",
        "section": "Module 3: Data Warehousing",
        "document_id": 207,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use big queries carefully,\nI created by bigquery dataset on an account where my free trial was exhausted, and got a bill of $80.\nUse big query in free credits and destroy all the datasets after creation.\nCheck your Billing daily! Especially if you’ve spinned up a VM.",
        "question": "Why is it important to check your billing daily when using BigQuery?",
        "section": "Module 3: Data Warehousing",
        "document_id": 207,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use big queries carefully,\nI created by bigquery dataset on an account where my free trial was exhausted, and got a bill of $80.\nUse big query in free credits and destroy all the datasets after creation.\nCheck your Billing daily! Especially if you’ve spinned up a VM.",
        "question": "What should you do with your datasets after using free credits in BigQuery?",
        "section": "Module 3: Data Warehousing",
        "document_id": 207,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use big queries carefully,\nI created by bigquery dataset on an account where my free trial was exhausted, and got a bill of $80.\nUse big query in free credits and destroy all the datasets after creation.\nCheck your Billing daily! Especially if you’ve spinned up a VM.",
        "question": "What additional resource is mentioned that could result in charges if not monitored?",
        "section": "Module 3: Data Warehousing",
        "document_id": 207,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Be careful when you create your resources on GCP, all of them have to share the same Region in order to allow load data from GCS Bucket to BigQuery. If you forgot it when you created them, you can create a new dataset on BigQuery using the same Region which you used on your GCS Bucket.\nThis means that your GCS Bucket and the BigQuery dataset are placed in different regions. You have to create a new dataset inside BigQuery in the same region with your GCS bucket and store the data in the newly created dataset.",
        "question": "What must be shared among resources on GCP to allow data loading from GCS Bucket to BigQuery?",
        "section": "Module 3: Data Warehousing",
        "document_id": 208,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Be careful when you create your resources on GCP, all of them have to share the same Region in order to allow load data from GCS Bucket to BigQuery. If you forgot it when you created them, you can create a new dataset on BigQuery using the same Region which you used on your GCS Bucket.\nThis means that your GCS Bucket and the BigQuery dataset are placed in different regions. You have to create a new dataset inside BigQuery in the same region with your GCS bucket and store the data in the newly created dataset.",
        "question": "What should you do if your GCS Bucket and BigQuery dataset are in different regions?",
        "section": "Module 3: Data Warehousing",
        "document_id": 208,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Be careful when you create your resources on GCP, all of them have to share the same Region in order to allow load data from GCS Bucket to BigQuery. If you forgot it when you created them, you can create a new dataset on BigQuery using the same Region which you used on your GCS Bucket.\nThis means that your GCS Bucket and the BigQuery dataset are placed in different regions. You have to create a new dataset inside BigQuery in the same region with your GCS bucket and store the data in the newly created dataset.",
        "question": "How can you ensure that your BigQuery dataset is compatible with your GCS Bucket?",
        "section": "Module 3: Data Warehousing",
        "document_id": 208,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Be careful when you create your resources on GCP, all of them have to share the same Region in order to allow load data from GCS Bucket to BigQuery. If you forgot it when you created them, you can create a new dataset on BigQuery using the same Region which you used on your GCS Bucket.\nThis means that your GCS Bucket and the BigQuery dataset are placed in different regions. You have to create a new dataset inside BigQuery in the same region with your GCS bucket and store the data in the newly created dataset.",
        "question": "What is the consequence of forgetting to set the same region for GCS and BigQuery resources?",
        "section": "Module 3: Data Warehousing",
        "document_id": 208,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Be careful when you create your resources on GCP, all of them have to share the same Region in order to allow load data from GCS Bucket to BigQuery. If you forgot it when you created them, you can create a new dataset on BigQuery using the same Region which you used on your GCS Bucket.\nThis means that your GCS Bucket and the BigQuery dataset are placed in different regions. You have to create a new dataset inside BigQuery in the same region with your GCS bucket and store the data in the newly created dataset.",
        "question": "What action is necessary to create a new dataset in BigQuery if the initial one is in a different region than the GCS Bucket?",
        "section": "Module 3: Data Warehousing",
        "document_id": 208,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure to create the BigQuery dataset in the very same location that you've created the GCS Bucket. For instance, if your GCS Bucket was created in `us-central1`, then BigQuery dataset must be created in the same region (us-central1, in this example)",
        "question": "What is the relationship between the location of a BigQuery dataset and a GCS Bucket?",
        "section": "Module 3: Data Warehousing",
        "document_id": 209,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure to create the BigQuery dataset in the very same location that you've created the GCS Bucket. For instance, if your GCS Bucket was created in `us-central1`, then BigQuery dataset must be created in the same region (us-central1, in this example)",
        "question": "In which region should the BigQuery dataset be created if the GCS Bucket is in 'us-central1'?",
        "section": "Module 3: Data Warehousing",
        "document_id": 209,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure to create the BigQuery dataset in the very same location that you've created the GCS Bucket. For instance, if your GCS Bucket was created in `us-central1`, then BigQuery dataset must be created in the same region (us-central1, in this example)",
        "question": "Why is it important to create the BigQuery dataset in the same location as the GCS Bucket?",
        "section": "Module 3: Data Warehousing",
        "document_id": 209,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure to create the BigQuery dataset in the very same location that you've created the GCS Bucket. For instance, if your GCS Bucket was created in `us-central1`, then BigQuery dataset must be created in the same region (us-central1, in this example)",
        "question": "What happens if the BigQuery dataset and GCS Bucket are in different regions?",
        "section": "Module 3: Data Warehousing",
        "document_id": 209,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure to create the BigQuery dataset in the very same location that you've created the GCS Bucket. For instance, if your GCS Bucket was created in `us-central1`, then BigQuery dataset must be created in the same region (us-central1, in this example)",
        "question": "Can you create a BigQuery dataset in a different location than the GCS Bucket?",
        "section": "Module 3: Data Warehousing",
        "document_id": 209,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "By the way, this isn’t a problem/solution, but a useful hint:\nPlease, remember to save your progress in BigQuery SQL Editor.\nI was almost finishing the homework, when my Chrome Tab froze and I had to reload it. Then I lost my entire SQL script.\nSave your script from time to time. Just click on the button at the top bar. Your saved file will be available on the left panel.\nAlternatively, you can copy paste your queries into an .sql file in your preferred editor (Notepad++, VS Code, etc.). Using the .sql extension will provide convenient color formatting.",
        "question": "What should you do to avoid losing your progress in BigQuery SQL Editor?",
        "section": "Module 3: Data Warehousing",
        "document_id": 210,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "By the way, this isn’t a problem/solution, but a useful hint:\nPlease, remember to save your progress in BigQuery SQL Editor.\nI was almost finishing the homework, when my Chrome Tab froze and I had to reload it. Then I lost my entire SQL script.\nSave your script from time to time. Just click on the button at the top bar. Your saved file will be available on the left panel.\nAlternatively, you can copy paste your queries into an .sql file in your preferred editor (Notepad++, VS Code, etc.). Using the .sql extension will provide convenient color formatting.",
        "question": "What happened to the author while working in BigQuery SQL Editor?",
        "section": "Module 3: Data Warehousing",
        "document_id": 210,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "By the way, this isn’t a problem/solution, but a useful hint:\nPlease, remember to save your progress in BigQuery SQL Editor.\nI was almost finishing the homework, when my Chrome Tab froze and I had to reload it. Then I lost my entire SQL script.\nSave your script from time to time. Just click on the button at the top bar. Your saved file will be available on the left panel.\nAlternatively, you can copy paste your queries into an .sql file in your preferred editor (Notepad++, VS Code, etc.). Using the .sql extension will provide convenient color formatting.",
        "question": "How can you save your SQL script in BigQuery SQL Editor?",
        "section": "Module 3: Data Warehousing",
        "document_id": 210,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "By the way, this isn’t a problem/solution, but a useful hint:\nPlease, remember to save your progress in BigQuery SQL Editor.\nI was almost finishing the homework, when my Chrome Tab froze and I had to reload it. Then I lost my entire SQL script.\nSave your script from time to time. Just click on the button at the top bar. Your saved file will be available on the left panel.\nAlternatively, you can copy paste your queries into an .sql file in your preferred editor (Notepad++, VS Code, etc.). Using the .sql extension will provide convenient color formatting.",
        "question": "What is the advantage of using the .sql file extension when saving queries?",
        "section": "Module 3: Data Warehousing",
        "document_id": 210,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "By the way, this isn’t a problem/solution, but a useful hint:\nPlease, remember to save your progress in BigQuery SQL Editor.\nI was almost finishing the homework, when my Chrome Tab froze and I had to reload it. Then I lost my entire SQL script.\nSave your script from time to time. Just click on the button at the top bar. Your saved file will be available on the left panel.\nAlternatively, you can copy paste your queries into an .sql file in your preferred editor (Notepad++, VS Code, etc.). Using the .sql extension will provide convenient color formatting.",
        "question": "Which text editors are suggested for copying and pasting SQL queries?",
        "section": "Module 3: Data Warehousing",
        "document_id": 210,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans :  While real-time analytics might not be explicitly mentioned, BigQuery has real-time data streaming capabilities, allowing for potential integration in future project iterations.",
        "question": "What capabilities does BigQuery have regarding real-time data?",
        "section": "Module 3: Data Warehousing",
        "document_id": 211,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans :  While real-time analytics might not be explicitly mentioned, BigQuery has real-time data streaming capabilities, allowing for potential integration in future project iterations.",
        "question": "Is real-time analytics explicitly mentioned in the text?",
        "section": "Module 3: Data Warehousing",
        "document_id": 211,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans :  While real-time analytics might not be explicitly mentioned, BigQuery has real-time data streaming capabilities, allowing for potential integration in future project iterations.",
        "question": "How might real-time data streaming be utilized in future projects?",
        "section": "Module 3: Data Warehousing",
        "document_id": 211,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans :  While real-time analytics might not be explicitly mentioned, BigQuery has real-time data streaming capabilities, allowing for potential integration in future project iterations.",
        "question": "What assumptions can be made about the future iterations of projects with BigQuery?",
        "section": "Module 3: Data Warehousing",
        "document_id": 211,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans :  While real-time analytics might not be explicitly mentioned, BigQuery has real-time data streaming capabilities, allowing for potential integration in future project iterations.",
        "question": "Can BigQuery's real-time data streaming be integrated into existing projects?",
        "section": "Module 3: Data Warehousing",
        "document_id": 211,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "could not parse 'pickup_datetime' as timestamp for field pickup_datetime (position 2)\nThis error is caused by invalid data in the timestamp column. A way to identify the problem is to define the schema from the external table using string datatype. This enables the queries to work at which point we can filter out the invalid rows from the import to the materialised table and insert the fields with the timestamp data type.",
        "question": "What does the error message indicate about the 'pickup_datetime' field?",
        "section": "Module 3: Data Warehousing",
        "document_id": 212,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "could not parse 'pickup_datetime' as timestamp for field pickup_datetime (position 2)\nThis error is caused by invalid data in the timestamp column. A way to identify the problem is to define the schema from the external table using string datatype. This enables the queries to work at which point we can filter out the invalid rows from the import to the materialised table and insert the fields with the timestamp data type.",
        "question": "What is the cause of the error related to 'pickup_datetime'?",
        "section": "Module 3: Data Warehousing",
        "document_id": 212,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "could not parse 'pickup_datetime' as timestamp for field pickup_datetime (position 2)\nThis error is caused by invalid data in the timestamp column. A way to identify the problem is to define the schema from the external table using string datatype. This enables the queries to work at which point we can filter out the invalid rows from the import to the materialised table and insert the fields with the timestamp data type.",
        "question": "How can one identify invalid data in the timestamp column?",
        "section": "Module 3: Data Warehousing",
        "document_id": 212,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "could not parse 'pickup_datetime' as timestamp for field pickup_datetime (position 2)\nThis error is caused by invalid data in the timestamp column. A way to identify the problem is to define the schema from the external table using string datatype. This enables the queries to work at which point we can filter out the invalid rows from the import to the materialised table and insert the fields with the timestamp data type.",
        "question": "What method is suggested to work with the external table's schema?",
        "section": "Module 3: Data Warehousing",
        "document_id": 212,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "could not parse 'pickup_datetime' as timestamp for field pickup_datetime (position 2)\nThis error is caused by invalid data in the timestamp column. A way to identify the problem is to define the schema from the external table using string datatype. This enables the queries to work at which point we can filter out the invalid rows from the import to the materialised table and insert the fields with the timestamp data type.",
        "question": "What steps should be taken after defining the schema as string datatype?",
        "section": "Module 3: Data Warehousing",
        "document_id": 212,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Background:\n`pd.read_parquet`\n`pd.to_datetime`\n`pq.write_to_dataset`\nReference:\nhttps://stackoverflow.com/questions/48314880/are-parquet-file-created-with-pyarrow-vs-pyspark-compatible\nhttps://stackoverflow.com/questions/57798479/editing-parquet-files-with-python-causes-errors-to-datetime-format\nhttps://www.reddit.com/r/bigquery/comments/16aoq0u/parquet_timestamp_to_bq_coming_across_as_int/?share_id=YXqCs5Jl6hQcw-kg6-VgF&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1\nSolution:\nAdd `use_deprecated_int96_timestamps=True` to `pq.write_to_dataset` function, like below\npq.write_to_dataset(\ntable,\nroot_path=root_path,\nfilesystem=gcs,\nuse_deprecated_int96_timestamps=True\n# Write timestamps to INT96 Parquet format\n)",
        "question": "What is the purpose of the `pd.read_parquet` function?",
        "section": "Module 3: Data Warehousing",
        "document_id": 213,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Background:\n`pd.read_parquet`\n`pd.to_datetime`\n`pq.write_to_dataset`\nReference:\nhttps://stackoverflow.com/questions/48314880/are-parquet-file-created-with-pyarrow-vs-pyspark-compatible\nhttps://stackoverflow.com/questions/57798479/editing-parquet-files-with-python-causes-errors-to-datetime-format\nhttps://www.reddit.com/r/bigquery/comments/16aoq0u/parquet_timestamp_to_bq_coming_across_as_int/?share_id=YXqCs5Jl6hQcw-kg6-VgF&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1\nSolution:\nAdd `use_deprecated_int96_timestamps=True` to `pq.write_to_dataset` function, like below\npq.write_to_dataset(\ntable,\nroot_path=root_path,\nfilesystem=gcs,\nuse_deprecated_int96_timestamps=True\n# Write timestamps to INT96 Parquet format\n)",
        "question": "How can `pd.to_datetime` help with datetime conversions in a parquet file?",
        "section": "Module 3: Data Warehousing",
        "document_id": 213,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Background:\n`pd.read_parquet`\n`pd.to_datetime`\n`pq.write_to_dataset`\nReference:\nhttps://stackoverflow.com/questions/48314880/are-parquet-file-created-with-pyarrow-vs-pyspark-compatible\nhttps://stackoverflow.com/questions/57798479/editing-parquet-files-with-python-causes-errors-to-datetime-format\nhttps://www.reddit.com/r/bigquery/comments/16aoq0u/parquet_timestamp_to_bq_coming_across_as_int/?share_id=YXqCs5Jl6hQcw-kg6-VgF&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1\nSolution:\nAdd `use_deprecated_int96_timestamps=True` to `pq.write_to_dataset` function, like below\npq.write_to_dataset(\ntable,\nroot_path=root_path,\nfilesystem=gcs,\nuse_deprecated_int96_timestamps=True\n# Write timestamps to INT96 Parquet format\n)",
        "question": "What does the `pq.write_to_dataset` function do in the context of writing parquet files?",
        "section": "Module 3: Data Warehousing",
        "document_id": 213,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Background:\n`pd.read_parquet`\n`pd.to_datetime`\n`pq.write_to_dataset`\nReference:\nhttps://stackoverflow.com/questions/48314880/are-parquet-file-created-with-pyarrow-vs-pyspark-compatible\nhttps://stackoverflow.com/questions/57798479/editing-parquet-files-with-python-causes-errors-to-datetime-format\nhttps://www.reddit.com/r/bigquery/comments/16aoq0u/parquet_timestamp_to_bq_coming_across_as_int/?share_id=YXqCs5Jl6hQcw-kg6-VgF&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1\nSolution:\nAdd `use_deprecated_int96_timestamps=True` to `pq.write_to_dataset` function, like below\npq.write_to_dataset(\ntable,\nroot_path=root_path,\nfilesystem=gcs,\nuse_deprecated_int96_timestamps=True\n# Write timestamps to INT96 Parquet format\n)",
        "question": "Why might one need to use `use_deprecated_int96_timestamps=True` when writing to a parquet dataset?",
        "section": "Module 3: Data Warehousing",
        "document_id": 213,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Background:\n`pd.read_parquet`\n`pd.to_datetime`\n`pq.write_to_dataset`\nReference:\nhttps://stackoverflow.com/questions/48314880/are-parquet-file-created-with-pyarrow-vs-pyspark-compatible\nhttps://stackoverflow.com/questions/57798479/editing-parquet-files-with-python-causes-errors-to-datetime-format\nhttps://www.reddit.com/r/bigquery/comments/16aoq0u/parquet_timestamp_to_bq_coming_across_as_int/?share_id=YXqCs5Jl6hQcw-kg6-VgF&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1\nSolution:\nAdd `use_deprecated_int96_timestamps=True` to `pq.write_to_dataset` function, like below\npq.write_to_dataset(\ntable,\nroot_path=root_path,\nfilesystem=gcs,\nuse_deprecated_int96_timestamps=True\n# Write timestamps to INT96 Parquet format\n)",
        "question": "What are some potential compatibility issues between parquet files created with PyArrow and PySpark?",
        "section": "Module 3: Data Warehousing",
        "document_id": 213,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution:\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage use PyArrow to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won't be converted to timestamp when loaded by BigQuery later on.\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport os\nif 'data_exporter' not in globals():\nfrom mage_ai.data_preparation.decorators import data_exporter\n# Replace with the location of your service account key JSON file.\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/personal-gcp.json'\nbucket_name = \"<YOUR_BUCKET_NAME>\"\nobject_key = 'nyc_taxi_data_2022.parquet'\nwhere = f'{bucket_name}/{object_key}'\n@data_exporter\ndef export_data(data, *args, **kwargs):\ntable = pa.Table.from_pandas(data, preserve_index=False)\ngcs = pa.fs.GcsFileSystem()\npq.write_table(\ntable,\nwhere,\n# Convert integer columns in Epoch milliseconds\n# to Timestamp columns in microseconds ('us') so\n# they can be loaded into BigQuery with the right\n# data type\ncoerce_timestamps='us',\nfilesystem=gcs\n)\nSolution 2:\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage, provide PyArrow with explicit schema to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won't be converted to timestamp when loaded by BigQuery later on.\nschema = pa.schema([\n('vendor_id', pa.int64()),\n('lpep_pickup_datetime', pa.timestamp('ns')),\n('lpep_dropoff_datetime', pa.timestamp('ns')),\n('store_and_fwd_flag', pa.string()),\n('ratecode_id', pa.int64()),\n('pu_location_id', pa.int64()),\n('do_location_id', pa.int64()),\n('passenger_count', pa.int64()),\n('trip_distance', pa.float64()),\n('fare_amount', pa.float64()),\n('extra', pa.float64()),\n('mta_tax', pa.float64()),\n('tip_amount', pa.float64()),\n('tolls_amount', pa.float64()),\n('improvement_surcharge', pa.float64()),\n('total_amount', pa.float64()),\n('payment_type', pa.int64()),\n('trip_type', pa.int64()),\n('congestion_surcharge', pa.float64()),\n('lpep_pickup_month', pa.int64())\n])\ntable = pa.Table.from_pandas(data, schema=schema)",
        "question": "What library is recommended for generating Parquet files for Google Cloud Storage?",
        "section": "Module 3: Data Warehousing",
        "document_id": 214,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution:\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage use PyArrow to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won't be converted to timestamp when loaded by BigQuery later on.\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport os\nif 'data_exporter' not in globals():\nfrom mage_ai.data_preparation.decorators import data_exporter\n# Replace with the location of your service account key JSON file.\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/personal-gcp.json'\nbucket_name = \"<YOUR_BUCKET_NAME>\"\nobject_key = 'nyc_taxi_data_2022.parquet'\nwhere = f'{bucket_name}/{object_key}'\n@data_exporter\ndef export_data(data, *args, **kwargs):\ntable = pa.Table.from_pandas(data, preserve_index=False)\ngcs = pa.fs.GcsFileSystem()\npq.write_table(\ntable,\nwhere,\n# Convert integer columns in Epoch milliseconds\n# to Timestamp columns in microseconds ('us') so\n# they can be loaded into BigQuery with the right\n# data type\ncoerce_timestamps='us',\nfilesystem=gcs\n)\nSolution 2:\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage, provide PyArrow with explicit schema to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won't be converted to timestamp when loaded by BigQuery later on.\nschema = pa.schema([\n('vendor_id', pa.int64()),\n('lpep_pickup_datetime', pa.timestamp('ns')),\n('lpep_dropoff_datetime', pa.timestamp('ns')),\n('store_and_fwd_flag', pa.string()),\n('ratecode_id', pa.int64()),\n('pu_location_id', pa.int64()),\n('do_location_id', pa.int64()),\n('passenger_count', pa.int64()),\n('trip_distance', pa.float64()),\n('fare_amount', pa.float64()),\n('extra', pa.float64()),\n('mta_tax', pa.float64()),\n('tip_amount', pa.float64()),\n('tolls_amount', pa.float64()),\n('improvement_surcharge', pa.float64()),\n('total_amount', pa.float64()),\n('payment_type', pa.int64()),\n('trip_type', pa.int64()),\n('congestion_surcharge', pa.float64()),\n('lpep_pickup_month', pa.int64())\n])\ntable = pa.Table.from_pandas(data, schema=schema)",
        "question": "What happens if the datetime columns are not converted to the correct logical type when using BigQuery?",
        "section": "Module 3: Data Warehousing",
        "document_id": 214,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution:\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage use PyArrow to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won't be converted to timestamp when loaded by BigQuery later on.\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport os\nif 'data_exporter' not in globals():\nfrom mage_ai.data_preparation.decorators import data_exporter\n# Replace with the location of your service account key JSON file.\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/personal-gcp.json'\nbucket_name = \"<YOUR_BUCKET_NAME>\"\nobject_key = 'nyc_taxi_data_2022.parquet'\nwhere = f'{bucket_name}/{object_key}'\n@data_exporter\ndef export_data(data, *args, **kwargs):\ntable = pa.Table.from_pandas(data, preserve_index=False)\ngcs = pa.fs.GcsFileSystem()\npq.write_table(\ntable,\nwhere,\n# Convert integer columns in Epoch milliseconds\n# to Timestamp columns in microseconds ('us') so\n# they can be loaded into BigQuery with the right\n# data type\ncoerce_timestamps='us',\nfilesystem=gcs\n)\nSolution 2:\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage, provide PyArrow with explicit schema to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won't be converted to timestamp when loaded by BigQuery later on.\nschema = pa.schema([\n('vendor_id', pa.int64()),\n('lpep_pickup_datetime', pa.timestamp('ns')),\n('lpep_dropoff_datetime', pa.timestamp('ns')),\n('store_and_fwd_flag', pa.string()),\n('ratecode_id', pa.int64()),\n('pu_location_id', pa.int64()),\n('do_location_id', pa.int64()),\n('passenger_count', pa.int64()),\n('trip_distance', pa.float64()),\n('fare_amount', pa.float64()),\n('extra', pa.float64()),\n('mta_tax', pa.float64()),\n('tip_amount', pa.float64()),\n('tolls_amount', pa.float64()),\n('improvement_surcharge', pa.float64()),\n('total_amount', pa.float64()),\n('payment_type', pa.int64()),\n('trip_type', pa.int64()),\n('congestion_surcharge', pa.float64()),\n('lpep_pickup_month', pa.int64())\n])\ntable = pa.Table.from_pandas(data, schema=schema)",
        "question": "What is the purpose of the 'coerce_timestamps' parameter in the PyArrow write_table method?",
        "section": "Module 3: Data Warehousing",
        "document_id": 214,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution:\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage use PyArrow to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won't be converted to timestamp when loaded by BigQuery later on.\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport os\nif 'data_exporter' not in globals():\nfrom mage_ai.data_preparation.decorators import data_exporter\n# Replace with the location of your service account key JSON file.\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/personal-gcp.json'\nbucket_name = \"<YOUR_BUCKET_NAME>\"\nobject_key = 'nyc_taxi_data_2022.parquet'\nwhere = f'{bucket_name}/{object_key}'\n@data_exporter\ndef export_data(data, *args, **kwargs):\ntable = pa.Table.from_pandas(data, preserve_index=False)\ngcs = pa.fs.GcsFileSystem()\npq.write_table(\ntable,\nwhere,\n# Convert integer columns in Epoch milliseconds\n# to Timestamp columns in microseconds ('us') so\n# they can be loaded into BigQuery with the right\n# data type\ncoerce_timestamps='us',\nfilesystem=gcs\n)\nSolution 2:\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage, provide PyArrow with explicit schema to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won't be converted to timestamp when loaded by BigQuery later on.\nschema = pa.schema([\n('vendor_id', pa.int64()),\n('lpep_pickup_datetime', pa.timestamp('ns')),\n('lpep_dropoff_datetime', pa.timestamp('ns')),\n('store_and_fwd_flag', pa.string()),\n('ratecode_id', pa.int64()),\n('pu_location_id', pa.int64()),\n('do_location_id', pa.int64()),\n('passenger_count', pa.int64()),\n('trip_distance', pa.float64()),\n('fare_amount', pa.float64()),\n('extra', pa.float64()),\n('mta_tax', pa.float64()),\n('tip_amount', pa.float64()),\n('tolls_amount', pa.float64()),\n('improvement_surcharge', pa.float64()),\n('total_amount', pa.float64()),\n('payment_type', pa.int64()),\n('trip_type', pa.int64()),\n('congestion_surcharge', pa.float64()),\n('lpep_pickup_month', pa.int64())\n])\ntable = pa.Table.from_pandas(data, schema=schema)",
        "question": "How do you provide an explicit schema when using PyArrow for exporting data?",
        "section": "Module 3: Data Warehousing",
        "document_id": 214,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution:\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage use PyArrow to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won't be converted to timestamp when loaded by BigQuery later on.\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport os\nif 'data_exporter' not in globals():\nfrom mage_ai.data_preparation.decorators import data_exporter\n# Replace with the location of your service account key JSON file.\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/personal-gcp.json'\nbucket_name = \"<YOUR_BUCKET_NAME>\"\nobject_key = 'nyc_taxi_data_2022.parquet'\nwhere = f'{bucket_name}/{object_key}'\n@data_exporter\ndef export_data(data, *args, **kwargs):\ntable = pa.Table.from_pandas(data, preserve_index=False)\ngcs = pa.fs.GcsFileSystem()\npq.write_table(\ntable,\nwhere,\n# Convert integer columns in Epoch milliseconds\n# to Timestamp columns in microseconds ('us') so\n# they can be loaded into BigQuery with the right\n# data type\ncoerce_timestamps='us',\nfilesystem=gcs\n)\nSolution 2:\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage, provide PyArrow with explicit schema to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won't be converted to timestamp when loaded by BigQuery later on.\nschema = pa.schema([\n('vendor_id', pa.int64()),\n('lpep_pickup_datetime', pa.timestamp('ns')),\n('lpep_dropoff_datetime', pa.timestamp('ns')),\n('store_and_fwd_flag', pa.string()),\n('ratecode_id', pa.int64()),\n('pu_location_id', pa.int64()),\n('do_location_id', pa.int64()),\n('passenger_count', pa.int64()),\n('trip_distance', pa.float64()),\n('fare_amount', pa.float64()),\n('extra', pa.float64()),\n('mta_tax', pa.float64()),\n('tip_amount', pa.float64()),\n('tolls_amount', pa.float64()),\n('improvement_surcharge', pa.float64()),\n('total_amount', pa.float64()),\n('payment_type', pa.int64()),\n('trip_type', pa.int64()),\n('congestion_surcharge', pa.float64()),\n('lpep_pickup_month', pa.int64())\n])\ntable = pa.Table.from_pandas(data, schema=schema)",
        "question": "What environment variable needs to be set for authentication when accessing Google Cloud services in the provided code?",
        "section": "Module 3: Data Warehousing",
        "document_id": 214,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Reference:\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage\nSolution:\nfrom google.cloud import bigquery\n# Set table_id to the ID of the table to create\ntable_id = f\"{project_id}.{dataset_name}.{table_name}\"\n# Construct a BigQuery client object\nclient = bigquery.Client()\n# Set the external source format of your table\nexternal_source_format = \"PARQUET\"\n# Set the source_uris to point to your data in Google Cloud\nsource_uris = [ f'gs://{bucket_name}/{object_key}/*']\n# Create ExternalConfig object with external source format\nexternal_config = bigquery.ExternalConfig(external_source_format)\n# Set source_uris that point to your data in Google Cloud\nexternal_config.source_uris = source_uris\nexternal_config.autodetect = True\ntable = bigquery.Table(table_id)\n# Set the external data configuration of the table\ntable.external_data_configuration = external_config\ntable = client.create_table(table)  # Make an API request.\nprint(f'Created table with external source: {table_id}')\nprint(f'Format: {table.external_data_configuration.source_format}')",
        "question": "What is the purpose of the 'table_id' variable in the code?",
        "section": "Module 3: Data Warehousing",
        "document_id": 215,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Reference:\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage\nSolution:\nfrom google.cloud import bigquery\n# Set table_id to the ID of the table to create\ntable_id = f\"{project_id}.{dataset_name}.{table_name}\"\n# Construct a BigQuery client object\nclient = bigquery.Client()\n# Set the external source format of your table\nexternal_source_format = \"PARQUET\"\n# Set the source_uris to point to your data in Google Cloud\nsource_uris = [ f'gs://{bucket_name}/{object_key}/*']\n# Create ExternalConfig object with external source format\nexternal_config = bigquery.ExternalConfig(external_source_format)\n# Set source_uris that point to your data in Google Cloud\nexternal_config.source_uris = source_uris\nexternal_config.autodetect = True\ntable = bigquery.Table(table_id)\n# Set the external data configuration of the table\ntable.external_data_configuration = external_config\ntable = client.create_table(table)  # Make an API request.\nprint(f'Created table with external source: {table_id}')\nprint(f'Format: {table.external_data_configuration.source_format}')",
        "question": "Which external source format is being used in the provided solution?",
        "section": "Module 3: Data Warehousing",
        "document_id": 215,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Reference:\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage\nSolution:\nfrom google.cloud import bigquery\n# Set table_id to the ID of the table to create\ntable_id = f\"{project_id}.{dataset_name}.{table_name}\"\n# Construct a BigQuery client object\nclient = bigquery.Client()\n# Set the external source format of your table\nexternal_source_format = \"PARQUET\"\n# Set the source_uris to point to your data in Google Cloud\nsource_uris = [ f'gs://{bucket_name}/{object_key}/*']\n# Create ExternalConfig object with external source format\nexternal_config = bigquery.ExternalConfig(external_source_format)\n# Set source_uris that point to your data in Google Cloud\nexternal_config.source_uris = source_uris\nexternal_config.autodetect = True\ntable = bigquery.Table(table_id)\n# Set the external data configuration of the table\ntable.external_data_configuration = external_config\ntable = client.create_table(table)  # Make an API request.\nprint(f'Created table with external source: {table_id}')\nprint(f'Format: {table.external_data_configuration.source_format}')",
        "question": "How does the code connect to Google Cloud BigQuery?",
        "section": "Module 3: Data Warehousing",
        "document_id": 215,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Reference:\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage\nSolution:\nfrom google.cloud import bigquery\n# Set table_id to the ID of the table to create\ntable_id = f\"{project_id}.{dataset_name}.{table_name}\"\n# Construct a BigQuery client object\nclient = bigquery.Client()\n# Set the external source format of your table\nexternal_source_format = \"PARQUET\"\n# Set the source_uris to point to your data in Google Cloud\nsource_uris = [ f'gs://{bucket_name}/{object_key}/*']\n# Create ExternalConfig object with external source format\nexternal_config = bigquery.ExternalConfig(external_source_format)\n# Set source_uris that point to your data in Google Cloud\nexternal_config.source_uris = source_uris\nexternal_config.autodetect = True\ntable = bigquery.Table(table_id)\n# Set the external data configuration of the table\ntable.external_data_configuration = external_config\ntable = client.create_table(table)  # Make an API request.\nprint(f'Created table with external source: {table_id}')\nprint(f'Format: {table.external_data_configuration.source_format}')",
        "question": "What does the 'external_config.autodetect' property do in the context of the code?",
        "section": "Module 3: Data Warehousing",
        "document_id": 215,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Reference:\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage\nSolution:\nfrom google.cloud import bigquery\n# Set table_id to the ID of the table to create\ntable_id = f\"{project_id}.{dataset_name}.{table_name}\"\n# Construct a BigQuery client object\nclient = bigquery.Client()\n# Set the external source format of your table\nexternal_source_format = \"PARQUET\"\n# Set the source_uris to point to your data in Google Cloud\nsource_uris = [ f'gs://{bucket_name}/{object_key}/*']\n# Create ExternalConfig object with external source format\nexternal_config = bigquery.ExternalConfig(external_source_format)\n# Set source_uris that point to your data in Google Cloud\nexternal_config.source_uris = source_uris\nexternal_config.autodetect = True\ntable = bigquery.Table(table_id)\n# Set the external data configuration of the table\ntable.external_data_configuration = external_config\ntable = client.create_table(table)  # Make an API request.\nprint(f'Created table with external source: {table_id}')\nprint(f'Format: {table.external_data_configuration.source_format}')",
        "question": "What output is generated when a table with an external source is successfully created?",
        "section": "Module 3: Data Warehousing",
        "document_id": 215,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Reference:\nhttps://stackoverflow.com/questions/60941726/can-bigquery-api-overwrite-existing-table-view-with-create-table-tables-inser\nSolution:\nCombine with “Create External Table using Python”, use it before “client.create_table” function.\ndef tableExists(tableID, client):\n\"\"\"\nCheck if a table already exists using the tableID.\nreturn : (Boolean)\n\"\"\"\ntry:\ntable = client.get_table(tableID)\nreturn True\nexcept Exception as e: # NotFound:\nreturn False",
        "question": "What is the purpose of the function 'tableExists' in the provided solution?",
        "section": "Module 3: Data Warehousing",
        "document_id": 216,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Reference:\nhttps://stackoverflow.com/questions/60941726/can-bigquery-api-overwrite-existing-table-view-with-create-table-tables-inser\nSolution:\nCombine with “Create External Table using Python”, use it before “client.create_table” function.\ndef tableExists(tableID, client):\n\"\"\"\nCheck if a table already exists using the tableID.\nreturn : (Boolean)\n\"\"\"\ntry:\ntable = client.get_table(tableID)\nreturn True\nexcept Exception as e: # NotFound:\nreturn False",
        "question": "How can you check if a table already exists in BigQuery using Python?",
        "section": "Module 3: Data Warehousing",
        "document_id": 216,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Reference:\nhttps://stackoverflow.com/questions/60941726/can-bigquery-api-overwrite-existing-table-view-with-create-table-tables-inser\nSolution:\nCombine with “Create External Table using Python”, use it before “client.create_table” function.\ndef tableExists(tableID, client):\n\"\"\"\nCheck if a table already exists using the tableID.\nreturn : (Boolean)\n\"\"\"\ntry:\ntable = client.get_table(tableID)\nreturn True\nexcept Exception as e: # NotFound:\nreturn False",
        "question": "What does the 'client.get_table' method do in the context of the 'tableExists' function?",
        "section": "Module 3: Data Warehousing",
        "document_id": 216,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Reference:\nhttps://stackoverflow.com/questions/60941726/can-bigquery-api-overwrite-existing-table-view-with-create-table-tables-inser\nSolution:\nCombine with “Create External Table using Python”, use it before “client.create_table” function.\ndef tableExists(tableID, client):\n\"\"\"\nCheck if a table already exists using the tableID.\nreturn : (Boolean)\n\"\"\"\ntry:\ntable = client.get_table(tableID)\nreturn True\nexcept Exception as e: # NotFound:\nreturn False",
        "question": "What type of value does the 'tableExists' function return?",
        "section": "Module 3: Data Warehousing",
        "document_id": 216,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Reference:\nhttps://stackoverflow.com/questions/60941726/can-bigquery-api-overwrite-existing-table-view-with-create-table-tables-inser\nSolution:\nCombine with “Create External Table using Python”, use it before “client.create_table” function.\ndef tableExists(tableID, client):\n\"\"\"\nCheck if a table already exists using the tableID.\nreturn : (Boolean)\n\"\"\"\ntry:\ntable = client.get_table(tableID)\nreturn True\nexcept Exception as e: # NotFound:\nreturn False",
        "question": "Which Stack Overflow question is referenced in the text?",
        "section": "Module 3: Data Warehousing",
        "document_id": 216,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To avoid this error you can upload data from Google Cloud Storage to BigQuery through BigQuery Cloud Shell using the command:\n$ bq load  --autodetect --allow_quoted_newlines --source_format=CSV dataset_name.table_name \"gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz\"",
        "question": "What command is used to upload data from Google Cloud Storage to BigQuery?",
        "section": "Module 3: Data Warehousing",
        "document_id": 217,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To avoid this error you can upload data from Google Cloud Storage to BigQuery through BigQuery Cloud Shell using the command:\n$ bq load  --autodetect --allow_quoted_newlines --source_format=CSV dataset_name.table_name \"gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz\"",
        "question": "What are the parameters used in the bq load command?",
        "section": "Module 3: Data Warehousing",
        "document_id": 217,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To avoid this error you can upload data from Google Cloud Storage to BigQuery through BigQuery Cloud Shell using the command:\n$ bq load  --autodetect --allow_quoted_newlines --source_format=CSV dataset_name.table_name \"gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz\"",
        "question": "What does the --autodetect option do in the bq load command?",
        "section": "Module 3: Data Warehousing",
        "document_id": 217,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To avoid this error you can upload data from Google Cloud Storage to BigQuery through BigQuery Cloud Shell using the command:\n$ bq load  --autodetect --allow_quoted_newlines --source_format=CSV dataset_name.table_name \"gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz\"",
        "question": "How does the source format affect the data loading process in BigQuery?",
        "section": "Module 3: Data Warehousing",
        "document_id": 217,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To avoid this error you can upload data from Google Cloud Storage to BigQuery through BigQuery Cloud Shell using the command:\n$ bq load  --autodetect --allow_quoted_newlines --source_format=CSV dataset_name.table_name \"gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz\"",
        "question": "What is the significance of using 'gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz' in the command?",
        "section": "Module 3: Data Warehousing",
        "document_id": 217,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution: This problem arises if your gcs and bigquery storage is in different regions.\nOne potential way to solve it:\nGo to your google cloud bucket and check the region in field named “Location”\nNow in bigquery, click on three dot icon near your project name and select create dataset.\nIn region filed choose the same regions as you saw in your google cloud bucket",
        "question": "What is the issue described in the solution?",
        "section": "Module 3: Data Warehousing",
        "document_id": 218,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution: This problem arises if your gcs and bigquery storage is in different regions.\nOne potential way to solve it:\nGo to your google cloud bucket and check the region in field named “Location”\nNow in bigquery, click on three dot icon near your project name and select create dataset.\nIn region filed choose the same regions as you saw in your google cloud bucket",
        "question": "How can you check the region of your Google Cloud bucket?",
        "section": "Module 3: Data Warehousing",
        "document_id": 218,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution: This problem arises if your gcs and bigquery storage is in different regions.\nOne potential way to solve it:\nGo to your google cloud bucket and check the region in field named “Location”\nNow in bigquery, click on three dot icon near your project name and select create dataset.\nIn region filed choose the same regions as you saw in your google cloud bucket",
        "question": "What steps should you take to create a dataset in BigQuery?",
        "section": "Module 3: Data Warehousing",
        "document_id": 218,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution: This problem arises if your gcs and bigquery storage is in different regions.\nOne potential way to solve it:\nGo to your google cloud bucket and check the region in field named “Location”\nNow in bigquery, click on three dot icon near your project name and select create dataset.\nIn region filed choose the same regions as you saw in your google cloud bucket",
        "question": "Why is it important to ensure that GCS and BigQuery are in the same region?",
        "section": "Module 3: Data Warehousing",
        "document_id": 218,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution: This problem arises if your gcs and bigquery storage is in different regions.\nOne potential way to solve it:\nGo to your google cloud bucket and check the region in field named “Location”\nNow in bigquery, click on three dot icon near your project name and select create dataset.\nIn region filed choose the same regions as you saw in your google cloud bucket",
        "question": "What icon do you need to click near your project name in BigQuery to create a dataset?",
        "section": "Module 3: Data Warehousing",
        "document_id": 218,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There are multiple benefits of using Cloud Functions to automate tasks in Google Cloud.\nUse below Cloud Function python script to load files directly to BigQuery. Use your project id, dataset id & table id as defined by you.\nimport tempfile\nimport requests\nimport logging\nfrom google.cloud import bigquery\ndef hello_world(request):\n# table_id = <project_id.dataset_id.table_id>\ntable_id = 'de-zoomcap-project.dezoomcamp.fhv-2019'\n# Create a new BigQuery client\nclient = bigquery.Client()\nfor month in range(4, 13):\n# Define the schema for the data in the CSV.gz files\nurl = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz'.format(month)\n# Download the CSV.gz file from Github\nresponse = requests.get(url)\n# Create new table if loading first month data else append\nwrite_disposition_string = \"WRITE_APPEND\" if month > 1 else \"WRITE_TRUNCATE\"\n# Defining LoadJobConfig with schema of table to prevent it from changing with every table\njob_config = bigquery.LoadJobConfig(\nschema=[\nbigquery.SchemaField(\"dispatching_base_num\", \"STRING\"),\nbigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\"),\nbigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\"),\nbigquery.SchemaField(\"PUlocationID\", \"STRING\"),\nbigquery.SchemaField(\"DOlocationID\", \"STRING\"),\nbigquery.SchemaField(\"SR_Flag\", \"STRING\"),\nbigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\n],\nskip_leading_rows=1,\nwrite_disposition=write_disposition_string,\nautodetect=True,\nsource_format=\"CSV\",\n)\n# Load the data into BigQuery\n# Create a temporary file to prevent the exception- AttributeError: 'bytes' object has no attribute 'tell'\"\nwith tempfile.NamedTemporaryFile() as f:\nf.write(response.content)\nf.seek(0)\njob = client.load_table_from_file(\nf,\ntable_id,\nlocation=\"US\",\njob_config=job_config,\n)\njob.result()\nlogging.info(\"Data for month %d successfully loaded into table %s.\", month, table_id)\nreturn 'Data loaded into table {}.'.format(table_id)",
        "question": "What is the purpose of the Cloud Function Python script mentioned in the text?",
        "section": "Module 3: Data Warehousing",
        "document_id": 219,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There are multiple benefits of using Cloud Functions to automate tasks in Google Cloud.\nUse below Cloud Function python script to load files directly to BigQuery. Use your project id, dataset id & table id as defined by you.\nimport tempfile\nimport requests\nimport logging\nfrom google.cloud import bigquery\ndef hello_world(request):\n# table_id = <project_id.dataset_id.table_id>\ntable_id = 'de-zoomcap-project.dezoomcamp.fhv-2019'\n# Create a new BigQuery client\nclient = bigquery.Client()\nfor month in range(4, 13):\n# Define the schema for the data in the CSV.gz files\nurl = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz'.format(month)\n# Download the CSV.gz file from Github\nresponse = requests.get(url)\n# Create new table if loading first month data else append\nwrite_disposition_string = \"WRITE_APPEND\" if month > 1 else \"WRITE_TRUNCATE\"\n# Defining LoadJobConfig with schema of table to prevent it from changing with every table\njob_config = bigquery.LoadJobConfig(\nschema=[\nbigquery.SchemaField(\"dispatching_base_num\", \"STRING\"),\nbigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\"),\nbigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\"),\nbigquery.SchemaField(\"PUlocationID\", \"STRING\"),\nbigquery.SchemaField(\"DOlocationID\", \"STRING\"),\nbigquery.SchemaField(\"SR_Flag\", \"STRING\"),\nbigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\n],\nskip_leading_rows=1,\nwrite_disposition=write_disposition_string,\nautodetect=True,\nsource_format=\"CSV\",\n)\n# Load the data into BigQuery\n# Create a temporary file to prevent the exception- AttributeError: 'bytes' object has no attribute 'tell'\"\nwith tempfile.NamedTemporaryFile() as f:\nf.write(response.content)\nf.seek(0)\njob = client.load_table_from_file(\nf,\ntable_id,\nlocation=\"US\",\njob_config=job_config,\n)\njob.result()\nlogging.info(\"Data for month %d successfully loaded into table %s.\", month, table_id)\nreturn 'Data loaded into table {}.'.format(table_id)",
        "question": "How does the script determine whether to create a new table or append data when loading files into BigQuery?",
        "section": "Module 3: Data Warehousing",
        "document_id": 219,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There are multiple benefits of using Cloud Functions to automate tasks in Google Cloud.\nUse below Cloud Function python script to load files directly to BigQuery. Use your project id, dataset id & table id as defined by you.\nimport tempfile\nimport requests\nimport logging\nfrom google.cloud import bigquery\ndef hello_world(request):\n# table_id = <project_id.dataset_id.table_id>\ntable_id = 'de-zoomcap-project.dezoomcamp.fhv-2019'\n# Create a new BigQuery client\nclient = bigquery.Client()\nfor month in range(4, 13):\n# Define the schema for the data in the CSV.gz files\nurl = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz'.format(month)\n# Download the CSV.gz file from Github\nresponse = requests.get(url)\n# Create new table if loading first month data else append\nwrite_disposition_string = \"WRITE_APPEND\" if month > 1 else \"WRITE_TRUNCATE\"\n# Defining LoadJobConfig with schema of table to prevent it from changing with every table\njob_config = bigquery.LoadJobConfig(\nschema=[\nbigquery.SchemaField(\"dispatching_base_num\", \"STRING\"),\nbigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\"),\nbigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\"),\nbigquery.SchemaField(\"PUlocationID\", \"STRING\"),\nbigquery.SchemaField(\"DOlocationID\", \"STRING\"),\nbigquery.SchemaField(\"SR_Flag\", \"STRING\"),\nbigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\n],\nskip_leading_rows=1,\nwrite_disposition=write_disposition_string,\nautodetect=True,\nsource_format=\"CSV\",\n)\n# Load the data into BigQuery\n# Create a temporary file to prevent the exception- AttributeError: 'bytes' object has no attribute 'tell'\"\nwith tempfile.NamedTemporaryFile() as f:\nf.write(response.content)\nf.seek(0)\njob = client.load_table_from_file(\nf,\ntable_id,\nlocation=\"US\",\njob_config=job_config,\n)\njob.result()\nlogging.info(\"Data for month %d successfully loaded into table %s.\", month, table_id)\nreturn 'Data loaded into table {}.'.format(table_id)",
        "question": "What schema fields are defined in the LoadJobConfig for the BigQuery table?",
        "section": "Module 3: Data Warehousing",
        "document_id": 219,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There are multiple benefits of using Cloud Functions to automate tasks in Google Cloud.\nUse below Cloud Function python script to load files directly to BigQuery. Use your project id, dataset id & table id as defined by you.\nimport tempfile\nimport requests\nimport logging\nfrom google.cloud import bigquery\ndef hello_world(request):\n# table_id = <project_id.dataset_id.table_id>\ntable_id = 'de-zoomcap-project.dezoomcamp.fhv-2019'\n# Create a new BigQuery client\nclient = bigquery.Client()\nfor month in range(4, 13):\n# Define the schema for the data in the CSV.gz files\nurl = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz'.format(month)\n# Download the CSV.gz file from Github\nresponse = requests.get(url)\n# Create new table if loading first month data else append\nwrite_disposition_string = \"WRITE_APPEND\" if month > 1 else \"WRITE_TRUNCATE\"\n# Defining LoadJobConfig with schema of table to prevent it from changing with every table\njob_config = bigquery.LoadJobConfig(\nschema=[\nbigquery.SchemaField(\"dispatching_base_num\", \"STRING\"),\nbigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\"),\nbigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\"),\nbigquery.SchemaField(\"PUlocationID\", \"STRING\"),\nbigquery.SchemaField(\"DOlocationID\", \"STRING\"),\nbigquery.SchemaField(\"SR_Flag\", \"STRING\"),\nbigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\n],\nskip_leading_rows=1,\nwrite_disposition=write_disposition_string,\nautodetect=True,\nsource_format=\"CSV\",\n)\n# Load the data into BigQuery\n# Create a temporary file to prevent the exception- AttributeError: 'bytes' object has no attribute 'tell'\"\nwith tempfile.NamedTemporaryFile() as f:\nf.write(response.content)\nf.seek(0)\njob = client.load_table_from_file(\nf,\ntable_id,\nlocation=\"US\",\njob_config=job_config,\n)\njob.result()\nlogging.info(\"Data for month %d successfully loaded into table %s.\", month, table_id)\nreturn 'Data loaded into table {}.'.format(table_id)",
        "question": "What URL pattern is used to download the CSV.gz files in the script?",
        "section": "Module 3: Data Warehousing",
        "document_id": 219,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There are multiple benefits of using Cloud Functions to automate tasks in Google Cloud.\nUse below Cloud Function python script to load files directly to BigQuery. Use your project id, dataset id & table id as defined by you.\nimport tempfile\nimport requests\nimport logging\nfrom google.cloud import bigquery\ndef hello_world(request):\n# table_id = <project_id.dataset_id.table_id>\ntable_id = 'de-zoomcap-project.dezoomcamp.fhv-2019'\n# Create a new BigQuery client\nclient = bigquery.Client()\nfor month in range(4, 13):\n# Define the schema for the data in the CSV.gz files\nurl = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz'.format(month)\n# Download the CSV.gz file from Github\nresponse = requests.get(url)\n# Create new table if loading first month data else append\nwrite_disposition_string = \"WRITE_APPEND\" if month > 1 else \"WRITE_TRUNCATE\"\n# Defining LoadJobConfig with schema of table to prevent it from changing with every table\njob_config = bigquery.LoadJobConfig(\nschema=[\nbigquery.SchemaField(\"dispatching_base_num\", \"STRING\"),\nbigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\"),\nbigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\"),\nbigquery.SchemaField(\"PUlocationID\", \"STRING\"),\nbigquery.SchemaField(\"DOlocationID\", \"STRING\"),\nbigquery.SchemaField(\"SR_Flag\", \"STRING\"),\nbigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\n],\nskip_leading_rows=1,\nwrite_disposition=write_disposition_string,\nautodetect=True,\nsource_format=\"CSV\",\n)\n# Load the data into BigQuery\n# Create a temporary file to prevent the exception- AttributeError: 'bytes' object has no attribute 'tell'\"\nwith tempfile.NamedTemporaryFile() as f:\nf.write(response.content)\nf.seek(0)\njob = client.load_table_from_file(\nf,\ntable_id,\nlocation=\"US\",\njob_config=job_config,\n)\njob.result()\nlogging.info(\"Data for month %d successfully loaded into table %s.\", month, table_id)\nreturn 'Data loaded into table {}.'.format(table_id)",
        "question": "What logging message is generated after successfully loading data for a month into the BigQuery table?",
        "section": "Module 3: Data Warehousing",
        "document_id": 219,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to uncheck cache preferences in query settings",
        "question": "What action needs to be taken regarding cache preferences?",
        "section": "Module 3: Data Warehousing",
        "document_id": 220,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to uncheck cache preferences in query settings",
        "question": "Where can you find the cache preferences settings?",
        "section": "Module 3: Data Warehousing",
        "document_id": 220,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to uncheck cache preferences in query settings",
        "question": "Why might you need to uncheck cache preferences?",
        "section": "Module 3: Data Warehousing",
        "document_id": 220,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to uncheck cache preferences in query settings",
        "question": "What are query settings in relation to cache preferences?",
        "section": "Module 3: Data Warehousing",
        "document_id": 220,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to uncheck cache preferences in query settings",
        "question": "What effect does unchecking cache preferences have on queries?",
        "section": "Module 3: Data Warehousing",
        "document_id": 220,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: When you inject data into GCS using Pandas, there is a chance that some dataset has missing values on  DOlocationID and PUlocationID. Pandas by default will cast these columns as float data type, causing inconsistent data type between parquet in GCS and schema defined in big query. You will see something like this:\nSolution:\nFix the data type issue in data pipeline\nBefore injecting data into GCS, use astype and Int64 (which is different from int64 and accept both missing value and integer exist in the column) to cast the columns.\nSomething like:\ndf[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\ndf[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\nNOTE: It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery",
        "question": "What issue arises when injecting data into GCS using Pandas regarding DOlocationID and PUlocationID?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 221,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: When you inject data into GCS using Pandas, there is a chance that some dataset has missing values on  DOlocationID and PUlocationID. Pandas by default will cast these columns as float data type, causing inconsistent data type between parquet in GCS and schema defined in big query. You will see something like this:\nSolution:\nFix the data type issue in data pipeline\nBefore injecting data into GCS, use astype and Int64 (which is different from int64 and accept both missing value and integer exist in the column) to cast the columns.\nSomething like:\ndf[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\ndf[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\nNOTE: It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery",
        "question": "Why does Pandas cast DOlocationID and PUlocationID as float data types by default?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 221,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: When you inject data into GCS using Pandas, there is a chance that some dataset has missing values on  DOlocationID and PUlocationID. Pandas by default will cast these columns as float data type, causing inconsistent data type between parquet in GCS and schema defined in big query. You will see something like this:\nSolution:\nFix the data type issue in data pipeline\nBefore injecting data into GCS, use astype and Int64 (which is different from int64 and accept both missing value and integer exist in the column) to cast the columns.\nSomething like:\ndf[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\ndf[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\nNOTE: It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery",
        "question": "What is the recommended solution to fix the data type issue in the data pipeline?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 221,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: When you inject data into GCS using Pandas, there is a chance that some dataset has missing values on  DOlocationID and PUlocationID. Pandas by default will cast these columns as float data type, causing inconsistent data type between parquet in GCS and schema defined in big query. You will see something like this:\nSolution:\nFix the data type issue in data pipeline\nBefore injecting data into GCS, use astype and Int64 (which is different from int64 and accept both missing value and integer exist in the column) to cast the columns.\nSomething like:\ndf[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\ndf[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\nNOTE: It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery",
        "question": "How can you cast the columns DOlocationID and PUlocationID to the Int64 data type using Pandas?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 221,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: When you inject data into GCS using Pandas, there is a chance that some dataset has missing values on  DOlocationID and PUlocationID. Pandas by default will cast these columns as float data type, causing inconsistent data type between parquet in GCS and schema defined in big query. You will see something like this:\nSolution:\nFix the data type issue in data pipeline\nBefore injecting data into GCS, use astype and Int64 (which is different from int64 and accept both missing value and integer exist in the column) to cast the columns.\nSomething like:\ndf[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\ndf[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\nNOTE: It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery",
        "question": "What is the importance of defining the data type of all columns in the Transformation section of the ETL pipeline before loading to BigQuery?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 221,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem occurs when misplacing content after fro``m clause in BigQuery SQLs.\nCheck to remove any extra apaces or any other symbols, keep in lowercases, digits and dashes only",
        "question": "What mistakes can occur after the 'from' clause in BigQuery SQLs?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 222,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem occurs when misplacing content after fro``m clause in BigQuery SQLs.\nCheck to remove any extra apaces or any other symbols, keep in lowercases, digits and dashes only",
        "question": "What should you check for to avoid issues in BigQuery SQLs?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 222,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem occurs when misplacing content after fro``m clause in BigQuery SQLs.\nCheck to remove any extra apaces or any other symbols, keep in lowercases, digits and dashes only",
        "question": "Which characters should be retained when cleaning up SQL content?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 222,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem occurs when misplacing content after fro``m clause in BigQuery SQLs.\nCheck to remove any extra apaces or any other symbols, keep in lowercases, digits and dashes only",
        "question": "Why is it important to keep SQL content in lowercases, digits, and dashes only?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 222,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem occurs when misplacing content after fro``m clause in BigQuery SQLs.\nCheck to remove any extra apaces or any other symbols, keep in lowercases, digits and dashes only",
        "question": "How can extra spaces or symbols affect BigQuery SQL queries?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 222,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No. Based on the documentation for Bigquery, it does not support more than 1 column to be partitioned.\n[source]",
        "question": "What does the documentation for Bigquery state about column partitioning?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 223,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No. Based on the documentation for Bigquery, it does not support more than 1 column to be partitioned.\n[source]",
        "question": "Is it possible to partition multiple columns in Bigquery?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 223,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No. Based on the documentation for Bigquery, it does not support more than 1 column to be partitioned.\n[source]",
        "question": "How many columns can be partitioned in Bigquery according to the documentation?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 223,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No. Based on the documentation for Bigquery, it does not support more than 1 column to be partitioned.\n[source]",
        "question": "What limitations are mentioned regarding column partitioning in Bigquery?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 223,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "No. Based on the documentation for Bigquery, it does not support more than 1 column to be partitioned.\n[source]",
        "question": "Where can one find information about Bigquery's partitioning capabilities?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 223,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error Message:\nPARTITION BY expression must be DATE(<timestamp_column>), DATE(<datetime_column>), DATETIME_TRUNC(<datetime_column>, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(<timestamp_column>, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, MONTH/YEAR), or RANGE_BUCKET(<int64_column>, GENERATE_ARRAY(<int64_value>, <int64_value>[, <int64_value>]))\nSolution:\nConvert the column to datetime first.\ndf[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\ndf[\"dropOff_datetime\"] = pd.to_datetime(df[\"dropOff_datetime\"])",
        "question": "What type of expressions are allowed in the PARTITION BY clause?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 224,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error Message:\nPARTITION BY expression must be DATE(<timestamp_column>), DATE(<datetime_column>), DATETIME_TRUNC(<datetime_column>, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(<timestamp_column>, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, MONTH/YEAR), or RANGE_BUCKET(<int64_column>, GENERATE_ARRAY(<int64_value>, <int64_value>[, <int64_value>]))\nSolution:\nConvert the column to datetime first.\ndf[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\ndf[\"dropOff_datetime\"] = pd.to_datetime(df[\"dropOff_datetime\"])",
        "question": "What should the columns be converted to in order to resolve the error message?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 224,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error Message:\nPARTITION BY expression must be DATE(<timestamp_column>), DATE(<datetime_column>), DATETIME_TRUNC(<datetime_column>, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(<timestamp_column>, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, MONTH/YEAR), or RANGE_BUCKET(<int64_column>, GENERATE_ARRAY(<int64_value>, <int64_value>[, <int64_value>]))\nSolution:\nConvert the column to datetime first.\ndf[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\ndf[\"dropOff_datetime\"] = pd.to_datetime(df[\"dropOff_datetime\"])",
        "question": "How do you convert a column to datetime in pandas?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 224,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error Message:\nPARTITION BY expression must be DATE(<timestamp_column>), DATE(<datetime_column>), DATETIME_TRUNC(<datetime_column>, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(<timestamp_column>, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, MONTH/YEAR), or RANGE_BUCKET(<int64_column>, GENERATE_ARRAY(<int64_value>, <int64_value>[, <int64_value>]))\nSolution:\nConvert the column to datetime first.\ndf[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\ndf[\"dropOff_datetime\"] = pd.to_datetime(df[\"dropOff_datetime\"])",
        "question": "What is the purpose of the DATE_TRUNC function mentioned in the error message?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 224,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error Message:\nPARTITION BY expression must be DATE(<timestamp_column>), DATE(<datetime_column>), DATETIME_TRUNC(<datetime_column>, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(<timestamp_column>, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, MONTH/YEAR), or RANGE_BUCKET(<int64_column>, GENERATE_ARRAY(<int64_value>, <int64_value>[, <int64_value>]))\nSolution:\nConvert the column to datetime first.\ndf[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\ndf[\"dropOff_datetime\"] = pd.to_datetime(df[\"dropOff_datetime\"])",
        "question": "Which pandas function is used to convert a column to datetime format?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 224,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Native tables are tables where the data is stored in BigQuery.  External tables store the data outside BigQuery, with BigQuery storing metadata about that external table.\nResources:\nhttps://cloud.google.com/bigquery/docs/external-tables\nhttps://cloud.google.com/bigquery/docs/tables-intro",
        "question": "What are native tables in BigQuery?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 225,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Native tables are tables where the data is stored in BigQuery.  External tables store the data outside BigQuery, with BigQuery storing metadata about that external table.\nResources:\nhttps://cloud.google.com/bigquery/docs/external-tables\nhttps://cloud.google.com/bigquery/docs/tables-intro",
        "question": "How do external tables differ from native tables in BigQuery?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 225,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Native tables are tables where the data is stored in BigQuery.  External tables store the data outside BigQuery, with BigQuery storing metadata about that external table.\nResources:\nhttps://cloud.google.com/bigquery/docs/external-tables\nhttps://cloud.google.com/bigquery/docs/tables-intro",
        "question": "What type of data do external tables store?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 225,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Native tables are tables where the data is stored in BigQuery.  External tables store the data outside BigQuery, with BigQuery storing metadata about that external table.\nResources:\nhttps://cloud.google.com/bigquery/docs/external-tables\nhttps://cloud.google.com/bigquery/docs/tables-intro",
        "question": "Where is the metadata for external tables stored?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 225,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Native tables are tables where the data is stored in BigQuery.  External tables store the data outside BigQuery, with BigQuery storing metadata about that external table.\nResources:\nhttps://cloud.google.com/bigquery/docs/external-tables\nhttps://cloud.google.com/bigquery/docs/tables-intro",
        "question": "Can you give an example of where external tables might store their data?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 225,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To solve this error mention the location = US when creating the dim_zones table\n{{ config(\nmaterialized='table',\nlocation='US'\n) }}\nJust Update this part to solve the issue and run the dim_zones again and then run the fact_trips",
        "question": "What location should be mentioned when creating the dim_zones table to solve the error?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 227,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To solve this error mention the location = US when creating the dim_zones table\n{{ config(\nmaterialized='table',\nlocation='US'\n) }}\nJust Update this part to solve the issue and run the dim_zones again and then run the fact_trips",
        "question": "What does the config statement for dim_zones include?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 227,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To solve this error mention the location = US when creating the dim_zones table\n{{ config(\nmaterialized='table',\nlocation='US'\n) }}\nJust Update this part to solve the issue and run the dim_zones again and then run the fact_trips",
        "question": "What should be updated to resolve the issue before running dim_zones again?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 227,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To solve this error mention the location = US when creating the dim_zones table\n{{ config(\nmaterialized='table',\nlocation='US'\n) }}\nJust Update this part to solve the issue and run the dim_zones again and then run the fact_trips",
        "question": "What is the next step after updating the dim_zones table?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 227,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To solve this error mention the location = US when creating the dim_zones table\n{{ config(\nmaterialized='table',\nlocation='US'\n) }}\nJust Update this part to solve the issue and run the dim_zones again and then run the fact_trips",
        "question": "What materialization method is used for the dim_zones table?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 227,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution: proceed with setting up serving_dir on your computer as in the extract_model.md file. Then instead of\ndocker pull tensorflow/serving\nuse\ndocker pull emacski/tensorflow-serving\nThen\ndocker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=`pwd`/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t emacski/tensorflow-serving\nThen run the curl command as written, and you should get a prediction.",
        "question": "What is the purpose of setting up serving_dir on your computer?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 228,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution: proceed with setting up serving_dir on your computer as in the extract_model.md file. Then instead of\ndocker pull tensorflow/serving\nuse\ndocker pull emacski/tensorflow-serving\nThen\ndocker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=`pwd`/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t emacski/tensorflow-serving\nThen run the curl command as written, and you should get a prediction.",
        "question": "What command should be used instead of 'docker pull tensorflow/serving'?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 228,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution: proceed with setting up serving_dir on your computer as in the extract_model.md file. Then instead of\ndocker pull tensorflow/serving\nuse\ndocker pull emacski/tensorflow-serving\nThen\ndocker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=`pwd`/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t emacski/tensorflow-serving\nThen run the curl command as written, and you should get a prediction.",
        "question": "What parameters are used in the 'docker run' command?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 228,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution: proceed with setting up serving_dir on your computer as in the extract_model.md file. Then instead of\ndocker pull tensorflow/serving\nuse\ndocker pull emacski/tensorflow-serving\nThen\ndocker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=`pwd`/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t emacski/tensorflow-serving\nThen run the curl command as written, and you should get a prediction.",
        "question": "What does the '-e MODEL_NAME=tip_model' option in the 'docker run' command specify?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 228,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution: proceed with setting up serving_dir on your computer as in the extract_model.md file. Then instead of\ndocker pull tensorflow/serving\nuse\ndocker pull emacski/tensorflow-serving\nThen\ndocker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=`pwd`/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t emacski/tensorflow-serving\nThen run the curl command as written, and you should get a prediction.",
        "question": "What final step is mentioned to obtain a prediction after running the docker command?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 228,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Try deleting data you’ve saved to your VM locally during ETLs\nKill processes related to deleted files\nDownload ncdu and look for large files (pay particular attention to files related to Prefect)\nIf you delete any files related to Prefect, eliminate caching from your flow code",
        "question": "What should you try deleting during ETLs?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 229,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Try deleting data you’ve saved to your VM locally during ETLs\nKill processes related to deleted files\nDownload ncdu and look for large files (pay particular attention to files related to Prefect)\nIf you delete any files related to Prefect, eliminate caching from your flow code",
        "question": "What processes should be killed after deleting files?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 229,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Try deleting data you’ve saved to your VM locally during ETLs\nKill processes related to deleted files\nDownload ncdu and look for large files (pay particular attention to files related to Prefect)\nIf you delete any files related to Prefect, eliminate caching from your flow code",
        "question": "What tool can be downloaded to check for large files?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 229,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Try deleting data you’ve saved to your VM locally during ETLs\nKill processes related to deleted files\nDownload ncdu and look for large files (pay particular attention to files related to Prefect)\nIf you delete any files related to Prefect, eliminate caching from your flow code",
        "question": "What specific files should you pay attention to when identifying large files?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 229,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Try deleting data you’ve saved to your VM locally during ETLs\nKill processes related to deleted files\nDownload ncdu and look for large files (pay particular attention to files related to Prefect)\nIf you delete any files related to Prefect, eliminate caching from your flow code",
        "question": "What should be eliminated from your flow code if you delete files related to Prefect?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 229,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: What they mean is that they don't want you to do anything more than that. You should load the files into the bucket and create an external table based on those files (but nothing like cleaning the data and putting it in parquet format)",
        "question": "What should you do with the files once they are loaded into the bucket?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 230,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: What they mean is that they don't want you to do anything more than that. You should load the files into the bucket and create an external table based on those files (but nothing like cleaning the data and putting it in parquet format)",
        "question": "What type of table should you create after loading the files?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 230,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: What they mean is that they don't want you to do anything more than that. You should load the files into the bucket and create an external table based on those files (but nothing like cleaning the data and putting it in parquet format)",
        "question": "Is it necessary to clean the data before creating the external table?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 230,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: What they mean is that they don't want you to do anything more than that. You should load the files into the bucket and create an external table based on those files (but nothing like cleaning the data and putting it in parquet format)",
        "question": "What format should the data not be put into according to the text?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 230,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: What they mean is that they don't want you to do anything more than that. You should load the files into the bucket and create an external table based on those files (but nothing like cleaning the data and putting it in parquet format)",
        "question": "What does the instruction imply about additional actions required after loading the files?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 230,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If for whatever reason you try to read parquets directly from nyc.gov’s cloudfront into pandas, you might run into this error:\npyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds\nCause:\nthere is one errant data record where the dropOff_datetime was set to year 3019 instead of 2019.\npandas uses “timestamp[ns]” (as noted above), and int64 only allows a ~580 year range, centered on 2000. See `pd.Timestamp.max` and `pd.Timestamp.min`\nThis becomes out of bounds when pandas tries to read it because 3019 > 2300 (approx value of pd.Timestamp.Max\nFix:\nUse pyarrow to read it:\nimport pyarrow.parquet as pq df = pq.read_table('fhv_tripdata_2019-02.parquet').to_pandas(safe=False)\nHowever this results in weird timestamps for the offending record\nRead the datetime columns separately using pq.read_table\n\ntable = pq.read_table(‘taxi.parquet’)\ndatetimes = [‘list of datetime column names’]\ndf_dts = pd.DataFrame()\nfor col in datetimes:\ndf_dts[col] = pd.to_datetime(table .column(col), errors='coerce')\n\nThe `errors=’coerce’` parameter will convert the out of bounds timestamps into either the max or the min\nUse parquet.compute.filter to remove the offending rows\n\nimport pyarrow.compute as pc\ntable = pq.read_table(\"‘taxi.parquet\")\ndf = table.filter(\npc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\n).to_pandas()",
        "question": "What error might occur when reading parquets from nyc.gov’s cloudfront into pandas?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 231,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If for whatever reason you try to read parquets directly from nyc.gov’s cloudfront into pandas, you might run into this error:\npyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds\nCause:\nthere is one errant data record where the dropOff_datetime was set to year 3019 instead of 2019.\npandas uses “timestamp[ns]” (as noted above), and int64 only allows a ~580 year range, centered on 2000. See `pd.Timestamp.max` and `pd.Timestamp.min`\nThis becomes out of bounds when pandas tries to read it because 3019 > 2300 (approx value of pd.Timestamp.Max\nFix:\nUse pyarrow to read it:\nimport pyarrow.parquet as pq df = pq.read_table('fhv_tripdata_2019-02.parquet').to_pandas(safe=False)\nHowever this results in weird timestamps for the offending record\nRead the datetime columns separately using pq.read_table\n\ntable = pq.read_table(‘taxi.parquet’)\ndatetimes = [‘list of datetime column names’]\ndf_dts = pd.DataFrame()\nfor col in datetimes:\ndf_dts[col] = pd.to_datetime(table .column(col), errors='coerce')\n\nThe `errors=’coerce’` parameter will convert the out of bounds timestamps into either the max or the min\nUse parquet.compute.filter to remove the offending rows\n\nimport pyarrow.compute as pc\ntable = pq.read_table(\"‘taxi.parquet\")\ndf = table.filter(\npc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\n).to_pandas()",
        "question": "What is the cause of the 'ArrowInvalid' error mentioned in the text?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 231,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If for whatever reason you try to read parquets directly from nyc.gov’s cloudfront into pandas, you might run into this error:\npyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds\nCause:\nthere is one errant data record where the dropOff_datetime was set to year 3019 instead of 2019.\npandas uses “timestamp[ns]” (as noted above), and int64 only allows a ~580 year range, centered on 2000. See `pd.Timestamp.max` and `pd.Timestamp.min`\nThis becomes out of bounds when pandas tries to read it because 3019 > 2300 (approx value of pd.Timestamp.Max\nFix:\nUse pyarrow to read it:\nimport pyarrow.parquet as pq df = pq.read_table('fhv_tripdata_2019-02.parquet').to_pandas(safe=False)\nHowever this results in weird timestamps for the offending record\nRead the datetime columns separately using pq.read_table\n\ntable = pq.read_table(‘taxi.parquet’)\ndatetimes = [‘list of datetime column names’]\ndf_dts = pd.DataFrame()\nfor col in datetimes:\ndf_dts[col] = pd.to_datetime(table .column(col), errors='coerce')\n\nThe `errors=’coerce’` parameter will convert the out of bounds timestamps into either the max or the min\nUse parquet.compute.filter to remove the offending rows\n\nimport pyarrow.compute as pc\ntable = pq.read_table(\"‘taxi.parquet\")\ndf = table.filter(\npc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\n).to_pandas()",
        "question": "How can pyarrow be used to read parquet files without running into the mentioned error?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 231,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If for whatever reason you try to read parquets directly from nyc.gov’s cloudfront into pandas, you might run into this error:\npyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds\nCause:\nthere is one errant data record where the dropOff_datetime was set to year 3019 instead of 2019.\npandas uses “timestamp[ns]” (as noted above), and int64 only allows a ~580 year range, centered on 2000. See `pd.Timestamp.max` and `pd.Timestamp.min`\nThis becomes out of bounds when pandas tries to read it because 3019 > 2300 (approx value of pd.Timestamp.Max\nFix:\nUse pyarrow to read it:\nimport pyarrow.parquet as pq df = pq.read_table('fhv_tripdata_2019-02.parquet').to_pandas(safe=False)\nHowever this results in weird timestamps for the offending record\nRead the datetime columns separately using pq.read_table\n\ntable = pq.read_table(‘taxi.parquet’)\ndatetimes = [‘list of datetime column names’]\ndf_dts = pd.DataFrame()\nfor col in datetimes:\ndf_dts[col] = pd.to_datetime(table .column(col), errors='coerce')\n\nThe `errors=’coerce’` parameter will convert the out of bounds timestamps into either the max or the min\nUse parquet.compute.filter to remove the offending rows\n\nimport pyarrow.compute as pc\ntable = pq.read_table(\"‘taxi.parquet\")\ndf = table.filter(\npc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\n).to_pandas()",
        "question": "What does the 'errors=’coerce’' parameter do when converting timestamps in pandas?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 231,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If for whatever reason you try to read parquets directly from nyc.gov’s cloudfront into pandas, you might run into this error:\npyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds\nCause:\nthere is one errant data record where the dropOff_datetime was set to year 3019 instead of 2019.\npandas uses “timestamp[ns]” (as noted above), and int64 only allows a ~580 year range, centered on 2000. See `pd.Timestamp.max` and `pd.Timestamp.min`\nThis becomes out of bounds when pandas tries to read it because 3019 > 2300 (approx value of pd.Timestamp.Max\nFix:\nUse pyarrow to read it:\nimport pyarrow.parquet as pq df = pq.read_table('fhv_tripdata_2019-02.parquet').to_pandas(safe=False)\nHowever this results in weird timestamps for the offending record\nRead the datetime columns separately using pq.read_table\n\ntable = pq.read_table(‘taxi.parquet’)\ndatetimes = [‘list of datetime column names’]\ndf_dts = pd.DataFrame()\nfor col in datetimes:\ndf_dts[col] = pd.to_datetime(table .column(col), errors='coerce')\n\nThe `errors=’coerce’` parameter will convert the out of bounds timestamps into either the max or the min\nUse parquet.compute.filter to remove the offending rows\n\nimport pyarrow.compute as pc\ntable = pq.read_table(\"‘taxi.parquet\")\ndf = table.filter(\npc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\n).to_pandas()",
        "question": "How can you filter out offending rows in a parquet table using pyarrow?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 231,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Answer: The 2022 NYC taxi data parquet files are available for each month separately. Therefore, you need to add all 12 files to your GCS bucket and then refer to them using the URIs option when creating an external table in BigQuery. You can use the wildcard \"*\" to refer to all 12 files using a single string.",
        "question": "What format are the 2022 NYC taxi data files available in?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 232,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Answer: The 2022 NYC taxi data parquet files are available for each month separately. Therefore, you need to add all 12 files to your GCS bucket and then refer to them using the URIs option when creating an external table in BigQuery. You can use the wildcard \"*\" to refer to all 12 files using a single string.",
        "question": "How many separate files are there for the 2022 NYC taxi data?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 232,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Answer: The 2022 NYC taxi data parquet files are available for each month separately. Therefore, you need to add all 12 files to your GCS bucket and then refer to them using the URIs option when creating an external table in BigQuery. You can use the wildcard \"*\" to refer to all 12 files using a single string.",
        "question": "What needs to be done before creating an external table in BigQuery?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 232,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Answer: The 2022 NYC taxi data parquet files are available for each month separately. Therefore, you need to add all 12 files to your GCS bucket and then refer to them using the URIs option when creating an external table in BigQuery. You can use the wildcard \"*\" to refer to all 12 files using a single string.",
        "question": "What option can be used to refer to all 12 files in a single string?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 232,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Answer: The 2022 NYC taxi data parquet files are available for each month separately. Therefore, you need to add all 12 files to your GCS bucket and then refer to them using the URIs option when creating an external table in BigQuery. You can use the wildcard \"*\" to refer to all 12 files using a single string.",
        "question": "Where should the 2022 NYC taxi data files be uploaded?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 232,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This can help avoid schema issues in the homework. \nDownload files locally and use the ‘upload files’ button in GCS at the desired path. You can upload many files at once. You can also choose to upload a folder.",
        "question": "What are the benefits of avoiding schema issues in homework?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 233,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This can help avoid schema issues in the homework. \nDownload files locally and use the ‘upload files’ button in GCS at the desired path. You can upload many files at once. You can also choose to upload a folder.",
        "question": "How can you download files locally for use in GCS?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 233,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This can help avoid schema issues in the homework. \nDownload files locally and use the ‘upload files’ button in GCS at the desired path. You can upload many files at once. You can also choose to upload a folder.",
        "question": "Is it possible to upload multiple files at once in GCS?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 233,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This can help avoid schema issues in the homework. \nDownload files locally and use the ‘upload files’ button in GCS at the desired path. You can upload many files at once. You can also choose to upload a folder.",
        "question": "What option is available for uploading a folder in GCS?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 233,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This can help avoid schema issues in the homework. \nDownload files locally and use the ‘upload files’ button in GCS at the desired path. You can upload many files at once. You can also choose to upload a folder.",
        "question": "Where can you use the 'upload files' button in GCS?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 233,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: Take a careful look at the format of the dates in the question.",
        "question": "What should you focus on in the question?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 234,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: Take a careful look at the format of the dates in the question.",
        "question": "Why is it important to look at the format of the dates?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 234,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: Take a careful look at the format of the dates in the question.",
        "question": "How can the format of the dates affect the understanding of the question?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 234,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: Take a careful look at the format of the dates in the question.",
        "question": "What might happen if you overlook the date format?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 234,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: Take a careful look at the format of the dates in the question.",
        "question": "Can you provide examples of different date formats?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 234,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Many people aren’t getting an exact match, but are very close to one of the options. As per Alexey said to choose the closest option.",
        "question": "What should people do if they aren't getting an exact match?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 235,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Many people aren’t getting an exact match, but are very close to one of the options. As per Alexey said to choose the closest option.",
        "question": "Who suggested choosing the closest option?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 235,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Many people aren’t getting an exact match, but are very close to one of the options. As per Alexey said to choose the closest option.",
        "question": "Are most people finding exact matches or close options?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 235,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Many people aren’t getting an exact match, but are very close to one of the options. As per Alexey said to choose the closest option.",
        "question": "What does Alexey recommend regarding selection?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 235,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Many people aren’t getting an exact match, but are very close to one of the options. As per Alexey said to choose the closest option.",
        "question": "Is it common for people to find only approximate matches?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 235,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 41721: invalid start byte\nSolution:\nStep 1: When reading the data from the web into the pandas dataframe mention the encoding as follows:\npd.read_csv(dataset_url, low_memory=False, encoding='latin1')\nStep 2: When writing the dataframe from the local system to GCS as a csv mention the encoding as follows:\ndf.to_csv(path_on_gsc, compression=\"gzip\", encoding='utf-8')\nAlternative: use pd.read_parquet(url)",
        "question": "What error is indicated by the message 'UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 41721'?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 236,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 41721: invalid start byte\nSolution:\nStep 1: When reading the data from the web into the pandas dataframe mention the encoding as follows:\npd.read_csv(dataset_url, low_memory=False, encoding='latin1')\nStep 2: When writing the dataframe from the local system to GCS as a csv mention the encoding as follows:\ndf.to_csv(path_on_gsc, compression=\"gzip\", encoding='utf-8')\nAlternative: use pd.read_parquet(url)",
        "question": "What encoding should be specified when reading data from the web into a pandas dataframe to avoid the UnicodeDecodeError?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 236,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 41721: invalid start byte\nSolution:\nStep 1: When reading the data from the web into the pandas dataframe mention the encoding as follows:\npd.read_csv(dataset_url, low_memory=False, encoding='latin1')\nStep 2: When writing the dataframe from the local system to GCS as a csv mention the encoding as follows:\ndf.to_csv(path_on_gsc, compression=\"gzip\", encoding='utf-8')\nAlternative: use pd.read_parquet(url)",
        "question": "How can you write a pandas dataframe to Google Cloud Storage (GCS) as a CSV while specifying the correct encoding?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 236,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 41721: invalid start byte\nSolution:\nStep 1: When reading the data from the web into the pandas dataframe mention the encoding as follows:\npd.read_csv(dataset_url, low_memory=False, encoding='latin1')\nStep 2: When writing the dataframe from the local system to GCS as a csv mention the encoding as follows:\ndf.to_csv(path_on_gsc, compression=\"gzip\", encoding='utf-8')\nAlternative: use pd.read_parquet(url)",
        "question": "What alternative method is suggested for reading data instead of using pd.read_csv?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 236,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 41721: invalid start byte\nSolution:\nStep 1: When reading the data from the web into the pandas dataframe mention the encoding as follows:\npd.read_csv(dataset_url, low_memory=False, encoding='latin1')\nStep 2: When writing the dataframe from the local system to GCS as a csv mention the encoding as follows:\ndf.to_csv(path_on_gsc, compression=\"gzip\", encoding='utf-8')\nAlternative: use pd.read_parquet(url)",
        "question": "What parameter can be adjusted in the pd.read_csv function to improve memory handling?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 236,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "A generator is a function in python that returns an iterator using the yield keyword.\nA generator is a special type of iterable, similar to a list or a tuple, but with a crucial difference. Instead of creating and storing all the values in memory at once, a generator generates values on-the-fly as you iterate over it. This makes generators memory-efficient, particularly when dealing with large datasets.",
        "question": "What is a generator in Python?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 237,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "A generator is a function in python that returns an iterator using the yield keyword.\nA generator is a special type of iterable, similar to a list or a tuple, but with a crucial difference. Instead of creating and storing all the values in memory at once, a generator generates values on-the-fly as you iterate over it. This makes generators memory-efficient, particularly when dealing with large datasets.",
        "question": "How does a generator differ from a list or a tuple?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 237,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "A generator is a function in python that returns an iterator using the yield keyword.\nA generator is a special type of iterable, similar to a list or a tuple, but with a crucial difference. Instead of creating and storing all the values in memory at once, a generator generates values on-the-fly as you iterate over it. This makes generators memory-efficient, particularly when dealing with large datasets.",
        "question": "What keyword is used to define a generator function?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 237,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "A generator is a function in python that returns an iterator using the yield keyword.\nA generator is a special type of iterable, similar to a list or a tuple, but with a crucial difference. Instead of creating and storing all the values in memory at once, a generator generates values on-the-fly as you iterate over it. This makes generators memory-efficient, particularly when dealing with large datasets.",
        "question": "Why are generators considered memory-efficient?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 237,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "A generator is a function in python that returns an iterator using the yield keyword.\nA generator is a special type of iterable, similar to a list or a tuple, but with a crucial difference. Instead of creating and storing all the values in memory at once, a generator generates values on-the-fly as you iterate over it. This makes generators memory-efficient, particularly when dealing with large datasets.",
        "question": "How do generators produce values during iteration?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 237,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The read_parquet function supports a list of files as an argument. The list of files will be merged into a single result table.",
        "question": "What does the read_parquet function support as an argument?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 238,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The read_parquet function supports a list of files as an argument. The list of files will be merged into a single result table.",
        "question": "What happens to the list of files when passed to the read_parquet function?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 238,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The read_parquet function supports a list of files as an argument. The list of files will be merged into a single result table.",
        "question": "Can multiple files be processed by the read_parquet function at once?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 238,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The read_parquet function supports a list of files as an argument. The list of files will be merged into a single result table.",
        "question": "What is the result of merging files using the read_parquet function?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 238,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The read_parquet function supports a list of files as an argument. The list of files will be merged into a single result table.",
        "question": "Is the output of the read_parquet function a singular or multiple table structure?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 238,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Incorrect:\ndf['DOlocationID'] = pd.to_numeric(df['DOlocationID'], downcast=integer) or\ndf['DOlocationID'] = df['DOlocationID'].astype(int)\nCorrect:\ndf['DOlocationID'] = df['DOlocationID'].astype('Int64')",
        "question": "What is the incorrect method for converting 'DOlocationID' to a numeric type?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 239,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Incorrect:\ndf['DOlocationID'] = pd.to_numeric(df['DOlocationID'], downcast=integer) or\ndf['DOlocationID'] = df['DOlocationID'].astype(int)\nCorrect:\ndf['DOlocationID'] = df['DOlocationID'].astype('Int64')",
        "question": "What function is used to convert 'DOlocationID' to an integer in the correct method?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 239,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Incorrect:\ndf['DOlocationID'] = pd.to_numeric(df['DOlocationID'], downcast=integer) or\ndf['DOlocationID'] = df['DOlocationID'].astype(int)\nCorrect:\ndf['DOlocationID'] = df['DOlocationID'].astype('Int64')",
        "question": "How does downcasting to integer differ from using 'Int64' in pandas?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 239,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Incorrect:\ndf['DOlocationID'] = pd.to_numeric(df['DOlocationID'], downcast=integer) or\ndf['DOlocationID'] = df['DOlocationID'].astype(int)\nCorrect:\ndf['DOlocationID'] = df['DOlocationID'].astype('Int64')",
        "question": "What will happen if you use 'pd.to_numeric' with 'downcast' incorrectly?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 239,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Incorrect:\ndf['DOlocationID'] = pd.to_numeric(df['DOlocationID'], downcast=integer) or\ndf['DOlocationID'] = df['DOlocationID'].astype(int)\nCorrect:\ndf['DOlocationID'] = df['DOlocationID'].astype('Int64')",
        "question": "Why is 'df['DOlocationID'].astype('Int64')' preferred over the other methods mentioned?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 239,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ValueError: Path /Users/kt/.prefect/storage/44ccce0813ed4f24ab2d3783de7a9c3a does not exist.\nRemove ```cache_key_fn=task_input_hash ``` as it’s in argument in your function & run your flow again.\nNote: catche key is beneficial if you happen to run the code multiple times, it won't repeat the process which you have finished running in the previous run.  That means, if you have this ```cache_key``` in your initial run, this might cause the error.",
        "question": "What error is indicated in the text?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 240,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ValueError: Path /Users/kt/.prefect/storage/44ccce0813ed4f24ab2d3783de7a9c3a does not exist.\nRemove ```cache_key_fn=task_input_hash ``` as it’s in argument in your function & run your flow again.\nNote: catche key is beneficial if you happen to run the code multiple times, it won't repeat the process which you have finished running in the previous run.  That means, if you have this ```cache_key``` in your initial run, this might cause the error.",
        "question": "What should be removed from the function to resolve the issue?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 240,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ValueError: Path /Users/kt/.prefect/storage/44ccce0813ed4f24ab2d3783de7a9c3a does not exist.\nRemove ```cache_key_fn=task_input_hash ``` as it’s in argument in your function & run your flow again.\nNote: catche key is beneficial if you happen to run the code multiple times, it won't repeat the process which you have finished running in the previous run.  That means, if you have this ```cache_key``` in your initial run, this might cause the error.",
        "question": "How does the cache key benefit repeated code execution?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 240,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ValueError: Path /Users/kt/.prefect/storage/44ccce0813ed4f24ab2d3783de7a9c3a does not exist.\nRemove ```cache_key_fn=task_input_hash ``` as it’s in argument in your function & run your flow again.\nNote: catche key is beneficial if you happen to run the code multiple times, it won't repeat the process which you have finished running in the previous run.  That means, if you have this ```cache_key``` in your initial run, this might cause the error.",
        "question": "What could cause the ValueError mentioned in the text?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 240,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ValueError: Path /Users/kt/.prefect/storage/44ccce0813ed4f24ab2d3783de7a9c3a does not exist.\nRemove ```cache_key_fn=task_input_hash ``` as it’s in argument in your function & run your flow again.\nNote: catche key is beneficial if you happen to run the code multiple times, it won't repeat the process which you have finished running in the previous run.  That means, if you have this ```cache_key``` in your initial run, this might cause the error.",
        "question": "What does the cache_key do in relation to previous runs of the code?",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "document_id": 240,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "@task\ndef download_file(url: str, file_path: str):\nresponse = requests.get(url)\nopen(file_path, \"wb\").write(response.content)\nreturn file_path\n@flow\ndef extract_from_web() -> None:\nfile_path = download_file(url=f'{url-filename}.csv.gz',file_path=f'{filename}.csv.gz')",
        "question": "What is the purpose of the download_file function?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 241,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "@task\ndef download_file(url: str, file_path: str):\nresponse = requests.get(url)\nopen(file_path, \"wb\").write(response.content)\nreturn file_path\n@flow\ndef extract_from_web() -> None:\nfile_path = download_file(url=f'{url-filename}.csv.gz',file_path=f'{filename}.csv.gz')",
        "question": "What parameters does the download_file function accept?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 241,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "@task\ndef download_file(url: str, file_path: str):\nresponse = requests.get(url)\nopen(file_path, \"wb\").write(response.content)\nreturn file_path\n@flow\ndef extract_from_web() -> None:\nfile_path = download_file(url=f'{url-filename}.csv.gz',file_path=f'{filename}.csv.gz')",
        "question": "What type of content does the download_file function write to a file?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 241,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "@task\ndef download_file(url: str, file_path: str):\nresponse = requests.get(url)\nopen(file_path, \"wb\").write(response.content)\nreturn file_path\n@flow\ndef extract_from_web() -> None:\nfile_path = download_file(url=f'{url-filename}.csv.gz',file_path=f'{filename}.csv.gz')",
        "question": "How is the file path determined in the extract_from_web function?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 241,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "@task\ndef download_file(url: str, file_path: str):\nresponse = requests.get(url)\nopen(file_path, \"wb\").write(response.content)\nreturn file_path\n@flow\ndef extract_from_web() -> None:\nfile_path = download_file(url=f'{url-filename}.csv.gz',file_path=f'{filename}.csv.gz')",
        "question": "What file format is being downloaded in the extract_from_web function?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 241,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Update the seed column types in the dbt_project.yaml file\nfor using double : float\nfor using int : numeric\nDBT Cloud production error: prod dataset not available in location EU\nProblem: I am trying to deploy my DBT  models to production, using DBT Cloud. The data should live in BigQuery.  The dataset location is EU.  However, when I am running the model in production, a prod dataset is being create in BigQuery with a location US and the dbt invoke build is failing giving me \"ERROR 404: porject.dataset:prod not available in location EU\". I tried different ways to fix this. I am not sure if there is a more simple solution then creating my project or buckets in location US. Hope anyone can help here.\nNote: Everything is working fine in development mode, the issue is just happening when scheduling and running job in production\nSolution: I created the prod dataset manually in BQ and specified EU, then I ran the job.",
        "question": "What changes need to be made to the seed column types in dbt_project.yaml?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 242,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Update the seed column types in the dbt_project.yaml file\nfor using double : float\nfor using int : numeric\nDBT Cloud production error: prod dataset not available in location EU\nProblem: I am trying to deploy my DBT  models to production, using DBT Cloud. The data should live in BigQuery.  The dataset location is EU.  However, when I am running the model in production, a prod dataset is being create in BigQuery with a location US and the dbt invoke build is failing giving me \"ERROR 404: porject.dataset:prod not available in location EU\". I tried different ways to fix this. I am not sure if there is a more simple solution then creating my project or buckets in location US. Hope anyone can help here.\nNote: Everything is working fine in development mode, the issue is just happening when scheduling and running job in production\nSolution: I created the prod dataset manually in BQ and specified EU, then I ran the job.",
        "question": "What error is encountered when deploying DBT models to production in DBT Cloud?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 242,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Update the seed column types in the dbt_project.yaml file\nfor using double : float\nfor using int : numeric\nDBT Cloud production error: prod dataset not available in location EU\nProblem: I am trying to deploy my DBT  models to production, using DBT Cloud. The data should live in BigQuery.  The dataset location is EU.  However, when I am running the model in production, a prod dataset is being create in BigQuery with a location US and the dbt invoke build is failing giving me \"ERROR 404: porject.dataset:prod not available in location EU\". I tried different ways to fix this. I am not sure if there is a more simple solution then creating my project or buckets in location US. Hope anyone can help here.\nNote: Everything is working fine in development mode, the issue is just happening when scheduling and running job in production\nSolution: I created the prod dataset manually in BQ and specified EU, then I ran the job.",
        "question": "Why is the prod dataset being created in the US location instead of EU?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 242,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Update the seed column types in the dbt_project.yaml file\nfor using double : float\nfor using int : numeric\nDBT Cloud production error: prod dataset not available in location EU\nProblem: I am trying to deploy my DBT  models to production, using DBT Cloud. The data should live in BigQuery.  The dataset location is EU.  However, when I am running the model in production, a prod dataset is being create in BigQuery with a location US and the dbt invoke build is failing giving me \"ERROR 404: porject.dataset:prod not available in location EU\". I tried different ways to fix this. I am not sure if there is a more simple solution then creating my project or buckets in location US. Hope anyone can help here.\nNote: Everything is working fine in development mode, the issue is just happening when scheduling and running job in production\nSolution: I created the prod dataset manually in BQ and specified EU, then I ran the job.",
        "question": "What was the manual solution implemented to address the dataset location issue?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 242,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Update the seed column types in the dbt_project.yaml file\nfor using double : float\nfor using int : numeric\nDBT Cloud production error: prod dataset not available in location EU\nProblem: I am trying to deploy my DBT  models to production, using DBT Cloud. The data should live in BigQuery.  The dataset location is EU.  However, when I am running the model in production, a prod dataset is being create in BigQuery with a location US and the dbt invoke build is failing giving me \"ERROR 404: porject.dataset:prod not available in location EU\". I tried different ways to fix this. I am not sure if there is a more simple solution then creating my project or buckets in location US. Hope anyone can help here.\nNote: Everything is working fine in development mode, the issue is just happening when scheduling and running job in production\nSolution: I created the prod dataset manually in BQ and specified EU, then I ran the job.",
        "question": "What is the difference in behavior between development mode and production mode in this context?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 242,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: This project does not have a development environment configured. Please create a development environment and configure your development credentials to use the dbt IDE.\nThe error itself tells us how to solve this issue, the guide is here. And from videos @1:42 and also slack chat",
        "question": "What does the error message indicate about the project's configuration?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 243,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: This project does not have a development environment configured. Please create a development environment and configure your development credentials to use the dbt IDE.\nThe error itself tells us how to solve this issue, the guide is here. And from videos @1:42 and also slack chat",
        "question": "How can one resolve the issue mentioned in the error message?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 243,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: This project does not have a development environment configured. Please create a development environment and configure your development credentials to use the dbt IDE.\nThe error itself tells us how to solve this issue, the guide is here. And from videos @1:42 and also slack chat",
        "question": "What resources are suggested for solving the development environment issue?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 243,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: This project does not have a development environment configured. Please create a development environment and configure your development credentials to use the dbt IDE.\nThe error itself tells us how to solve this issue, the guide is here. And from videos @1:42 and also slack chat",
        "question": "Where can one find the guide to address the configuration error?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 243,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: This project does not have a development environment configured. Please create a development environment and configure your development credentials to use the dbt IDE.\nThe error itself tells us how to solve this issue, the guide is here. And from videos @1:42 and also slack chat",
        "question": "What are the suggested mediums to seek help regarding the development environment issue?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 243,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Runtime Error\ndbt was unable to connect to the specified database.\nThe database returned the following error:\n>Database Error\nAccess Denied: Project <project_name>: User does not have bigquery.jobs.create permission in project <project_name>.\nCheck your database credentials and try again. For more information, visit:\nhttps://docs.getdbt.com/docs/configure-your-profile\nSteps to resolve error in Google Cloud:\n1. Navigate to IAM & Admin and select IAM\n2. Click Grant Access if your newly created dbt service account isn't listed\n3. In New principals field, add your service account\n4. Select a Role and search for BigQuery Job User to add\n5. Go back to dbt cloud project setup and Test your connection\n6. Note: Also add BigQuery Data Owner, Storage Object Admin, & Storage Admin to prevent permission issues later in the course",
        "question": "What does the error message indicate about the user's permissions?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 244,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Runtime Error\ndbt was unable to connect to the specified database.\nThe database returned the following error:\n>Database Error\nAccess Denied: Project <project_name>: User does not have bigquery.jobs.create permission in project <project_name>.\nCheck your database credentials and try again. For more information, visit:\nhttps://docs.getdbt.com/docs/configure-your-profile\nSteps to resolve error in Google Cloud:\n1. Navigate to IAM & Admin and select IAM\n2. Click Grant Access if your newly created dbt service account isn't listed\n3. In New principals field, add your service account\n4. Select a Role and search for BigQuery Job User to add\n5. Go back to dbt cloud project setup and Test your connection\n6. Note: Also add BigQuery Data Owner, Storage Object Admin, & Storage Admin to prevent permission issues later in the course",
        "question": "What steps should be taken to resolve the 'Access Denied' error in Google Cloud?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 244,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Runtime Error\ndbt was unable to connect to the specified database.\nThe database returned the following error:\n>Database Error\nAccess Denied: Project <project_name>: User does not have bigquery.jobs.create permission in project <project_name>.\nCheck your database credentials and try again. For more information, visit:\nhttps://docs.getdbt.com/docs/configure-your-profile\nSteps to resolve error in Google Cloud:\n1. Navigate to IAM & Admin and select IAM\n2. Click Grant Access if your newly created dbt service account isn't listed\n3. In New principals field, add your service account\n4. Select a Role and search for BigQuery Job User to add\n5. Go back to dbt cloud project setup and Test your connection\n6. Note: Also add BigQuery Data Owner, Storage Object Admin, & Storage Admin to prevent permission issues later in the course",
        "question": "Which role should be added to the service account to resolve the connection issue?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 244,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Runtime Error\ndbt was unable to connect to the specified database.\nThe database returned the following error:\n>Database Error\nAccess Denied: Project <project_name>: User does not have bigquery.jobs.create permission in project <project_name>.\nCheck your database credentials and try again. For more information, visit:\nhttps://docs.getdbt.com/docs/configure-your-profile\nSteps to resolve error in Google Cloud:\n1. Navigate to IAM & Admin and select IAM\n2. Click Grant Access if your newly created dbt service account isn't listed\n3. In New principals field, add your service account\n4. Select a Role and search for BigQuery Job User to add\n5. Go back to dbt cloud project setup and Test your connection\n6. Note: Also add BigQuery Data Owner, Storage Object Admin, & Storage Admin to prevent permission issues later in the course",
        "question": "What additional roles should be assigned to prevent future permission issues?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 244,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Runtime Error\ndbt was unable to connect to the specified database.\nThe database returned the following error:\n>Database Error\nAccess Denied: Project <project_name>: User does not have bigquery.jobs.create permission in project <project_name>.\nCheck your database credentials and try again. For more information, visit:\nhttps://docs.getdbt.com/docs/configure-your-profile\nSteps to resolve error in Google Cloud:\n1. Navigate to IAM & Admin and select IAM\n2. Click Grant Access if your newly created dbt service account isn't listed\n3. In New principals field, add your service account\n4. Select a Role and search for BigQuery Job User to add\n5. Go back to dbt cloud project setup and Test your connection\n6. Note: Also add BigQuery Data Owner, Storage Object Admin, & Storage Admin to prevent permission issues later in the course",
        "question": "Where can more information about configuring dbt profiles be found?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 244,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "error: This dbt Cloud run was cancelled because a valid dbt project was not found. Please check that the repository contains a proper dbt_project.yml config file. If your dbt project is located in a subdirectory of the connected repository, be sure to specify its location on the Project settings page in dbt Cloud",
        "question": "What causes a dbt Cloud run to be cancelled?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 245,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "error: This dbt Cloud run was cancelled because a valid dbt project was not found. Please check that the repository contains a proper dbt_project.yml config file. If your dbt project is located in a subdirectory of the connected repository, be sure to specify its location on the Project settings page in dbt Cloud",
        "question": "What is a necessary file for a valid dbt project?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 245,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "error: This dbt Cloud run was cancelled because a valid dbt project was not found. Please check that the repository contains a proper dbt_project.yml config file. If your dbt project is located in a subdirectory of the connected repository, be sure to specify its location on the Project settings page in dbt Cloud",
        "question": "Where should the dbt_project.yml config file be located?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 245,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "error: This dbt Cloud run was cancelled because a valid dbt project was not found. Please check that the repository contains a proper dbt_project.yml config file. If your dbt project is located in a subdirectory of the connected repository, be sure to specify its location on the Project settings page in dbt Cloud",
        "question": "How can you specify the location of a dbt project in dbt Cloud?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 245,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "error: This dbt Cloud run was cancelled because a valid dbt project was not found. Please check that the repository contains a proper dbt_project.yml config file. If your dbt project is located in a subdirectory of the connected repository, be sure to specify its location on the Project settings page in dbt Cloud",
        "question": "What should you do if your dbt project is in a subdirectory of the connected repository?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 245,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: Failed to clone repository.\ngit clone git@github.com:DataTalksClub/data-engineering-zoomcamp.git /usr/src/develop/…\nCloning into '/usr/src/develop/...\nWarning: Permanently added 'github.com,140.82.114.4' (ECDSA) to the list of known hosts.\ngit@github.com: Permission denied (publickey).\nfatal: Could not read from remote repository.\nIssue: You don’t have permissions to write to DataTalksClub/data-engineering-zoomcamp.git\nSolution 1: Clone the repository and use this forked repo, which contains your github username. Then, proceed to specify the path, as in:\n[your github username]/data-engineering-zoomcamp.git\nSolution 2: create a fresh repo for dbt-lessons. We’d need to do branching and PRs in this lesson, so it might be a good idea to also not mess up your whole other repo. Then you don’t have to create a subfolder for the dbt project files\nSolution 3: Use https link",
        "question": "What is the error message received when attempting to clone the repository?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 246,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: Failed to clone repository.\ngit clone git@github.com:DataTalksClub/data-engineering-zoomcamp.git /usr/src/develop/…\nCloning into '/usr/src/develop/...\nWarning: Permanently added 'github.com,140.82.114.4' (ECDSA) to the list of known hosts.\ngit@github.com: Permission denied (publickey).\nfatal: Could not read from remote repository.\nIssue: You don’t have permissions to write to DataTalksClub/data-engineering-zoomcamp.git\nSolution 1: Clone the repository and use this forked repo, which contains your github username. Then, proceed to specify the path, as in:\n[your github username]/data-engineering-zoomcamp.git\nSolution 2: create a fresh repo for dbt-lessons. We’d need to do branching and PRs in this lesson, so it might be a good idea to also not mess up your whole other repo. Then you don’t have to create a subfolder for the dbt project files\nSolution 3: Use https link",
        "question": "What does the warning about 'github.com' indicate in the context of cloning a repository?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 246,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: Failed to clone repository.\ngit clone git@github.com:DataTalksClub/data-engineering-zoomcamp.git /usr/src/develop/…\nCloning into '/usr/src/develop/...\nWarning: Permanently added 'github.com,140.82.114.4' (ECDSA) to the list of known hosts.\ngit@github.com: Permission denied (publickey).\nfatal: Could not read from remote repository.\nIssue: You don’t have permissions to write to DataTalksClub/data-engineering-zoomcamp.git\nSolution 1: Clone the repository and use this forked repo, which contains your github username. Then, proceed to specify the path, as in:\n[your github username]/data-engineering-zoomcamp.git\nSolution 2: create a fresh repo for dbt-lessons. We’d need to do branching and PRs in this lesson, so it might be a good idea to also not mess up your whole other repo. Then you don’t have to create a subfolder for the dbt project files\nSolution 3: Use https link",
        "question": "What does the failure to clone the repository suggest about user permissions?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 246,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: Failed to clone repository.\ngit clone git@github.com:DataTalksClub/data-engineering-zoomcamp.git /usr/src/develop/…\nCloning into '/usr/src/develop/...\nWarning: Permanently added 'github.com,140.82.114.4' (ECDSA) to the list of known hosts.\ngit@github.com: Permission denied (publickey).\nfatal: Could not read from remote repository.\nIssue: You don’t have permissions to write to DataTalksClub/data-engineering-zoomcamp.git\nSolution 1: Clone the repository and use this forked repo, which contains your github username. Then, proceed to specify the path, as in:\n[your github username]/data-engineering-zoomcamp.git\nSolution 2: create a fresh repo for dbt-lessons. We’d need to do branching and PRs in this lesson, so it might be a good idea to also not mess up your whole other repo. Then you don’t have to create a subfolder for the dbt project files\nSolution 3: Use https link",
        "question": "What are the three proposed solutions to the cloning error?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 246,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: Failed to clone repository.\ngit clone git@github.com:DataTalksClub/data-engineering-zoomcamp.git /usr/src/develop/…\nCloning into '/usr/src/develop/...\nWarning: Permanently added 'github.com,140.82.114.4' (ECDSA) to the list of known hosts.\ngit@github.com: Permission denied (publickey).\nfatal: Could not read from remote repository.\nIssue: You don’t have permissions to write to DataTalksClub/data-engineering-zoomcamp.git\nSolution 1: Clone the repository and use this forked repo, which contains your github username. Then, proceed to specify the path, as in:\n[your github username]/data-engineering-zoomcamp.git\nSolution 2: create a fresh repo for dbt-lessons. We’d need to do branching and PRs in this lesson, so it might be a good idea to also not mess up your whole other repo. Then you don’t have to create a subfolder for the dbt project files\nSolution 3: Use https link",
        "question": "How can you create a fresh repository for dbt-lessons according to the solutions provided?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 246,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution:\nCheck if you’re on the Developer Plan. As per the prerequisites, you'll need to be enrolled in the Team Plan or Enterprise Plan to set up a CI Job in dbt Cloud.\nSo If you're on the Developer Plan, you'll need to upgrade to utilise CI Jobs.\nNote from another user: I’m in the Team Plan (trial period) but the option is still disabled. What worked for me instead was this. It works for the Developer (free) plan.",
        "question": "What plans are required to set up a CI Job in dbt Cloud?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 247,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution:\nCheck if you’re on the Developer Plan. As per the prerequisites, you'll need to be enrolled in the Team Plan or Enterprise Plan to set up a CI Job in dbt Cloud.\nSo If you're on the Developer Plan, you'll need to upgrade to utilise CI Jobs.\nNote from another user: I’m in the Team Plan (trial period) but the option is still disabled. What worked for me instead was this. It works for the Developer (free) plan.",
        "question": "What should a user on the Developer Plan do to utilize CI Jobs?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 247,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution:\nCheck if you’re on the Developer Plan. As per the prerequisites, you'll need to be enrolled in the Team Plan or Enterprise Plan to set up a CI Job in dbt Cloud.\nSo If you're on the Developer Plan, you'll need to upgrade to utilise CI Jobs.\nNote from another user: I’m in the Team Plan (trial period) but the option is still disabled. What worked for me instead was this. It works for the Developer (free) plan.",
        "question": "Can a user on the Team Plan during the trial period access CI Jobs?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 247,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution:\nCheck if you’re on the Developer Plan. As per the prerequisites, you'll need to be enrolled in the Team Plan or Enterprise Plan to set up a CI Job in dbt Cloud.\nSo If you're on the Developer Plan, you'll need to upgrade to utilise CI Jobs.\nNote from another user: I’m in the Team Plan (trial period) but the option is still disabled. What worked for me instead was this. It works for the Developer (free) plan.",
        "question": "What did another user suggest when facing issues with CI Jobs on the Team Plan?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 247,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution:\nCheck if you’re on the Developer Plan. As per the prerequisites, you'll need to be enrolled in the Team Plan or Enterprise Plan to set up a CI Job in dbt Cloud.\nSo If you're on the Developer Plan, you'll need to upgrade to utilise CI Jobs.\nNote from another user: I’m in the Team Plan (trial period) but the option is still disabled. What worked for me instead was this. It works for the Developer (free) plan.",
        "question": "Is it possible to access CI Jobs on the Developer (free) plan?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 247,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue: If the DBT cloud IDE loading indefinitely then giving you this error\nSolution: check the dbt_cloud_setup.md  file and make a SSH Key and use gitclone to import repo into dbt project, copy and paste deploy key back in your repo setting.",
        "question": "What issue is addressed in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 248,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue: If the DBT cloud IDE loading indefinitely then giving you this error\nSolution: check the dbt_cloud_setup.md  file and make a SSH Key and use gitclone to import repo into dbt project, copy and paste deploy key back in your repo setting.",
        "question": "What should you check if the DBT cloud IDE is loading indefinitely?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 248,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue: If the DBT cloud IDE loading indefinitely then giving you this error\nSolution: check the dbt_cloud_setup.md  file and make a SSH Key and use gitclone to import repo into dbt project, copy and paste deploy key back in your repo setting.",
        "question": "What action should be taken after creating an SSH key?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 248,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue: If the DBT cloud IDE loading indefinitely then giving you this error\nSolution: check the dbt_cloud_setup.md  file and make a SSH Key and use gitclone to import repo into dbt project, copy and paste deploy key back in your repo setting.",
        "question": "What is the purpose of the deploy key mentioned in the solution?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 248,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue: If the DBT cloud IDE loading indefinitely then giving you this error\nSolution: check the dbt_cloud_setup.md  file and make a SSH Key and use gitclone to import repo into dbt project, copy and paste deploy key back in your repo setting.",
        "question": "Where should you copy and paste the deploy key?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 248,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue: If you don’t define the column format while converting from csv to parquet Python will “choose” based on the first rows.\n✅Solution: Defined the schema while running web_to_gcp.py pipeline.\nSebastian adapted the script:\nhttps://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\nNeed a quick change to make the file work with gz files, added the following lines (and don’t forget to delete the file at the end of each iteration of the loop to avoid any problem of disk space)\nfile_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\nopen(file_name_gz, 'wb').write(r.content)\nos.system(f\"gzip -d {file_name_gz}\")\nos.system(f\"rm {file_name_init}.*\")\nSame ERROR - When running dbt run for fact_trips.sql, the task failed with error:\n“Parquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64”\n开启屏幕阅读器支持\n要启用屏幕阅读器支持，请按Ctrl+Alt+Z。要了解键盘快捷键，请按Ctrl+斜杠。\n查找和替换\nReason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.\nThere are some possible fixes:\nDrop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\nSELECT * EXCEPT (ehail_fee) FROM…\nModify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\nModify Airflow dag to make the conversion and avoid the error.\npv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {'ehail_fee': 'float64'}))\nSame type of ERROR - parquet files with different data types - Fix it with pandas\nHere is another possibility that could be interesting:\nYou can specify the dtypes when importing the file from csv to a dataframe with pandas\npd.from_csv(..., dtype=type_dict)\nOne obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital ‘I’. The type_dict is a python dictionary mapping the column names to the dtypes.\nSources:\nhttps://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\nNullable integer data type — pandas 1.5.3 documentation",
        "question": "What happens if you don't define the column format while converting from CSV to Parquet in Python?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 249,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue: If you don’t define the column format while converting from csv to parquet Python will “choose” based on the first rows.\n✅Solution: Defined the schema while running web_to_gcp.py pipeline.\nSebastian adapted the script:\nhttps://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\nNeed a quick change to make the file work with gz files, added the following lines (and don’t forget to delete the file at the end of each iteration of the loop to avoid any problem of disk space)\nfile_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\nopen(file_name_gz, 'wb').write(r.content)\nos.system(f\"gzip -d {file_name_gz}\")\nos.system(f\"rm {file_name_init}.*\")\nSame ERROR - When running dbt run for fact_trips.sql, the task failed with error:\n“Parquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64”\n开启屏幕阅读器支持\n要启用屏幕阅读器支持，请按Ctrl+Alt+Z。要了解键盘快捷键，请按Ctrl+斜杠。\n查找和替换\nReason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.\nThere are some possible fixes:\nDrop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\nSELECT * EXCEPT (ehail_fee) FROM…\nModify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\nModify Airflow dag to make the conversion and avoid the error.\npv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {'ehail_fee': 'float64'}))\nSame type of ERROR - parquet files with different data types - Fix it with pandas\nHere is another possibility that could be interesting:\nYou can specify the dtypes when importing the file from csv to a dataframe with pandas\npd.from_csv(..., dtype=type_dict)\nOne obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital ‘I’. The type_dict is a python dictionary mapping the column names to the dtypes.\nSources:\nhttps://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\nNullable integer data type — pandas 1.5.3 documentation",
        "question": "What solution was implemented to address the column format issue in the web_to_gcp.py pipeline?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 249,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue: If you don’t define the column format while converting from csv to parquet Python will “choose” based on the first rows.\n✅Solution: Defined the schema while running web_to_gcp.py pipeline.\nSebastian adapted the script:\nhttps://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\nNeed a quick change to make the file work with gz files, added the following lines (and don’t forget to delete the file at the end of each iteration of the loop to avoid any problem of disk space)\nfile_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\nopen(file_name_gz, 'wb').write(r.content)\nos.system(f\"gzip -d {file_name_gz}\")\nos.system(f\"rm {file_name_init}.*\")\nSame ERROR - When running dbt run for fact_trips.sql, the task failed with error:\n“Parquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64”\n开启屏幕阅读器支持\n要启用屏幕阅读器支持，请按Ctrl+Alt+Z。要了解键盘快捷键，请按Ctrl+斜杠。\n查找和替换\nReason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.\nThere are some possible fixes:\nDrop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\nSELECT * EXCEPT (ehail_fee) FROM…\nModify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\nModify Airflow dag to make the conversion and avoid the error.\npv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {'ehail_fee': 'float64'}))\nSame type of ERROR - parquet files with different data types - Fix it with pandas\nHere is another possibility that could be interesting:\nYou can specify the dtypes when importing the file from csv to a dataframe with pandas\npd.from_csv(..., dtype=type_dict)\nOne obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital ‘I’. The type_dict is a python dictionary mapping the column names to the dtypes.\nSources:\nhttps://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\nNullable integer data type — pandas 1.5.3 documentation",
        "question": "What error occurred when running dbt run for fact_trips.sql related to the ehail_fee column?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 249,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue: If you don’t define the column format while converting from csv to parquet Python will “choose” based on the first rows.\n✅Solution: Defined the schema while running web_to_gcp.py pipeline.\nSebastian adapted the script:\nhttps://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\nNeed a quick change to make the file work with gz files, added the following lines (and don’t forget to delete the file at the end of each iteration of the loop to avoid any problem of disk space)\nfile_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\nopen(file_name_gz, 'wb').write(r.content)\nos.system(f\"gzip -d {file_name_gz}\")\nos.system(f\"rm {file_name_init}.*\")\nSame ERROR - When running dbt run for fact_trips.sql, the task failed with error:\n“Parquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64”\n开启屏幕阅读器支持\n要启用屏幕阅读器支持，请按Ctrl+Alt+Z。要了解键盘快捷键，请按Ctrl+斜杠。\n查找和替换\nReason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.\nThere are some possible fixes:\nDrop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\nSELECT * EXCEPT (ehail_fee) FROM…\nModify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\nModify Airflow dag to make the conversion and avoid the error.\npv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {'ehail_fee': 'float64'}))\nSame type of ERROR - parquet files with different data types - Fix it with pandas\nHere is another possibility that could be interesting:\nYou can specify the dtypes when importing the file from csv to a dataframe with pandas\npd.from_csv(..., dtype=type_dict)\nOne obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital ‘I’. The type_dict is a python dictionary mapping the column names to the dtypes.\nSources:\nhttps://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\nNullable integer data type — pandas 1.5.3 documentation",
        "question": "What are some possible fixes for the mismatch error regarding the ehail_fee column in Parquet files?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 249,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue: If you don’t define the column format while converting from csv to parquet Python will “choose” based on the first rows.\n✅Solution: Defined the schema while running web_to_gcp.py pipeline.\nSebastian adapted the script:\nhttps://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\nNeed a quick change to make the file work with gz files, added the following lines (and don’t forget to delete the file at the end of each iteration of the loop to avoid any problem of disk space)\nfile_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\nopen(file_name_gz, 'wb').write(r.content)\nos.system(f\"gzip -d {file_name_gz}\")\nos.system(f\"rm {file_name_init}.*\")\nSame ERROR - When running dbt run for fact_trips.sql, the task failed with error:\n“Parquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64”\n开启屏幕阅读器支持\n要启用屏幕阅读器支持，请按Ctrl+Alt+Z。要了解键盘快捷键，请按Ctrl+斜杠。\n查找和替换\nReason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.\nThere are some possible fixes:\nDrop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\nSELECT * EXCEPT (ehail_fee) FROM…\nModify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\nModify Airflow dag to make the conversion and avoid the error.\npv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {'ehail_fee': 'float64'}))\nSame type of ERROR - parquet files with different data types - Fix it with pandas\nHere is another possibility that could be interesting:\nYou can specify the dtypes when importing the file from csv to a dataframe with pandas\npd.from_csv(..., dtype=type_dict)\nOne obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital ‘I’. The type_dict is a python dictionary mapping the column names to the dtypes.\nSources:\nhttps://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\nNullable integer data type — pandas 1.5.3 documentation",
        "question": "How can you specify data types when importing a CSV file into a DataFrame using pandas?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 249,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If the provided URL isn’t working for you (https://nyc-tlc.s3.amazonaws.com/trip+data/):\nWe can use the GitHub CLI to easily download the needed trip data from https://github.com/DataTalksClub/nyc-tlc-data, and manually upload to a GCS bucket.\nInstructions on how to download the CLI here: https://github.com/cli/cli\nCommands to use:\ngh auth login\ngh release list -R DataTalksClub/nyc-tlc-data\ngh release download yellow -R DataTalksClub/nyc-tlc-data\ngh release download green -R DataTalksClub/nyc-tlc-data\netc.\nNow you can upload the files to a GCS bucket using the GUI.",
        "question": "What should you do if the provided URL for trip data is not working?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 250,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If the provided URL isn’t working for you (https://nyc-tlc.s3.amazonaws.com/trip+data/):\nWe can use the GitHub CLI to easily download the needed trip data from https://github.com/DataTalksClub/nyc-tlc-data, and manually upload to a GCS bucket.\nInstructions on how to download the CLI here: https://github.com/cli/cli\nCommands to use:\ngh auth login\ngh release list -R DataTalksClub/nyc-tlc-data\ngh release download yellow -R DataTalksClub/nyc-tlc-data\ngh release download green -R DataTalksClub/nyc-tlc-data\netc.\nNow you can upload the files to a GCS bucket using the GUI.",
        "question": "How can you download trip data using the GitHub CLI?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 250,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If the provided URL isn’t working for you (https://nyc-tlc.s3.amazonaws.com/trip+data/):\nWe can use the GitHub CLI to easily download the needed trip data from https://github.com/DataTalksClub/nyc-tlc-data, and manually upload to a GCS bucket.\nInstructions on how to download the CLI here: https://github.com/cli/cli\nCommands to use:\ngh auth login\ngh release list -R DataTalksClub/nyc-tlc-data\ngh release download yellow -R DataTalksClub/nyc-tlc-data\ngh release download green -R DataTalksClub/nyc-tlc-data\netc.\nNow you can upload the files to a GCS bucket using the GUI.",
        "question": "What is the command to log in using the GitHub CLI?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 250,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If the provided URL isn’t working for you (https://nyc-tlc.s3.amazonaws.com/trip+data/):\nWe can use the GitHub CLI to easily download the needed trip data from https://github.com/DataTalksClub/nyc-tlc-data, and manually upload to a GCS bucket.\nInstructions on how to download the CLI here: https://github.com/cli/cli\nCommands to use:\ngh auth login\ngh release list -R DataTalksClub/nyc-tlc-data\ngh release download yellow -R DataTalksClub/nyc-tlc-data\ngh release download green -R DataTalksClub/nyc-tlc-data\netc.\nNow you can upload the files to a GCS bucket using the GUI.",
        "question": "Which GitHub command is used to list releases from the nyc-tlc-data repository?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 250,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If the provided URL isn’t working for you (https://nyc-tlc.s3.amazonaws.com/trip+data/):\nWe can use the GitHub CLI to easily download the needed trip data from https://github.com/DataTalksClub/nyc-tlc-data, and manually upload to a GCS bucket.\nInstructions on how to download the CLI here: https://github.com/cli/cli\nCommands to use:\ngh auth login\ngh release list -R DataTalksClub/nyc-tlc-data\ngh release download yellow -R DataTalksClub/nyc-tlc-data\ngh release download green -R DataTalksClub/nyc-tlc-data\netc.\nNow you can upload the files to a GCS bucket using the GUI.",
        "question": "How do you upload the downloaded files to a GCS bucket?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 250,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "R: This conversion is needed for the question 3 of homework, in order to process files for fhv data. The error is:\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 7 columns, got 1: B02765\nCause: Some random line breaks in this particular file.\nFixed by opening a bash in the container executing the dag and manually running the following command that deletes all \\n not preceded by \\r.\nperl -i -pe 's/(?<!\\r)\\n/\\1/g' fhv_tripdata_2020-01.csv\nAfter that, clear the failed task in Airflow to force re-execution.",
        "question": "What was the error encountered while processing the FHV data?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 251,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "R: This conversion is needed for the question 3 of homework, in order to process files for fhv data. The error is:\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 7 columns, got 1: B02765\nCause: Some random line breaks in this particular file.\nFixed by opening a bash in the container executing the dag and manually running the following command that deletes all \\n not preceded by \\r.\nperl -i -pe 's/(?<!\\r)\\n/\\1/g' fhv_tripdata_2020-01.csv\nAfter that, clear the failed task in Airflow to force re-execution.",
        "question": "How many columns did the CSV parser expect compared to what it actually received?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 251,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "R: This conversion is needed for the question 3 of homework, in order to process files for fhv data. The error is:\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 7 columns, got 1: B02765\nCause: Some random line breaks in this particular file.\nFixed by opening a bash in the container executing the dag and manually running the following command that deletes all \\n not preceded by \\r.\nperl -i -pe 's/(?<!\\r)\\n/\\1/g' fhv_tripdata_2020-01.csv\nAfter that, clear the failed task in Airflow to force re-execution.",
        "question": "What caused the CSV parse error in the FHV data file?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 251,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "R: This conversion is needed for the question 3 of homework, in order to process files for fhv data. The error is:\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 7 columns, got 1: B02765\nCause: Some random line breaks in this particular file.\nFixed by opening a bash in the container executing the dag and manually running the following command that deletes all \\n not preceded by \\r.\nperl -i -pe 's/(?<!\\r)\\n/\\1/g' fhv_tripdata_2020-01.csv\nAfter that, clear the failed task in Airflow to force re-execution.",
        "question": "What command was used to fix the random line breaks in the file?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 251,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "R: This conversion is needed for the question 3 of homework, in order to process files for fhv data. The error is:\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 7 columns, got 1: B02765\nCause: Some random line breaks in this particular file.\nFixed by opening a bash in the container executing the dag and manually running the following command that deletes all \\n not preceded by \\r.\nperl -i -pe 's/(?<!\\r)\\n/\\1/g' fhv_tripdata_2020-01.csv\nAfter that, clear the failed task in Airflow to force re-execution.",
        "question": "What action needs to be taken in Airflow after fixing the issue with the file?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 251,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I initially followed data-engineering-zoomcamp/03-data-warehouse/extras/web_to_gcs.py at main · DataTalksClub/data-engineering-bootcamp (github.com)\nBut it was taking forever for the yellow trip data and when I tried to download and upload the parquet files directly to GCS, that works fine but when creating the Bigquery table, there was a schema inconsistency issue\nThen I found another hack shared in the slack which was suggested by Victoria.\n[Optional] Hack for loading data to BigQuery for Week 4 - YouTube\nPlease watch until the end as there is few schema changes required to be done",
        "question": "What was the initial issue encountered while processing the yellow trip data?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 252,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I initially followed data-engineering-zoomcamp/03-data-warehouse/extras/web_to_gcs.py at main · DataTalksClub/data-engineering-bootcamp (github.com)\nBut it was taking forever for the yellow trip data and when I tried to download and upload the parquet files directly to GCS, that works fine but when creating the Bigquery table, there was a schema inconsistency issue\nThen I found another hack shared in the slack which was suggested by Victoria.\n[Optional] Hack for loading data to BigQuery for Week 4 - YouTube\nPlease watch until the end as there is few schema changes required to be done",
        "question": "What approach was taken to download and upload the parquet files?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 252,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I initially followed data-engineering-zoomcamp/03-data-warehouse/extras/web_to_gcs.py at main · DataTalksClub/data-engineering-bootcamp (github.com)\nBut it was taking forever for the yellow trip data and when I tried to download and upload the parquet files directly to GCS, that works fine but when creating the Bigquery table, there was a schema inconsistency issue\nThen I found another hack shared in the slack which was suggested by Victoria.\n[Optional] Hack for loading data to BigQuery for Week 4 - YouTube\nPlease watch until the end as there is few schema changes required to be done",
        "question": "What problem arose when creating the BigQuery table after uploading the parquet files?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 252,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I initially followed data-engineering-zoomcamp/03-data-warehouse/extras/web_to_gcs.py at main · DataTalksClub/data-engineering-bootcamp (github.com)\nBut it was taking forever for the yellow trip data and when I tried to download and upload the parquet files directly to GCS, that works fine but when creating the Bigquery table, there was a schema inconsistency issue\nThen I found another hack shared in the slack which was suggested by Victoria.\n[Optional] Hack for loading data to BigQuery for Week 4 - YouTube\nPlease watch until the end as there is few schema changes required to be done",
        "question": "Who suggested the hack shared in the Slack for loading data to BigQuery?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 252,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I initially followed data-engineering-zoomcamp/03-data-warehouse/extras/web_to_gcs.py at main · DataTalksClub/data-engineering-bootcamp (github.com)\nBut it was taking forever for the yellow trip data and when I tried to download and upload the parquet files directly to GCS, that works fine but when creating the Bigquery table, there was a schema inconsistency issue\nThen I found another hack shared in the slack which was suggested by Victoria.\n[Optional] Hack for loading data to BigQuery for Week 4 - YouTube\nPlease watch until the end as there is few schema changes required to be done",
        "question": "What is the significance of watching the YouTube video mentioned in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 252,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "“gs\\storage_link\\*.parquet” need to be added in destination folder",
        "question": "What type of files need to be added in the destination folder?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 253,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "“gs\\storage_link\\*.parquet” need to be added in destination folder",
        "question": "What is the file format mentioned in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 253,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "“gs\\storage_link\\*.parquet” need to be added in destination folder",
        "question": "Where should the files be added according to the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 253,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "“gs\\storage_link\\*.parquet” need to be added in destination folder",
        "question": "What does 'gs\\storage_link\\*.parquet' represent?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 253,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "“gs\\storage_link\\*.parquet” need to be added in destination folder",
        "question": "Is there a specific folder mentioned for adding the files?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 253,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "One common cause experienced is lack of space after running prefect several times. When running prefect, check the folder ‘.prefect/storage’ and delete the logs now and then to avoid the problem.",
        "question": "What is a common issue experienced after running prefect multiple times?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 254,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "One common cause experienced is lack of space after running prefect several times. When running prefect, check the folder ‘.prefect/storage’ and delete the logs now and then to avoid the problem.",
        "question": "What should be checked when running prefect?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 254,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "One common cause experienced is lack of space after running prefect several times. When running prefect, check the folder ‘.prefect/storage’ and delete the logs now and then to avoid the problem.",
        "question": "Where can logs be found when using prefect?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 254,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "One common cause experienced is lack of space after running prefect several times. When running prefect, check the folder ‘.prefect/storage’ and delete the logs now and then to avoid the problem.",
        "question": "How can one prevent space issues when using prefect?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 254,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "One common cause experienced is lack of space after running prefect several times. When running prefect, check the folder ‘.prefect/storage’ and delete the logs now and then to avoid the problem.",
        "question": "What action should be taken regarding logs in the '.prefect/storage' folder?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 254,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can try to do this steps:",
        "question": "What steps can you try to do?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 255,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can try to do this steps:",
        "question": "Are there any specific steps mentioned in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 255,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can try to do this steps:",
        "question": "What is the purpose of the steps mentioned?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 255,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can try to do this steps:",
        "question": "Can these steps be applied to any situation?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 255,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can try to do this steps:",
        "question": "How should one begin the process of following these steps?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 255,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "R: Go to BigQuery, and check the location of BOTH\nThe source dataset (trips_data_all), and\nThe schema you’re trying to write to (name should be \tdbt_<first initial><last name> (if you didn’t change the default settings at the end when setting up your project))\nLikely, your source data will be in your region, but the write location will be a multi-regional location (US in this example). Delete these datasets, and recreate them with your specified region and the correct naming format.\nAlternatively, instead of removing datasets, you can specify the single-region location you are using. E.g. instead of ‘location: US’, specify the region, so ‘location: US-east1’. See this Github comment for more detail. Additionally please see this post of Sandy\nIn DBT cloud you can actually specify the location using the following steps:\nGPo to your profile page (top right drop-down --> profile)\nThen go to under Credentials --> Analytics (you may have customised this name)\nClick on Bigquery >\nHit Edit\nUpdate your location, you may need to re-upload your service account JSON to re-fetch your private key, and save. (NOTE: be sure to exactly copy the region BigQuery specifies your dataset is in.)",
        "question": "What is the recommended naming format for the schema when setting up your project in BigQuery?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 256,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "R: Go to BigQuery, and check the location of BOTH\nThe source dataset (trips_data_all), and\nThe schema you’re trying to write to (name should be \tdbt_<first initial><last name> (if you didn’t change the default settings at the end when setting up your project))\nLikely, your source data will be in your region, but the write location will be a multi-regional location (US in this example). Delete these datasets, and recreate them with your specified region and the correct naming format.\nAlternatively, instead of removing datasets, you can specify the single-region location you are using. E.g. instead of ‘location: US’, specify the region, so ‘location: US-east1’. See this Github comment for more detail. Additionally please see this post of Sandy\nIn DBT cloud you can actually specify the location using the following steps:\nGPo to your profile page (top right drop-down --> profile)\nThen go to under Credentials --> Analytics (you may have customised this name)\nClick on Bigquery >\nHit Edit\nUpdate your location, you may need to re-upload your service account JSON to re-fetch your private key, and save. (NOTE: be sure to exactly copy the region BigQuery specifies your dataset is in.)",
        "question": "How can you check the location of the source dataset in BigQuery?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 256,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "R: Go to BigQuery, and check the location of BOTH\nThe source dataset (trips_data_all), and\nThe schema you’re trying to write to (name should be \tdbt_<first initial><last name> (if you didn’t change the default settings at the end when setting up your project))\nLikely, your source data will be in your region, but the write location will be a multi-regional location (US in this example). Delete these datasets, and recreate them with your specified region and the correct naming format.\nAlternatively, instead of removing datasets, you can specify the single-region location you are using. E.g. instead of ‘location: US’, specify the region, so ‘location: US-east1’. See this Github comment for more detail. Additionally please see this post of Sandy\nIn DBT cloud you can actually specify the location using the following steps:\nGPo to your profile page (top right drop-down --> profile)\nThen go to under Credentials --> Analytics (you may have customised this name)\nClick on Bigquery >\nHit Edit\nUpdate your location, you may need to re-upload your service account JSON to re-fetch your private key, and save. (NOTE: be sure to exactly copy the region BigQuery specifies your dataset is in.)",
        "question": "What should you do if your write location is a multi-regional location?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 256,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "R: Go to BigQuery, and check the location of BOTH\nThe source dataset (trips_data_all), and\nThe schema you’re trying to write to (name should be \tdbt_<first initial><last name> (if you didn’t change the default settings at the end when setting up your project))\nLikely, your source data will be in your region, but the write location will be a multi-regional location (US in this example). Delete these datasets, and recreate them with your specified region and the correct naming format.\nAlternatively, instead of removing datasets, you can specify the single-region location you are using. E.g. instead of ‘location: US’, specify the region, so ‘location: US-east1’. See this Github comment for more detail. Additionally please see this post of Sandy\nIn DBT cloud you can actually specify the location using the following steps:\nGPo to your profile page (top right drop-down --> profile)\nThen go to under Credentials --> Analytics (you may have customised this name)\nClick on Bigquery >\nHit Edit\nUpdate your location, you may need to re-upload your service account JSON to re-fetch your private key, and save. (NOTE: be sure to exactly copy the region BigQuery specifies your dataset is in.)",
        "question": "What are the steps to specify the location in DBT cloud?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 256,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "R: Go to BigQuery, and check the location of BOTH\nThe source dataset (trips_data_all), and\nThe schema you’re trying to write to (name should be \tdbt_<first initial><last name> (if you didn’t change the default settings at the end when setting up your project))\nLikely, your source data will be in your region, but the write location will be a multi-regional location (US in this example). Delete these datasets, and recreate them with your specified region and the correct naming format.\nAlternatively, instead of removing datasets, you can specify the single-region location you are using. E.g. instead of ‘location: US’, specify the region, so ‘location: US-east1’. See this Github comment for more detail. Additionally please see this post of Sandy\nIn DBT cloud you can actually specify the location using the following steps:\nGPo to your profile page (top right drop-down --> profile)\nThen go to under Credentials --> Analytics (you may have customised this name)\nClick on Bigquery >\nHit Edit\nUpdate your location, you may need to re-upload your service account JSON to re-fetch your private key, and save. (NOTE: be sure to exactly copy the region BigQuery specifies your dataset is in.)",
        "question": "What may be required to re-upload when updating your location in DBT cloud?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 256,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`\nFix:\nReplace dbt_utils.surrogate_key  with dbt_utils.generate_surrogate_key in stg_green_tripdata.sql\nWhen executing dbt run after fact_trips.sql has been created, the task failed with error:\nR: “Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.”\n1. Fixed by adding the Storage Object Viewer role to the service account in use in BigQuery.\n2. Add the related roles to the service account in use in GCS.",
        "question": "What has `dbt_utils.surrogate_key` been replaced with?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 257,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`\nFix:\nReplace dbt_utils.surrogate_key  with dbt_utils.generate_surrogate_key in stg_green_tripdata.sql\nWhen executing dbt run after fact_trips.sql has been created, the task failed with error:\nR: “Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.”\n1. Fixed by adding the Storage Object Viewer role to the service account in use in BigQuery.\n2. Add the related roles to the service account in use in GCS.",
        "question": "How should the `dbt_utils.surrogate_key` be updated in the file stg_green_tripdata.sql?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 257,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`\nFix:\nReplace dbt_utils.surrogate_key  with dbt_utils.generate_surrogate_key in stg_green_tripdata.sql\nWhen executing dbt run after fact_trips.sql has been created, the task failed with error:\nR: “Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.”\n1. Fixed by adding the Storage Object Viewer role to the service account in use in BigQuery.\n2. Add the related roles to the service account in use in GCS.",
        "question": "What error occurred when executing dbt run after creating fact_trips.sql?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 257,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`\nFix:\nReplace dbt_utils.surrogate_key  with dbt_utils.generate_surrogate_key in stg_green_tripdata.sql\nWhen executing dbt run after fact_trips.sql has been created, the task failed with error:\nR: “Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.”\n1. Fixed by adding the Storage Object Viewer role to the service account in use in BigQuery.\n2. Add the related roles to the service account in use in GCS.",
        "question": "How was the 'Access Denied' error resolved in BigQuery?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 257,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`\nFix:\nReplace dbt_utils.surrogate_key  with dbt_utils.generate_surrogate_key in stg_green_tripdata.sql\nWhen executing dbt run after fact_trips.sql has been created, the task failed with error:\nR: “Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.”\n1. Fixed by adding the Storage Object Viewer role to the service account in use in BigQuery.\n2. Add the related roles to the service account in use in GCS.",
        "question": "What additional roles need to be added to the service account in GCS?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 257,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to create packages.yml file in main project directory and add packages’ meta data:\npackages:\n- package: dbt-labs/dbt_utils\nversion: 0.8.0\nAfter creating file run:\nAnd hit enter.",
        "question": "What file needs to be created in the main project directory?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 258,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to create packages.yml file in main project directory and add packages’ meta data:\npackages:\n- package: dbt-labs/dbt_utils\nversion: 0.8.0\nAfter creating file run:\nAnd hit enter.",
        "question": "What meta data should be added to the packages.yml file?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 258,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to create packages.yml file in main project directory and add packages’ meta data:\npackages:\n- package: dbt-labs/dbt_utils\nversion: 0.8.0\nAfter creating file run:\nAnd hit enter.",
        "question": "Which package is specified in the example?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 258,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to create packages.yml file in main project directory and add packages’ meta data:\npackages:\n- package: dbt-labs/dbt_utils\nversion: 0.8.0\nAfter creating file run:\nAnd hit enter.",
        "question": "What is the version number mentioned for the dbt_utils package?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 258,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to create packages.yml file in main project directory and add packages’ meta data:\npackages:\n- package: dbt-labs/dbt_utils\nversion: 0.8.0\nAfter creating file run:\nAnd hit enter.",
        "question": "What should you do after creating the packages.yml file?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 258,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ensure you properly format your yml file. Check the build logs if the run was completed successfully. You can expand the command history console (where you type the --vars '{'is_test_run': 'false'}')  and click on any stage’s logs to expand and read errors messages or warnings.",
        "question": "What is the importance of properly formatting a yml file?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 259,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ensure you properly format your yml file. Check the build logs if the run was completed successfully. You can expand the command history console (where you type the --vars '{'is_test_run': 'false'}')  and click on any stage’s logs to expand and read errors messages or warnings.",
        "question": "How can you check if the run was completed successfully?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 259,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ensure you properly format your yml file. Check the build logs if the run was completed successfully. You can expand the command history console (where you type the --vars '{'is_test_run': 'false'}')  and click on any stage’s logs to expand and read errors messages or warnings.",
        "question": "What are the steps to expand the command history console?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 259,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ensure you properly format your yml file. Check the build logs if the run was completed successfully. You can expand the command history console (where you type the --vars '{'is_test_run': 'false'}')  and click on any stage’s logs to expand and read errors messages or warnings.",
        "question": "How can you read error messages or warnings from the logs?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 259,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ensure you properly format your yml file. Check the build logs if the run was completed successfully. You can expand the command history console (where you type the --vars '{'is_test_run': 'false'}')  and click on any stage’s logs to expand and read errors messages or warnings.",
        "question": "What does the command '--vars '{'is_test_run': 'false'}'' indicate?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 259,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure you use:\ndbt run --var ‘is_test_run: false’ or\ndbt build --var ‘is_test_run: false’\n(watch out for formatted text from this document: re-type the single quotes). If that does not work, use --vars '{'is_test_run': 'false'}' with each phrase separately quoted.",
        "question": "What command should be used to run dbt with the variable 'is_test_run' set to false?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 260,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure you use:\ndbt run --var ‘is_test_run: false’ or\ndbt build --var ‘is_test_run: false’\n(watch out for formatted text from this document: re-type the single quotes). If that does not work, use --vars '{'is_test_run': 'false'}' with each phrase separately quoted.",
        "question": "What should you watch out for when retyping quotes in the dbt command?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 260,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure you use:\ndbt run --var ‘is_test_run: false’ or\ndbt build --var ‘is_test_run: false’\n(watch out for formatted text from this document: re-type the single quotes). If that does not work, use --vars '{'is_test_run': 'false'}' with each phrase separately quoted.",
        "question": "How can you specify variables when using the dbt build command?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 260,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure you use:\ndbt run --var ‘is_test_run: false’ or\ndbt build --var ‘is_test_run: false’\n(watch out for formatted text from this document: re-type the single quotes). If that does not work, use --vars '{'is_test_run': 'false'}' with each phrase separately quoted.",
        "question": "What is the alternative syntax for passing the 'is_test_run' variable if the initial command does not work?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 260,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure you use:\ndbt run --var ‘is_test_run: false’ or\ndbt build --var ‘is_test_run: false’\n(watch out for formatted text from this document: re-type the single quotes). If that does not work, use --vars '{'is_test_run': 'false'}' with each phrase separately quoted.",
        "question": "Why is it important to re-type the single quotes from the document when using dbt commands?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 260,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check if you specified if_exists argument correctly when writing data from GCS to BigQuery. When I wrote my automated flow for each month of the years 2019 and 2020 for green and yellow data I had specified if_exists=\"replace\" while I was experimenting with the flow setup. Once you want to run the flow for all months in 2019 and 2020 make sure to set if_exists=\"append\"\nif_exists=\"replace\" will replace the whole table with only the month data that you are writing into BigQuery in that one iteration -> you end up with only one month in BigQuery (the last one you inserted)\nif_exists=\"append\" will append the new monthly data -> you end up with data from all months",
        "question": "What does the if_exists argument control when writing data to BigQuery?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 261,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check if you specified if_exists argument correctly when writing data from GCS to BigQuery. When I wrote my automated flow for each month of the years 2019 and 2020 for green and yellow data I had specified if_exists=\"replace\" while I was experimenting with the flow setup. Once you want to run the flow for all months in 2019 and 2020 make sure to set if_exists=\"append\"\nif_exists=\"replace\" will replace the whole table with only the month data that you are writing into BigQuery in that one iteration -> you end up with only one month in BigQuery (the last one you inserted)\nif_exists=\"append\" will append the new monthly data -> you end up with data from all months",
        "question": "What happens if you set if_exists to 'replace' during the flow setup?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 261,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check if you specified if_exists argument correctly when writing data from GCS to BigQuery. When I wrote my automated flow for each month of the years 2019 and 2020 for green and yellow data I had specified if_exists=\"replace\" while I was experimenting with the flow setup. Once you want to run the flow for all months in 2019 and 2020 make sure to set if_exists=\"append\"\nif_exists=\"replace\" will replace the whole table with only the month data that you are writing into BigQuery in that one iteration -> you end up with only one month in BigQuery (the last one you inserted)\nif_exists=\"append\" will append the new monthly data -> you end up with data from all months",
        "question": "How should the if_exists argument be set for running the flow for all months of 2019 and 2020?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 261,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check if you specified if_exists argument correctly when writing data from GCS to BigQuery. When I wrote my automated flow for each month of the years 2019 and 2020 for green and yellow data I had specified if_exists=\"replace\" while I was experimenting with the flow setup. Once you want to run the flow for all months in 2019 and 2020 make sure to set if_exists=\"append\"\nif_exists=\"replace\" will replace the whole table with only the month data that you are writing into BigQuery in that one iteration -> you end up with only one month in BigQuery (the last one you inserted)\nif_exists=\"append\" will append the new monthly data -> you end up with data from all months",
        "question": "What is the consequence of using if_exists='append' when writing monthly data to BigQuery?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 261,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check if you specified if_exists argument correctly when writing data from GCS to BigQuery. When I wrote my automated flow for each month of the years 2019 and 2020 for green and yellow data I had specified if_exists=\"replace\" while I was experimenting with the flow setup. Once you want to run the flow for all months in 2019 and 2020 make sure to set if_exists=\"append\"\nif_exists=\"replace\" will replace the whole table with only the month data that you are writing into BigQuery in that one iteration -> you end up with only one month in BigQuery (the last one you inserted)\nif_exists=\"append\" will append the new monthly data -> you end up with data from all months",
        "question": "What months were included in the automated flow described in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 261,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "R: After the second SELECT, change this line:\ndate_trunc('month', pickup_datetime) as revenue_month,\nTo this line:\ndate_trunc(pickup_datetime, month) as revenue_month,\nMake sure that “month” isn’t surrounded by quotes!",
        "question": "What change should be made to the line after the second SELECT?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 262,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "R: After the second SELECT, change this line:\ndate_trunc('month', pickup_datetime) as revenue_month,\nTo this line:\ndate_trunc(pickup_datetime, month) as revenue_month,\nMake sure that “month” isn’t surrounded by quotes!",
        "question": "How should the function date_trunc be modified?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 262,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "R: After the second SELECT, change this line:\ndate_trunc('month', pickup_datetime) as revenue_month,\nTo this line:\ndate_trunc(pickup_datetime, month) as revenue_month,\nMake sure that “month” isn’t surrounded by quotes!",
        "question": "What is the correct way to specify 'month' in the function?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 262,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "R: After the second SELECT, change this line:\ndate_trunc('month', pickup_datetime) as revenue_month,\nTo this line:\ndate_trunc(pickup_datetime, month) as revenue_month,\nMake sure that “month” isn’t surrounded by quotes!",
        "question": "What does date_trunc do in the context of this SQL query?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 262,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "R: After the second SELECT, change this line:\ndate_trunc('month', pickup_datetime) as revenue_month,\nTo this line:\ndate_trunc(pickup_datetime, month) as revenue_month,\nMake sure that “month” isn’t surrounded by quotes!",
        "question": "Why is it important that 'month' isn't surrounded by quotes?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 262,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For this instead:\n{{ dbt_utils.generate_surrogate_key([ \n     field_a, \n     field_b, \n     field_c,\n     …,\n     field_z\n]) }}",
        "question": "What is the purpose of the dbt_utils.generate_surrogate_key function?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 263,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For this instead:\n{{ dbt_utils.generate_surrogate_key([ \n     field_a, \n     field_b, \n     field_c,\n     …,\n     field_z\n]) }}",
        "question": "Which fields are included in the surrogate key generation?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 263,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For this instead:\n{{ dbt_utils.generate_surrogate_key([ \n     field_a, \n     field_b, \n     field_c,\n     …,\n     field_z\n]) }}",
        "question": "How does the dbt_utils.generate_surrogate_key function utilize field_a, field_b, and field_c?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 263,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For this instead:\n{{ dbt_utils.generate_surrogate_key([ \n     field_a, \n     field_b, \n     field_c,\n     …,\n     field_z\n]) }}",
        "question": "What syntax is used to call the dbt_utils.generate_surrogate_key function?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 263,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For this instead:\n{{ dbt_utils.generate_surrogate_key([ \n     field_a, \n     field_b, \n     field_c,\n     …,\n     field_z\n]) }}",
        "question": "Can the list of fields for generating the surrogate key be extended beyond field_z?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 263,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remove the dataset from BigQuery which was created by dbt and run dbt run again so that it will recreate the dataset in BigQuery with the correct location",
        "question": "What should be done to the dataset in BigQuery before running dbt again?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 264,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remove the dataset from BigQuery which was created by dbt and run dbt run again so that it will recreate the dataset in BigQuery with the correct location",
        "question": "What tool is mentioned for creating datasets in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 264,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remove the dataset from BigQuery which was created by dbt and run dbt run again so that it will recreate the dataset in BigQuery with the correct location",
        "question": "Why is it necessary to run dbt run again?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 264,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remove the dataset from BigQuery which was created by dbt and run dbt run again so that it will recreate the dataset in BigQuery with the correct location",
        "question": "What is the expected outcome of running dbt run after removing the dataset?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 264,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remove the dataset from BigQuery which was created by dbt and run dbt run again so that it will recreate the dataset in BigQuery with the correct location",
        "question": "What does dbt stand for in the context of this text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 264,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.\nDBT - Why am I getting a new dataset after running my CI/CD Job? / What is this new dbt dataset in BigQuery?\nAnswer: when you create the CI/CD job, under ‘Compare Changes against an environment (Deferral) make sure that you select ‘ No; do not defer to another environment’ - otherwise dbt won’t merge your dev models into production models; it will create a new environment called ‘dbt_cloud_pr_number of pull request’",
        "question": "What should you do to ensure the dataset created by dbt has all the rows?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 265,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.\nDBT - Why am I getting a new dataset after running my CI/CD Job? / What is this new dbt dataset in BigQuery?\nAnswer: when you create the CI/CD job, under ‘Compare Changes against an environment (Deferral) make sure that you select ‘ No; do not defer to another environment’ - otherwise dbt won’t merge your dev models into production models; it will create a new environment called ‘dbt_cloud_pr_number of pull request’",
        "question": "Why might a new dataset be created after running a CI/CD job in dbt?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 265,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.\nDBT - Why am I getting a new dataset after running my CI/CD Job? / What is this new dbt dataset in BigQuery?\nAnswer: when you create the CI/CD job, under ‘Compare Changes against an environment (Deferral) make sure that you select ‘ No; do not defer to another environment’ - otherwise dbt won’t merge your dev models into production models; it will create a new environment called ‘dbt_cloud_pr_number of pull request’",
        "question": "What does it mean to defer changes to another environment in dbt?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 265,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.\nDBT - Why am I getting a new dataset after running my CI/CD Job? / What is this new dbt dataset in BigQuery?\nAnswer: when you create the CI/CD job, under ‘Compare Changes against an environment (Deferral) make sure that you select ‘ No; do not defer to another environment’ - otherwise dbt won’t merge your dev models into production models; it will create a new environment called ‘dbt_cloud_pr_number of pull request’",
        "question": "How can you prevent dbt from creating a new environment during a CI/CD job?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 265,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.\nDBT - Why am I getting a new dataset after running my CI/CD Job? / What is this new dbt dataset in BigQuery?\nAnswer: when you create the CI/CD job, under ‘Compare Changes against an environment (Deferral) make sure that you select ‘ No; do not defer to another environment’ - otherwise dbt won’t merge your dev models into production models; it will create a new environment called ‘dbt_cloud_pr_number of pull request’",
        "question": "What is the significance of selecting 'No; do not defer to another environment' in dbt?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 265,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Vic created three different datasets in the videos.. dbt_<name> was used for development and you used a production dataset for the production environment. What was the use for the staging dataset?\nR: Staging, as the name suggests, is like an intermediate between the raw datasets and the fact and dim tables, which are the finished product, so to speak. You'll notice that the datasets in staging are materialised as views and not tables.\nVic didn't use it for the project, you just need to create production and dbt_name + trips_data_all that you had already.",
        "question": "What are the names of the three datasets created by Vic?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 266,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Vic created three different datasets in the videos.. dbt_<name> was used for development and you used a production dataset for the production environment. What was the use for the staging dataset?\nR: Staging, as the name suggests, is like an intermediate between the raw datasets and the fact and dim tables, which are the finished product, so to speak. You'll notice that the datasets in staging are materialised as views and not tables.\nVic didn't use it for the project, you just need to create production and dbt_name + trips_data_all that you had already.",
        "question": "What is the purpose of the staging dataset in the data process?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 266,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Vic created three different datasets in the videos.. dbt_<name> was used for development and you used a production dataset for the production environment. What was the use for the staging dataset?\nR: Staging, as the name suggests, is like an intermediate between the raw datasets and the fact and dim tables, which are the finished product, so to speak. You'll notice that the datasets in staging are materialised as views and not tables.\nVic didn't use it for the project, you just need to create production and dbt_name + trips_data_all that you had already.",
        "question": "How are the datasets in the staging phase materialized?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 266,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Vic created three different datasets in the videos.. dbt_<name> was used for development and you used a production dataset for the production environment. What was the use for the staging dataset?\nR: Staging, as the name suggests, is like an intermediate between the raw datasets and the fact and dim tables, which are the finished product, so to speak. You'll notice that the datasets in staging are materialised as views and not tables.\nVic didn't use it for the project, you just need to create production and dbt_name + trips_data_all that you had already.",
        "question": "Why did Vic not use the staging dataset for the project?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 266,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Vic created three different datasets in the videos.. dbt_<name> was used for development and you used a production dataset for the production environment. What was the use for the staging dataset?\nR: Staging, as the name suggests, is like an intermediate between the raw datasets and the fact and dim tables, which are the finished product, so to speak. You'll notice that the datasets in staging are materialised as views and not tables.\nVic didn't use it for the project, you just need to create production and dbt_name + trips_data_all that you had already.",
        "question": "What naming convention does Vic use for the development dataset?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 266,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Try removing the “network: host” line in docker-compose.",
        "question": "What is the suggested modification to the docker-compose file?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 267,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Try removing the “network: host” line in docker-compose.",
        "question": "Why might someone want to remove the 'network: host' line?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 267,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Try removing the “network: host” line in docker-compose.",
        "question": "What does the 'network: host' line do in a docker-compose file?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 267,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Try removing the “network: host” line in docker-compose.",
        "question": "What could be the impact of removing the 'network: host' line?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 267,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Try removing the “network: host” line in docker-compose.",
        "question": "In what scenarios might you not need the 'network: host' setting?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 267,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Go to Account settings >> Project >> Analytics >> Click on your connection >> go all the way down to Location and type in the GCP location just as displayed in GCP (e.g. europe-west6). You might need to reupload your GCP key.\nDelete your dataset in GBQ\nRebuild project: dbt build\nNewly built dataset should be in the correct location",
        "question": "What steps should you follow to update the GCP location in the project settings?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 268,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Go to Account settings >> Project >> Analytics >> Click on your connection >> go all the way down to Location and type in the GCP location just as displayed in GCP (e.g. europe-west6). You might need to reupload your GCP key.\nDelete your dataset in GBQ\nRebuild project: dbt build\nNewly built dataset should be in the correct location",
        "question": "Why might you need to reupload your GCP key?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 268,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Go to Account settings >> Project >> Analytics >> Click on your connection >> go all the way down to Location and type in the GCP location just as displayed in GCP (e.g. europe-west6). You might need to reupload your GCP key.\nDelete your dataset in GBQ\nRebuild project: dbt build\nNewly built dataset should be in the correct location",
        "question": "What action should you take after deleting your dataset in GBQ?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 268,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Go to Account settings >> Project >> Analytics >> Click on your connection >> go all the way down to Location and type in the GCP location just as displayed in GCP (e.g. europe-west6). You might need to reupload your GCP key.\nDelete your dataset in GBQ\nRebuild project: dbt build\nNewly built dataset should be in the correct location",
        "question": "What does the command 'dbt build' accomplish in this process?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 268,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Go to Account settings >> Project >> Analytics >> Click on your connection >> go all the way down to Location and type in the GCP location just as displayed in GCP (e.g. europe-west6). You might need to reupload your GCP key.\nDelete your dataset in GBQ\nRebuild project: dbt build\nNewly built dataset should be in the correct location",
        "question": "Where should the newly built dataset be located after rebuilding the project?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 268,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Create a new branch to edit. More on this can be found here in the dbt docs.",
        "question": "What is the first step mentioned for editing?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 269,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Create a new branch to edit. More on this can be found here in the dbt docs.",
        "question": "Where can more information be found about creating a new branch?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 269,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Create a new branch to edit. More on this can be found here in the dbt docs.",
        "question": "What does 'dbt docs' refer to in the context?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 269,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Create a new branch to edit. More on this can be found here in the dbt docs.",
        "question": "Why is it necessary to create a new branch before editing?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 269,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Create a new branch to edit. More on this can be found here in the dbt docs.",
        "question": "What might be included in the dbt docs related to creating a new branch?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 269,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Create a new branch for development, then you can merge it to the main branch\nCreate a new branch and switch to this branch. It allows you to make changes. Then you can commit and push the changes to the “main” branch.",
        "question": "What is the first step in the development process mentioned in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 270,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Create a new branch for development, then you can merge it to the main branch\nCreate a new branch and switch to this branch. It allows you to make changes. Then you can commit and push the changes to the “main” branch.",
        "question": "How does one switch to the new branch after creating it?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 270,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Create a new branch for development, then you can merge it to the main branch\nCreate a new branch and switch to this branch. It allows you to make changes. Then you can commit and push the changes to the “main” branch.",
        "question": "What can you do after making changes in the new branch?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 270,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Create a new branch for development, then you can merge it to the main branch\nCreate a new branch and switch to this branch. It allows you to make changes. Then you can commit and push the changes to the “main” branch.",
        "question": "What is the purpose of merging a new branch to the main branch?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 270,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Create a new branch for development, then you can merge it to the main branch\nCreate a new branch and switch to this branch. It allows you to make changes. Then you can commit and push the changes to the “main” branch.",
        "question": "What actions are performed after committing changes in the new branch?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 270,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error:\nTriggered by pull requests\nThis feature is only available for dbt repositories connected through dbt Cloud's native integration with Github, Gitlab, or Azure DevOps\nSolution: Contrary to the guide on DTC repo, don’t use the Git Clone option. Use the Github one instead. Step-by-step guide to UN-LINK Git Clone and RE-LINK with Github in the next entry below",
        "question": "What triggers the error mentioned in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 271,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error:\nTriggered by pull requests\nThis feature is only available for dbt repositories connected through dbt Cloud's native integration with Github, Gitlab, or Azure DevOps\nSolution: Contrary to the guide on DTC repo, don’t use the Git Clone option. Use the Github one instead. Step-by-step guide to UN-LINK Git Clone and RE-LINK with Github in the next entry below",
        "question": "Which repositories can use the discussed feature?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 271,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error:\nTriggered by pull requests\nThis feature is only available for dbt repositories connected through dbt Cloud's native integration with Github, Gitlab, or Azure DevOps\nSolution: Contrary to the guide on DTC repo, don’t use the Git Clone option. Use the Github one instead. Step-by-step guide to UN-LINK Git Clone and RE-LINK with Github in the next entry below",
        "question": "What should be avoided when connecting a dbt repository according to the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 271,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error:\nTriggered by pull requests\nThis feature is only available for dbt repositories connected through dbt Cloud's native integration with Github, Gitlab, or Azure DevOps\nSolution: Contrary to the guide on DTC repo, don’t use the Git Clone option. Use the Github one instead. Step-by-step guide to UN-LINK Git Clone and RE-LINK with Github in the next entry below",
        "question": "What is the recommended option to use instead of Git Clone?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 271,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error:\nTriggered by pull requests\nThis feature is only available for dbt repositories connected through dbt Cloud's native integration with Github, Gitlab, or Azure DevOps\nSolution: Contrary to the guide on DTC repo, don’t use the Git Clone option. Use the Github one instead. Step-by-step guide to UN-LINK Git Clone and RE-LINK with Github in the next entry below",
        "question": "Is there a guide provided for linking with Github?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 271,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re trying to configure CI with Github and on the job’s options you can’t see Run on Pull Requests? on triggers, you have to reconnect with Github using native connection instead clone by SSH. Follow these steps:\nOn Profile Settings > Linked Accounts connect your Github account with dbt project allowing the permissions asked. More info at https://docs.getdbt.com/docs/collaborate/git/connect-gith\nDisconnect your current Github’s configuration from Account Settings > Projects (analytics) > Github connection. At the bottom left appears the button Disconnect, press it.\nOnce we have confirmed the change, we can configure it again. This time, choose Github and it will appear in all repositories which you have allowed to work with dbt. Select your repository and it’s ready.\nGo to the Deploy > job configuration’s page and go down until Triggers and now you can see the option Run on Pull Requests:",
        "question": "What should you do if you can't see 'Run on Pull Requests?' option in CI with Github?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 272,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re trying to configure CI with Github and on the job’s options you can’t see Run on Pull Requests? on triggers, you have to reconnect with Github using native connection instead clone by SSH. Follow these steps:\nOn Profile Settings > Linked Accounts connect your Github account with dbt project allowing the permissions asked. More info at https://docs.getdbt.com/docs/collaborate/git/connect-gith\nDisconnect your current Github’s configuration from Account Settings > Projects (analytics) > Github connection. At the bottom left appears the button Disconnect, press it.\nOnce we have confirmed the change, we can configure it again. This time, choose Github and it will appear in all repositories which you have allowed to work with dbt. Select your repository and it’s ready.\nGo to the Deploy > job configuration’s page and go down until Triggers and now you can see the option Run on Pull Requests:",
        "question": "Where do you find the option to connect your Github account with a dbt project?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 272,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re trying to configure CI with Github and on the job’s options you can’t see Run on Pull Requests? on triggers, you have to reconnect with Github using native connection instead clone by SSH. Follow these steps:\nOn Profile Settings > Linked Accounts connect your Github account with dbt project allowing the permissions asked. More info at https://docs.getdbt.com/docs/collaborate/git/connect-gith\nDisconnect your current Github’s configuration from Account Settings > Projects (analytics) > Github connection. At the bottom left appears the button Disconnect, press it.\nOnce we have confirmed the change, we can configure it again. This time, choose Github and it will appear in all repositories which you have allowed to work with dbt. Select your repository and it’s ready.\nGo to the Deploy > job configuration’s page and go down until Triggers and now you can see the option Run on Pull Requests:",
        "question": "What steps do you need to take to disconnect your current Github configuration?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 272,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re trying to configure CI with Github and on the job’s options you can’t see Run on Pull Requests? on triggers, you have to reconnect with Github using native connection instead clone by SSH. Follow these steps:\nOn Profile Settings > Linked Accounts connect your Github account with dbt project allowing the permissions asked. More info at https://docs.getdbt.com/docs/collaborate/git/connect-gith\nDisconnect your current Github’s configuration from Account Settings > Projects (analytics) > Github connection. At the bottom left appears the button Disconnect, press it.\nOnce we have confirmed the change, we can configure it again. This time, choose Github and it will appear in all repositories which you have allowed to work with dbt. Select your repository and it’s ready.\nGo to the Deploy > job configuration’s page and go down until Triggers and now you can see the option Run on Pull Requests:",
        "question": "How do you reconnect your Github account after disconnecting?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 272,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re trying to configure CI with Github and on the job’s options you can’t see Run on Pull Requests? on triggers, you have to reconnect with Github using native connection instead clone by SSH. Follow these steps:\nOn Profile Settings > Linked Accounts connect your Github account with dbt project allowing the permissions asked. More info at https://docs.getdbt.com/docs/collaborate/git/connect-gith\nDisconnect your current Github’s configuration from Account Settings > Projects (analytics) > Github connection. At the bottom left appears the button Disconnect, press it.\nOnce we have confirmed the change, we can configure it again. This time, choose Github and it will appear in all repositories which you have allowed to work with dbt. Select your repository and it’s ready.\nGo to the Deploy > job configuration’s page and go down until Triggers and now you can see the option Run on Pull Requests:",
        "question": "What will you see under Triggers after correctly configuring Github with dbt?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 272,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you're following video DE Zoomcamp 4.3.1 - Building the First DBT Models, you may have encountered an issue at 14:25 where the Lineage graph isn't displayed and a Compilation Error occurs, as shown in the attached image. Don't worry - a quick fix for this is to simply save your schema.yml file. Once you've done this, you should be able to view your Lineage graph without any further issues.",
        "question": "What issue is encountered at 14:25 in video DE Zoomcamp 4.3.1?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 273,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you're following video DE Zoomcamp 4.3.1 - Building the First DBT Models, you may have encountered an issue at 14:25 where the Lineage graph isn't displayed and a Compilation Error occurs, as shown in the attached image. Don't worry - a quick fix for this is to simply save your schema.yml file. Once you've done this, you should be able to view your Lineage graph without any further issues.",
        "question": "What does the issue in the video lead to when it occurs?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 273,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you're following video DE Zoomcamp 4.3.1 - Building the First DBT Models, you may have encountered an issue at 14:25 where the Lineage graph isn't displayed and a Compilation Error occurs, as shown in the attached image. Don't worry - a quick fix for this is to simply save your schema.yml file. Once you've done this, you should be able to view your Lineage graph without any further issues.",
        "question": "What is the quick fix suggested for the Compilation Error?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 273,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you're following video DE Zoomcamp 4.3.1 - Building the First DBT Models, you may have encountered an issue at 14:25 where the Lineage graph isn't displayed and a Compilation Error occurs, as shown in the attached image. Don't worry - a quick fix for this is to simply save your schema.yml file. Once you've done this, you should be able to view your Lineage graph without any further issues.",
        "question": "What file needs to be saved to resolve the Lineage graph display issue?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 273,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you're following video DE Zoomcamp 4.3.1 - Building the First DBT Models, you may have encountered an issue at 14:25 where the Lineage graph isn't displayed and a Compilation Error occurs, as shown in the attached image. Don't worry - a quick fix for this is to simply save your schema.yml file. Once you've done this, you should be able to view your Lineage graph without any further issues.",
        "question": "What should happen after saving the schema.yml file?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 273,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "> in macro test_accepted_values (tests/generic/builtin.sql)\n> called by test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\nRemember that you have to add to dbt_project.yml the vars:\nvars:\npayment_type_values: [1, 2, 3, 4, 5, 6]",
        "question": "What is the name of the macro mentioned in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 274,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "> in macro test_accepted_values (tests/generic/builtin.sql)\n> called by test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\nRemember that you have to add to dbt_project.yml the vars:\nvars:\npayment_type_values: [1, 2, 3, 4, 5, 6]",
        "question": "Where is the macro 'test_accepted_values' located?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 274,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "> in macro test_accepted_values (tests/generic/builtin.sql)\n> called by test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\nRemember that you have to add to dbt_project.yml the vars:\nvars:\npayment_type_values: [1, 2, 3, 4, 5, 6]",
        "question": "Which model calls the macro 'test_accepted_values'?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 274,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "> in macro test_accepted_values (tests/generic/builtin.sql)\n> called by test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\nRemember that you have to add to dbt_project.yml the vars:\nvars:\npayment_type_values: [1, 2, 3, 4, 5, 6]",
        "question": "What values need to be added to dbt_project.yml for payment_type_values?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 274,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "> in macro test_accepted_values (tests/generic/builtin.sql)\n> called by test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\nRemember that you have to add to dbt_project.yml the vars:\nvars:\npayment_type_values: [1, 2, 3, 4, 5, 6]",
        "question": "How many values are included in the 'payment_type_values' list?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 274,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You will face this issue if you copied and pasted the exact macro directly from data-engineering-zoomcamp repo.\nBigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]; reason: invalidQuery, location: query, message: No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]')\nWhat you’d have to do is to change the data type of the numbers (1, 2, 3 etc.) to text by inserting ‘’, as the initial ‘payment_type’ data type should be string (Note: I extracted and loaded the green trips data using Google BQ Marketplace)\n{#\nThis macro returns the description of the payment_type\n#}\n{% macro get_payment_type_description(payment_type) -%}\ncase {{ payment_type }}\nwhen '1' then 'Credit card'\nwhen '2' then 'Cash'\nwhen '3' then 'No charge'\nwhen '4' then 'Dispute'\nwhen '5' then 'Unknown'\nwhen '6' then 'Voided trip'\nend\n{%- endmacro %}",
        "question": "What issue arises when copying and pasting the macro from the data-engineering-zoomcamp repo?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 275,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You will face this issue if you copied and pasted the exact macro directly from data-engineering-zoomcamp repo.\nBigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]; reason: invalidQuery, location: query, message: No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]')\nWhat you’d have to do is to change the data type of the numbers (1, 2, 3 etc.) to text by inserting ‘’, as the initial ‘payment_type’ data type should be string (Note: I extracted and loaded the green trips data using Google BQ Marketplace)\n{#\nThis macro returns the description of the payment_type\n#}\n{% macro get_payment_type_description(payment_type) -%}\ncase {{ payment_type }}\nwhen '1' then 'Credit card'\nwhen '2' then 'Cash'\nwhen '3' then 'No charge'\nwhen '4' then 'Dispute'\nwhen '5' then 'Unknown'\nwhen '6' then 'Voided trip'\nend\n{%- endmacro %}",
        "question": "What error message does the BigQuery adapter return?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 275,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You will face this issue if you copied and pasted the exact macro directly from data-engineering-zoomcamp repo.\nBigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]; reason: invalidQuery, location: query, message: No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]')\nWhat you’d have to do is to change the data type of the numbers (1, 2, 3 etc.) to text by inserting ‘’, as the initial ‘payment_type’ data type should be string (Note: I extracted and loaded the green trips data using Google BQ Marketplace)\n{#\nThis macro returns the description of the payment_type\n#}\n{% macro get_payment_type_description(payment_type) -%}\ncase {{ payment_type }}\nwhen '1' then 'Credit card'\nwhen '2' then 'Cash'\nwhen '3' then 'No charge'\nwhen '4' then 'Dispute'\nwhen '5' then 'Unknown'\nwhen '6' then 'Voided trip'\nend\n{%- endmacro %}",
        "question": "How should the data type of the numbers for 'payment_type' be changed to avoid errors?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 275,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You will face this issue if you copied and pasted the exact macro directly from data-engineering-zoomcamp repo.\nBigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]; reason: invalidQuery, location: query, message: No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]')\nWhat you’d have to do is to change the data type of the numbers (1, 2, 3 etc.) to text by inserting ‘’, as the initial ‘payment_type’ data type should be string (Note: I extracted and loaded the green trips data using Google BQ Marketplace)\n{#\nThis macro returns the description of the payment_type\n#}\n{% macro get_payment_type_description(payment_type) -%}\ncase {{ payment_type }}\nwhen '1' then 'Credit card'\nwhen '2' then 'Cash'\nwhen '3' then 'No charge'\nwhen '4' then 'Dispute'\nwhen '5' then 'Unknown'\nwhen '6' then 'Voided trip'\nend\n{%- endmacro %}",
        "question": "What does the macro 'get_payment_type_description' return based on the value of 'payment_type'?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 275,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You will face this issue if you copied and pasted the exact macro directly from data-engineering-zoomcamp repo.\nBigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]; reason: invalidQuery, location: query, message: No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]')\nWhat you’d have to do is to change the data type of the numbers (1, 2, 3 etc.) to text by inserting ‘’, as the initial ‘payment_type’ data type should be string (Note: I extracted and loaded the green trips data using Google BQ Marketplace)\n{#\nThis macro returns the description of the payment_type\n#}\n{% macro get_payment_type_description(payment_type) -%}\ncase {{ payment_type }}\nwhen '1' then 'Credit card'\nwhen '2' then 'Cash'\nwhen '3' then 'No charge'\nwhen '4' then 'Dispute'\nwhen '5' then 'Unknown'\nwhen '6' then 'Voided trip'\nend\n{%- endmacro %}",
        "question": "What are the possible descriptions returned by the 'get_payment_type_description' macro for different payment types?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 275,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The dbt error  log contains a link to BigQuery. When you follow it you will see your query and the problematic line will be highlighted.",
        "question": "What does the dbt error log contain?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 276,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The dbt error  log contains a link to BigQuery. When you follow it you will see your query and the problematic line will be highlighted.",
        "question": "What happens when you follow the link in the dbt error log?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 276,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The dbt error  log contains a link to BigQuery. When you follow it you will see your query and the problematic line will be highlighted.",
        "question": "How can you identify the problematic line in your query using the dbt error log?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 276,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The dbt error  log contains a link to BigQuery. When you follow it you will see your query and the problematic line will be highlighted.",
        "question": "Which database is mentioned in relation to the dbt error log?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 276,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The dbt error  log contains a link to BigQuery. When you follow it you will see your query and the problematic line will be highlighted.",
        "question": "What key functionality does the dbt error log provide to users?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 276,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is a default behaviour of dbt to append custom schema to initial schema. To override this behaviour simply create a macro named “generate_schema_name.sql”:\n{% macro generate_schema_name(custom_schema_name, node) -%}\n{%- set default_schema = target.schema -%}\n{%- if custom_schema_name is none -%}\n{{ default_schema }}\n{%- else -%}\n{{ custom_schema_name | trim }}\n{%- endif -%}\n{%- endmacro %}\nNow you can override default custom schema in “dbt_project.yml”:",
        "question": "What is the default behavior of dbt regarding custom schemas?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 277,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is a default behaviour of dbt to append custom schema to initial schema. To override this behaviour simply create a macro named “generate_schema_name.sql”:\n{% macro generate_schema_name(custom_schema_name, node) -%}\n{%- set default_schema = target.schema -%}\n{%- if custom_schema_name is none -%}\n{{ default_schema }}\n{%- else -%}\n{{ custom_schema_name | trim }}\n{%- endif -%}\n{%- endmacro %}\nNow you can override default custom schema in “dbt_project.yml”:",
        "question": "How can you override the default custom schema behavior in dbt?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 277,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is a default behaviour of dbt to append custom schema to initial schema. To override this behaviour simply create a macro named “generate_schema_name.sql”:\n{% macro generate_schema_name(custom_schema_name, node) -%}\n{%- set default_schema = target.schema -%}\n{%- if custom_schema_name is none -%}\n{{ default_schema }}\n{%- else -%}\n{{ custom_schema_name | trim }}\n{%- endif -%}\n{%- endmacro %}\nNow you can override default custom schema in “dbt_project.yml”:",
        "question": "What is the purpose of the macro named 'generate_schema_name.sql'?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 277,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is a default behaviour of dbt to append custom schema to initial schema. To override this behaviour simply create a macro named “generate_schema_name.sql”:\n{% macro generate_schema_name(custom_schema_name, node) -%}\n{%- set default_schema = target.schema -%}\n{%- if custom_schema_name is none -%}\n{{ default_schema }}\n{%- else -%}\n{{ custom_schema_name | trim }}\n{%- endif -%}\n{%- endmacro %}\nNow you can override default custom schema in “dbt_project.yml”:",
        "question": "What parameters does the 'generate_schema_name' macro accept?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 277,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is a default behaviour of dbt to append custom schema to initial schema. To override this behaviour simply create a macro named “generate_schema_name.sql”:\n{% macro generate_schema_name(custom_schema_name, node) -%}\n{%- set default_schema = target.schema -%}\n{%- if custom_schema_name is none -%}\n{{ default_schema }}\n{%- else -%}\n{{ custom_schema_name | trim }}\n{%- endif -%}\n{%- endmacro %}\nNow you can override default custom schema in “dbt_project.yml”:",
        "question": "Where do you specify the overridden schema in dbt?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 277,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There is a project setting which allows you to set `Project subdirectory` in dbt cloud:",
        "question": "What is the purpose of the project setting in dbt cloud?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 278,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There is a project setting which allows you to set `Project subdirectory` in dbt cloud:",
        "question": "How do you set the 'Project subdirectory' in dbt cloud?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 278,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There is a project setting which allows you to set `Project subdirectory` in dbt cloud:",
        "question": "What options are available for project settings in dbt cloud?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 278,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There is a project setting which allows you to set `Project subdirectory` in dbt cloud:",
        "question": "Can you explain the significance of 'Project subdirectory' in dbt cloud?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 278,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There is a project setting which allows you to set `Project subdirectory` in dbt cloud:",
        "question": "Is the 'Project subdirectory' setting mandatory in dbt cloud?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 278,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remember that you should modify accordingly your .sql models, to read from existing table names in BigQuery/postgres db\nExample: select * from {{ source('staging',<your table name in the database>') }}",
        "question": "What should you modify in your .sql models?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 279,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remember that you should modify accordingly your .sql models, to read from existing table names in BigQuery/postgres db\nExample: select * from {{ source('staging',<your table name in the database>') }}",
        "question": "What is the purpose of reading from existing table names?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 279,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remember that you should modify accordingly your .sql models, to read from existing table names in BigQuery/postgres db\nExample: select * from {{ source('staging',<your table name in the database>') }}",
        "question": "Which databases are mentioned in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 279,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remember that you should modify accordingly your .sql models, to read from existing table names in BigQuery/postgres db\nExample: select * from {{ source('staging',<your table name in the database>') }}",
        "question": "What is the syntax for selecting data from a source in the example provided?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 279,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remember that you should modify accordingly your .sql models, to read from existing table names in BigQuery/postgres db\nExample: select * from {{ source('staging',<your table name in the database>') }}",
        "question": "What is the placeholder used for table names in the given example?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 279,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure that you create a pull request from your Development branch to the Production branch (main by default). After that, check in your ‘seeds’ folder if the seed file is inside it.\nAnother thing to check is your .gitignore file. Make sure that the .csv extension is not included.",
        "question": "What is the purpose of creating a pull request from the Development branch to the Production branch?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 280,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure that you create a pull request from your Development branch to the Production branch (main by default). After that, check in your ‘seeds’ folder if the seed file is inside it.\nAnother thing to check is your .gitignore file. Make sure that the .csv extension is not included.",
        "question": "What should you verify is present in the 'seeds' folder after creating the pull request?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 280,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure that you create a pull request from your Development branch to the Production branch (main by default). After that, check in your ‘seeds’ folder if the seed file is inside it.\nAnother thing to check is your .gitignore file. Make sure that the .csv extension is not included.",
        "question": "Why is it important to check the .gitignore file?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 280,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure that you create a pull request from your Development branch to the Production branch (main by default). After that, check in your ‘seeds’ folder if the seed file is inside it.\nAnother thing to check is your .gitignore file. Make sure that the .csv extension is not included.",
        "question": "What file extension should not be included in the .gitignore file?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 280,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure that you create a pull request from your Development branch to the Production branch (main by default). After that, check in your ‘seeds’ folder if the seed file is inside it.\nAnother thing to check is your .gitignore file. Make sure that the .csv extension is not included.",
        "question": "What is the default name for the Production branch mentioned in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 280,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "1. Go to your dbt cloud service account\n1. Adding the  [Storage Object Admin,Storage Admin] role in addition tco BigQuery Admin.",
        "question": "What is the first step to take in the process described in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 281,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "1. Go to your dbt cloud service account\n1. Adding the  [Storage Object Admin,Storage Admin] role in addition tco BigQuery Admin.",
        "question": "Which roles should be added to the user in addition to BigQuery Admin?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 281,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "1. Go to your dbt cloud service account\n1. Adding the  [Storage Object Admin,Storage Admin] role in addition tco BigQuery Admin.",
        "question": "What specific roles are mentioned that need to be assigned?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 281,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "1. Go to your dbt cloud service account\n1. Adding the  [Storage Object Admin,Storage Admin] role in addition tco BigQuery Admin.",
        "question": "Which service account is being referenced?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 281,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "1. Go to your dbt cloud service account\n1. Adding the  [Storage Object Admin,Storage Admin] role in addition tco BigQuery Admin.",
        "question": "Is the addition of roles optional or required according to the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 281,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: when injecting data to bigquery, you may face the type error. This is because pandas by default will parse integer columns with missing value as float type.\nSolution:\nOne way to solve this problem is to specify/ cast data type Int64 during the data transformation stage.\nHowever, you may be lazy to type all the int columns. If that is the case, you can simply use convert_dtypes to infer the data type\n# Make pandas to infer correct data type (as pandas parse int with missing as float)\ndf.fillna(-999999, inplace=True)\ndf = df.convert_dtypes()\ndf = df.replace(-999999, None)",
        "question": "What is a common issue encountered when injecting data to BigQuery using pandas?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 282,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: when injecting data to bigquery, you may face the type error. This is because pandas by default will parse integer columns with missing value as float type.\nSolution:\nOne way to solve this problem is to specify/ cast data type Int64 during the data transformation stage.\nHowever, you may be lazy to type all the int columns. If that is the case, you can simply use convert_dtypes to infer the data type\n# Make pandas to infer correct data type (as pandas parse int with missing as float)\ndf.fillna(-999999, inplace=True)\ndf = df.convert_dtypes()\ndf = df.replace(-999999, None)",
        "question": "Why does pandas parse integer columns with missing values as float type?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 282,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: when injecting data to bigquery, you may face the type error. This is because pandas by default will parse integer columns with missing value as float type.\nSolution:\nOne way to solve this problem is to specify/ cast data type Int64 during the data transformation stage.\nHowever, you may be lazy to type all the int columns. If that is the case, you can simply use convert_dtypes to infer the data type\n# Make pandas to infer correct data type (as pandas parse int with missing as float)\ndf.fillna(-999999, inplace=True)\ndf = df.convert_dtypes()\ndf = df.replace(-999999, None)",
        "question": "How can you specify the data type during the data transformation stage?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 282,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: when injecting data to bigquery, you may face the type error. This is because pandas by default will parse integer columns with missing value as float type.\nSolution:\nOne way to solve this problem is to specify/ cast data type Int64 during the data transformation stage.\nHowever, you may be lazy to type all the int columns. If that is the case, you can simply use convert_dtypes to infer the data type\n# Make pandas to infer correct data type (as pandas parse int with missing as float)\ndf.fillna(-999999, inplace=True)\ndf = df.convert_dtypes()\ndf = df.replace(-999999, None)",
        "question": "What function can be used to infer the correct data type in pandas?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 282,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Problem: when injecting data to bigquery, you may face the type error. This is because pandas by default will parse integer columns with missing value as float type.\nSolution:\nOne way to solve this problem is to specify/ cast data type Int64 during the data transformation stage.\nHowever, you may be lazy to type all the int columns. If that is the case, you can simply use convert_dtypes to infer the data type\n# Make pandas to infer correct data type (as pandas parse int with missing as float)\ndf.fillna(-999999, inplace=True)\ndf = df.convert_dtypes()\ndf = df.replace(-999999, None)",
        "question": "What is the purpose of replacing -999999 with None after filling missing values?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 282,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Seed files loaded from directory with name ‘seed’, that’s why you should rename dir with name ‘data’ to ‘seed’",
        "question": "What is the name of the directory from which seed files are loaded?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 283,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Seed files loaded from directory with name ‘seed’, that’s why you should rename dir with name ‘data’ to ‘seed’",
        "question": "What should you rename the directory 'data' to?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 283,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Seed files loaded from directory with name ‘seed’, that’s why you should rename dir with name ‘data’ to ‘seed’",
        "question": "Why is it necessary to rename the directory to 'seed'?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 283,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Seed files loaded from directory with name ‘seed’, that’s why you should rename dir with name ‘data’ to ‘seed’",
        "question": "What happens if the directory is not renamed to 'seed'?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 283,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Seed files loaded from directory with name ‘seed’, that’s why you should rename dir with name ‘data’ to ‘seed’",
        "question": "Where are the seed files being loaded from?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 283,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check the .gitignore file and make sure you don’t have *.csv in it\n\nDbt error 404 was not found in location\nMy specific error:\nRuntime Error in rpc request (from remote system.sql) 404 Not found: Table dtc-de-0315:trips_data_all.green_tripdata_partitioned was not found in location europe-west6 Location: europe-west6 Job ID: 168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c\nMake sure all of your datasets have the correct region and not a generalised region:\nEurope-west6 as opposed to EU\n\nMatch this in dbt settings:\ndbt -> projects -> optional settings -> manually set location to match",
        "question": "What should you check in the .gitignore file?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 284,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check the .gitignore file and make sure you don’t have *.csv in it\n\nDbt error 404 was not found in location\nMy specific error:\nRuntime Error in rpc request (from remote system.sql) 404 Not found: Table dtc-de-0315:trips_data_all.green_tripdata_partitioned was not found in location europe-west6 Location: europe-west6 Job ID: 168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c\nMake sure all of your datasets have the correct region and not a generalised region:\nEurope-west6 as opposed to EU\n\nMatch this in dbt settings:\ndbt -> projects -> optional settings -> manually set location to match",
        "question": "What specific error message indicates a problem with the location of a table in dbt?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 284,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check the .gitignore file and make sure you don’t have *.csv in it\n\nDbt error 404 was not found in location\nMy specific error:\nRuntime Error in rpc request (from remote system.sql) 404 Not found: Table dtc-de-0315:trips_data_all.green_tripdata_partitioned was not found in location europe-west6 Location: europe-west6 Job ID: 168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c\nMake sure all of your datasets have the correct region and not a generalised region:\nEurope-west6 as opposed to EU\n\nMatch this in dbt settings:\ndbt -> projects -> optional settings -> manually set location to match",
        "question": "Which job ID is associated with the runtime error mentioned in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 284,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check the .gitignore file and make sure you don’t have *.csv in it\n\nDbt error 404 was not found in location\nMy specific error:\nRuntime Error in rpc request (from remote system.sql) 404 Not found: Table dtc-de-0315:trips_data_all.green_tripdata_partitioned was not found in location europe-west6 Location: europe-west6 Job ID: 168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c\nMake sure all of your datasets have the correct region and not a generalised region:\nEurope-west6 as opposed to EU\n\nMatch this in dbt settings:\ndbt -> projects -> optional settings -> manually set location to match",
        "question": "What is the correct region that should be specified in datasets according to the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 284,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check the .gitignore file and make sure you don’t have *.csv in it\n\nDbt error 404 was not found in location\nMy specific error:\nRuntime Error in rpc request (from remote system.sql) 404 Not found: Table dtc-de-0315:trips_data_all.green_tripdata_partitioned was not found in location europe-west6 Location: europe-west6 Job ID: 168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c\nMake sure all of your datasets have the correct region and not a generalised region:\nEurope-west6 as opposed to EU\n\nMatch this in dbt settings:\ndbt -> projects -> optional settings -> manually set location to match",
        "question": "Where can you manually set the location in dbt settings?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 284,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The easiest way to avoid these errors is by ingesting the relevant data in a .csv.gz file type. Then, do:\nCREATE OR REPLACE EXTERNAL TABLE `dtc-de.trips_data_all.fhv_tripdata`\nOPTIONS (\nformat = 'CSV',\nuris = ['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']\n);\nAs an example. You should no longer have any data type issues for week 4.",
        "question": "What file type is recommended to avoid errors when ingesting data?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 285,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The easiest way to avoid these errors is by ingesting the relevant data in a .csv.gz file type. Then, do:\nCREATE OR REPLACE EXTERNAL TABLE `dtc-de.trips_data_all.fhv_tripdata`\nOPTIONS (\nformat = 'CSV',\nuris = ['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']\n);\nAs an example. You should no longer have any data type issues for week 4.",
        "question": "What is the SQL command used to create or replace an external table?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 285,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The easiest way to avoid these errors is by ingesting the relevant data in a .csv.gz file type. Then, do:\nCREATE OR REPLACE EXTERNAL TABLE `dtc-de.trips_data_all.fhv_tripdata`\nOPTIONS (\nformat = 'CSV',\nuris = ['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']\n);\nAs an example. You should no longer have any data type issues for week 4.",
        "question": "Where is the sample CSV data located according to the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 285,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The easiest way to avoid these errors is by ingesting the relevant data in a .csv.gz file type. Then, do:\nCREATE OR REPLACE EXTERNAL TABLE `dtc-de.trips_data_all.fhv_tripdata`\nOPTIONS (\nformat = 'CSV',\nuris = ['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']\n);\nAs an example. You should no longer have any data type issues for week 4.",
        "question": "What specific week is mentioned as having data type issues in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 285,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The easiest way to avoid these errors is by ingesting the relevant data in a .csv.gz file type. Then, do:\nCREATE OR REPLACE EXTERNAL TABLE `dtc-de.trips_data_all.fhv_tripdata`\nOPTIONS (\nformat = 'CSV',\nuris = ['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']\n);\nAs an example. You should no longer have any data type issues for week 4.",
        "question": "What format is specified in the options for the external table?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 285,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is due to the way the deduplication is done in the two staging files.\nSolution: add order by in the partition by part of both staging files. Keep adding columns to order by until the number of rows in the fact_trips table is consistent when re-running the fact_trips model.\nExplanation (a bit convoluted, feel free to clarify, correct etc.)\nWe partition by vendor id and pickup_datetime and choose the first row (rn=1) from all these partitions. These partitions are not ordered, so every time we run this, the first row might be a different one. Since the first row is different between runs, it might or might not contain an unknown borough. Then, in the fact_trips model we will discard a different number of rows when we discard all values with an unknown borough.",
        "question": "What is the primary issue with deduplication in the staging files?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 286,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is due to the way the deduplication is done in the two staging files.\nSolution: add order by in the partition by part of both staging files. Keep adding columns to order by until the number of rows in the fact_trips table is consistent when re-running the fact_trips model.\nExplanation (a bit convoluted, feel free to clarify, correct etc.)\nWe partition by vendor id and pickup_datetime and choose the first row (rn=1) from all these partitions. These partitions are not ordered, so every time we run this, the first row might be a different one. Since the first row is different between runs, it might or might not contain an unknown borough. Then, in the fact_trips model we will discard a different number of rows when we discard all values with an unknown borough.",
        "question": "What solution is proposed to achieve consistent row counts in the fact_trips table?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 286,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is due to the way the deduplication is done in the two staging files.\nSolution: add order by in the partition by part of both staging files. Keep adding columns to order by until the number of rows in the fact_trips table is consistent when re-running the fact_trips model.\nExplanation (a bit convoluted, feel free to clarify, correct etc.)\nWe partition by vendor id and pickup_datetime and choose the first row (rn=1) from all these partitions. These partitions are not ordered, so every time we run this, the first row might be a different one. Since the first row is different between runs, it might or might not contain an unknown borough. Then, in the fact_trips model we will discard a different number of rows when we discard all values with an unknown borough.",
        "question": "How are the partitions defined in the current model for deduplication?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 286,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is due to the way the deduplication is done in the two staging files.\nSolution: add order by in the partition by part of both staging files. Keep adding columns to order by until the number of rows in the fact_trips table is consistent when re-running the fact_trips model.\nExplanation (a bit convoluted, feel free to clarify, correct etc.)\nWe partition by vendor id and pickup_datetime and choose the first row (rn=1) from all these partitions. These partitions are not ordered, so every time we run this, the first row might be a different one. Since the first row is different between runs, it might or might not contain an unknown borough. Then, in the fact_trips model we will discard a different number of rows when we discard all values with an unknown borough.",
        "question": "What role does the order by clause play in the deduplication process?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 286,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is due to the way the deduplication is done in the two staging files.\nSolution: add order by in the partition by part of both staging files. Keep adding columns to order by until the number of rows in the fact_trips table is consistent when re-running the fact_trips model.\nExplanation (a bit convoluted, feel free to clarify, correct etc.)\nWe partition by vendor id and pickup_datetime and choose the first row (rn=1) from all these partitions. These partitions are not ordered, so every time we run this, the first row might be a different one. Since the first row is different between runs, it might or might not contain an unknown borough. Then, in the fact_trips model we will discard a different number of rows when we discard all values with an unknown borough.",
        "question": "Why might the first row selected from partitions differ between runs?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 286,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you encounter data type error on trip_type column, it may due to some nan values that isn’t null in bigquery.\nSolution: try casting it to FLOAT datatype instead of NUMERIC",
        "question": "What might cause a data type error on the trip_type column?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 287,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you encounter data type error on trip_type column, it may due to some nan values that isn’t null in bigquery.\nSolution: try casting it to FLOAT datatype instead of NUMERIC",
        "question": "What role do nan values play in data type errors in BigQuery?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 287,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you encounter data type error on trip_type column, it may due to some nan values that isn’t null in bigquery.\nSolution: try casting it to FLOAT datatype instead of NUMERIC",
        "question": "What is the suggested solution for a data type error in the trip_type column?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 287,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you encounter data type error on trip_type column, it may due to some nan values that isn’t null in bigquery.\nSolution: try casting it to FLOAT datatype instead of NUMERIC",
        "question": "Which datatype should be used to cast the trip_type column if there is a data type error?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 287,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you encounter data type error on trip_type column, it may due to some nan values that isn’t null in bigquery.\nSolution: try casting it to FLOAT datatype instead of NUMERIC",
        "question": "Why might nan values not be recognized as null in BigQuery?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 287,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error could result if you are using some select * query without mentioning the name of table for ex:\nwith dim_zones as (\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`\nwhere borough != 'Unknown'\n),\nfhv as (\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`\n)\nselect * from fhv\ninner join dim_zones as pickup_zone\non fhv.PUlocationID = pickup_zone.locationid\ninner join dim_zones as dropoff_zone\non fhv.DOlocationID = dropoff_zone.locationid\n);\nTo resolve just replace use : select fhv.* from fhv",
        "question": "What is the issue caused by using a select * query in the provided SQL example?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 288,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error could result if you are using some select * query without mentioning the name of table for ex:\nwith dim_zones as (\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`\nwhere borough != 'Unknown'\n),\nfhv as (\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`\n)\nselect * from fhv\ninner join dim_zones as pickup_zone\non fhv.PUlocationID = pickup_zone.locationid\ninner join dim_zones as dropoff_zone\non fhv.DOlocationID = dropoff_zone.locationid\n);\nTo resolve just replace use : select fhv.* from fhv",
        "question": "What is the purpose of the 'with' clause in the given SQL query?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 288,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error could result if you are using some select * query without mentioning the name of table for ex:\nwith dim_zones as (\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`\nwhere borough != 'Unknown'\n),\nfhv as (\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`\n)\nselect * from fhv\ninner join dim_zones as pickup_zone\non fhv.PUlocationID = pickup_zone.locationid\ninner join dim_zones as dropoff_zone\non fhv.DOlocationID = dropoff_zone.locationid\n);\nTo resolve just replace use : select fhv.* from fhv",
        "question": "How can the error mentioned in the text be resolved?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 288,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error could result if you are using some select * query without mentioning the name of table for ex:\nwith dim_zones as (\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`\nwhere borough != 'Unknown'\n),\nfhv as (\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`\n)\nselect * from fhv\ninner join dim_zones as pickup_zone\non fhv.PUlocationID = pickup_zone.locationid\ninner join dim_zones as dropoff_zone\non fhv.DOlocationID = dropoff_zone.locationid\n);\nTo resolve just replace use : select fhv.* from fhv",
        "question": "What are the two tables being queried in the SQL example?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 288,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error could result if you are using some select * query without mentioning the name of table for ex:\nwith dim_zones as (\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`\nwhere borough != 'Unknown'\n),\nfhv as (\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`\n)\nselect * from fhv\ninner join dim_zones as pickup_zone\non fhv.PUlocationID = pickup_zone.locationid\ninner join dim_zones as dropoff_zone\non fhv.DOlocationID = dropoff_zone.locationid\n);\nTo resolve just replace use : select fhv.* from fhv",
        "question": "What does the inner join operation accomplish in the context of this SQL query?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 288,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Some ehail fees are null and casting them to integer gives Bad int64 value: 0.0 error,\nSolution:\nUsing safe_cast returns NULL instead of throwing an error. So use safe_cast from dbt_utils function in the jinja code for casting into integer as follows:\n{{ dbt_utils.safe_cast('ehail_fee',  api.Column.translate_type(\"integer\"))}} as ehail_fee,\nCan also just use safe_cast(ehail_fee as integer) without relying on dbt_utils.",
        "question": "What error occurs when null ehail fees are cast to integer?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 289,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Some ehail fees are null and casting them to integer gives Bad int64 value: 0.0 error,\nSolution:\nUsing safe_cast returns NULL instead of throwing an error. So use safe_cast from dbt_utils function in the jinja code for casting into integer as follows:\n{{ dbt_utils.safe_cast('ehail_fee',  api.Column.translate_type(\"integer\"))}} as ehail_fee,\nCan also just use safe_cast(ehail_fee as integer) without relying on dbt_utils.",
        "question": "What solution is proposed for handling null ehail fees in the code?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 289,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Some ehail fees are null and casting them to integer gives Bad int64 value: 0.0 error,\nSolution:\nUsing safe_cast returns NULL instead of throwing an error. So use safe_cast from dbt_utils function in the jinja code for casting into integer as follows:\n{{ dbt_utils.safe_cast('ehail_fee',  api.Column.translate_type(\"integer\"))}} as ehail_fee,\nCan also just use safe_cast(ehail_fee as integer) without relying on dbt_utils.",
        "question": "How does safe_cast function differ from a regular cast in this context?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 289,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Some ehail fees are null and casting them to integer gives Bad int64 value: 0.0 error,\nSolution:\nUsing safe_cast returns NULL instead of throwing an error. So use safe_cast from dbt_utils function in the jinja code for casting into integer as follows:\n{{ dbt_utils.safe_cast('ehail_fee',  api.Column.translate_type(\"integer\"))}} as ehail_fee,\nCan also just use safe_cast(ehail_fee as integer) without relying on dbt_utils.",
        "question": "What is the significance of using dbt_utils for casting in Jinja code?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 289,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Some ehail fees are null and casting them to integer gives Bad int64 value: 0.0 error,\nSolution:\nUsing safe_cast returns NULL instead of throwing an error. So use safe_cast from dbt_utils function in the jinja code for casting into integer as follows:\n{{ dbt_utils.safe_cast('ehail_fee',  api.Column.translate_type(\"integer\"))}} as ehail_fee,\nCan also just use safe_cast(ehail_fee as integer) without relying on dbt_utils.",
        "question": "Can safe_cast be used without dbt_utils? If so, how?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 289,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You might encounter this when building the fact_trips.sql model. The issue may be with the payment_type_description field.\nUsing safe_cast as above, would cause the entire field to become null. A better approach is to drop the offending decimal place, then cast to integer.\ncast(replace({{ payment_type }},'.0','') as integer)\nBad int64 value: 1.0 error (again)\n\nI found that there are more columns causing the bad INT64: ratecodeid and trip_type on Green_tripdata table.\nYou can use the queries below to address them:\nCAST(\nREGEXP_REPLACE(CAST(rate_code AS STRING), r'\\.0', '') AS INT64\n) AS ratecodeid,\nCAST(\nCASE\nWHEN REGEXP_CONTAINS(CAST(trip_type AS STRING), r'\\.\\d+') THEN NULL\nELSE CAST(trip_type AS INT64)\nEND AS INT64\n) AS trip_type,",
        "question": "What issue might be encountered when building the fact_trips.sql model?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 290,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You might encounter this when building the fact_trips.sql model. The issue may be with the payment_type_description field.\nUsing safe_cast as above, would cause the entire field to become null. A better approach is to drop the offending decimal place, then cast to integer.\ncast(replace({{ payment_type }},'.0','') as integer)\nBad int64 value: 1.0 error (again)\n\nI found that there are more columns causing the bad INT64: ratecodeid and trip_type on Green_tripdata table.\nYou can use the queries below to address them:\nCAST(\nREGEXP_REPLACE(CAST(rate_code AS STRING), r'\\.0', '') AS INT64\n) AS ratecodeid,\nCAST(\nCASE\nWHEN REGEXP_CONTAINS(CAST(trip_type AS STRING), r'\\.\\d+') THEN NULL\nELSE CAST(trip_type AS INT64)\nEND AS INT64\n) AS trip_type,",
        "question": "How can the payment_type_description field be correctly cast to avoid null values?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 290,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You might encounter this when building the fact_trips.sql model. The issue may be with the payment_type_description field.\nUsing safe_cast as above, would cause the entire field to become null. A better approach is to drop the offending decimal place, then cast to integer.\ncast(replace({{ payment_type }},'.0','') as integer)\nBad int64 value: 1.0 error (again)\n\nI found that there are more columns causing the bad INT64: ratecodeid and trip_type on Green_tripdata table.\nYou can use the queries below to address them:\nCAST(\nREGEXP_REPLACE(CAST(rate_code AS STRING), r'\\.0', '') AS INT64\n) AS ratecodeid,\nCAST(\nCASE\nWHEN REGEXP_CONTAINS(CAST(trip_type AS STRING), r'\\.\\d+') THEN NULL\nELSE CAST(trip_type AS INT64)\nEND AS INT64\n) AS trip_type,",
        "question": "What method is suggested for addressing bad INT64 values in the ratecodeid column?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 290,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You might encounter this when building the fact_trips.sql model. The issue may be with the payment_type_description field.\nUsing safe_cast as above, would cause the entire field to become null. A better approach is to drop the offending decimal place, then cast to integer.\ncast(replace({{ payment_type }},'.0','') as integer)\nBad int64 value: 1.0 error (again)\n\nI found that there are more columns causing the bad INT64: ratecodeid and trip_type on Green_tripdata table.\nYou can use the queries below to address them:\nCAST(\nREGEXP_REPLACE(CAST(rate_code AS STRING), r'\\.0', '') AS INT64\n) AS ratecodeid,\nCAST(\nCASE\nWHEN REGEXP_CONTAINS(CAST(trip_type AS STRING), r'\\.\\d+') THEN NULL\nELSE CAST(trip_type AS INT64)\nEND AS INT64\n) AS trip_type,",
        "question": "How does the provided SQL query handle the trip_type column to prevent errors?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 290,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You might encounter this when building the fact_trips.sql model. The issue may be with the payment_type_description field.\nUsing safe_cast as above, would cause the entire field to become null. A better approach is to drop the offending decimal place, then cast to integer.\ncast(replace({{ payment_type }},'.0','') as integer)\nBad int64 value: 1.0 error (again)\n\nI found that there are more columns causing the bad INT64: ratecodeid and trip_type on Green_tripdata table.\nYou can use the queries below to address them:\nCAST(\nREGEXP_REPLACE(CAST(rate_code AS STRING), r'\\.0', '') AS INT64\n) AS ratecodeid,\nCAST(\nCASE\nWHEN REGEXP_CONTAINS(CAST(trip_type AS STRING), r'\\.\\d+') THEN NULL\nELSE CAST(trip_type AS INT64)\nEND AS INT64\n) AS trip_type,",
        "question": "What regular expression is used to identify problematic decimal places in the columns?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 290,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The two solution above don’t work for me - I used the line below in `stg_green_trips.sql` to replace the original ehail_fee line:\n`{{ dbt.safe_cast('ehail_fee',  api.Column.translate_type(\"numeric\"))}} as ehail_fee,`",
        "question": "What was the issue the author faced with the initial solutions?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 291,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The two solution above don’t work for me - I used the line below in `stg_green_trips.sql` to replace the original ehail_fee line:\n`{{ dbt.safe_cast('ehail_fee',  api.Column.translate_type(\"numeric\"))}} as ehail_fee,`",
        "question": "Which SQL file did the author modify?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 291,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The two solution above don’t work for me - I used the line below in `stg_green_trips.sql` to replace the original ehail_fee line:\n`{{ dbt.safe_cast('ehail_fee',  api.Column.translate_type(\"numeric\"))}} as ehail_fee,`",
        "question": "What line of code did the author use to replace the original ehail_fee line?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 291,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The two solution above don’t work for me - I used the line below in `stg_green_trips.sql` to replace the original ehail_fee line:\n`{{ dbt.safe_cast('ehail_fee',  api.Column.translate_type(\"numeric\"))}} as ehail_fee,`",
        "question": "What function is used to safely cast the 'ehail_fee' in the author's solution?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 291,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The two solution above don’t work for me - I used the line below in `stg_green_trips.sql` to replace the original ehail_fee line:\n`{{ dbt.safe_cast('ehail_fee',  api.Column.translate_type(\"numeric\"))}} as ehail_fee,`",
        "question": "What data type is the 'ehail_fee' being cast to in the provided line of code?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 291,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remember to add a space between the variable and the value. Otherwise, it won't be interpreted as a dictionary.\nIt should be:\ndbt run --var 'is_test_run: false'",
        "question": "What is the importance of adding a space between the variable and the value?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 292,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remember to add a space between the variable and the value. Otherwise, it won't be interpreted as a dictionary.\nIt should be:\ndbt run --var 'is_test_run: false'",
        "question": "How should the command be formatted to correctly interpret it as a dictionary?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 292,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remember to add a space between the variable and the value. Otherwise, it won't be interpreted as a dictionary.\nIt should be:\ndbt run --var 'is_test_run: false'",
        "question": "What does the command 'dbt run --var' do?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 292,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remember to add a space between the variable and the value. Otherwise, it won't be interpreted as a dictionary.\nIt should be:\ndbt run --var 'is_test_run: false'",
        "question": "What is the significance of the value 'is_test_run: false'?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 292,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Remember to add a space between the variable and the value. Otherwise, it won't be interpreted as a dictionary.\nIt should be:\ndbt run --var 'is_test_run: false'",
        "question": "What might happen if there is no space between the variable and the value?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 292,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You don't need to change the environment type. If you are following the videos, you are creating a Production Deployment, so the only available option is the correct one.'",
        "question": "What type of deployment are you creating when following the videos?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 293,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You don't need to change the environment type. If you are following the videos, you are creating a Production Deployment, so the only available option is the correct one.'",
        "question": "Is it necessary to change the environment type?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 293,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You don't need to change the environment type. If you are following the videos, you are creating a Production Deployment, so the only available option is the correct one.'",
        "question": "What is the only available option if you are creating a Production Deployment?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 293,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You don't need to change the environment type. If you are following the videos, you are creating a Production Deployment, so the only available option is the correct one.'",
        "question": "What should you do if you are unsure about the deployment type?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 293,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You don't need to change the environment type. If you are following the videos, you are creating a Production Deployment, so the only available option is the correct one.'",
        "question": "Are there multiple options available for a Production Deployment?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 293,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Database Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\nAccess Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.\ncompiled Code at target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\nIn my case, I was set up in a different branch, so always check the branch you are working on. Change the 04-analytics-engineering/taxi_rides_ny/models/staging/schema.yml file in the\nsources:\n- name: staging\ndatabase: your_database_name\nIf this error will continue when running dbt job, As for changing the branch for your job, you can use the ‘Custom Branch’ settings in your dbt Cloud environment. This allows you to run your job on a different branch than the default one (usually main). To do this, you need to:\nGo to an environment and select Settings to edit it\nSelect Only run on a custom branch in General settings\nEnter the name of your custom branch (e.g. HW)\nClick Save\nCould not parse the dbt project. please check that the repository contains a valid dbt project\nRunning the Environment on the master branch causes this error, you must activate “Only run on a custom branch” checkbox and specify the branch you are  working when Environment is setup.",
        "question": "What is the error encountered in the model stg_yellow_tripdata?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 294,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Database Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\nAccess Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.\ncompiled Code at target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\nIn my case, I was set up in a different branch, so always check the branch you are working on. Change the 04-analytics-engineering/taxi_rides_ny/models/staging/schema.yml file in the\nsources:\n- name: staging\ndatabase: your_database_name\nIf this error will continue when running dbt job, As for changing the branch for your job, you can use the ‘Custom Branch’ settings in your dbt Cloud environment. This allows you to run your job on a different branch than the default one (usually main). To do this, you need to:\nGo to an environment and select Settings to edit it\nSelect Only run on a custom branch in General settings\nEnter the name of your custom branch (e.g. HW)\nClick Save\nCould not parse the dbt project. please check that the repository contains a valid dbt project\nRunning the Environment on the master branch causes this error, you must activate “Only run on a custom branch” checkbox and specify the branch you are  working when Environment is setup.",
        "question": "What are the possible reasons for the 'Access Denied' error?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 294,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Database Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\nAccess Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.\ncompiled Code at target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\nIn my case, I was set up in a different branch, so always check the branch you are working on. Change the 04-analytics-engineering/taxi_rides_ny/models/staging/schema.yml file in the\nsources:\n- name: staging\ndatabase: your_database_name\nIf this error will continue when running dbt job, As for changing the branch for your job, you can use the ‘Custom Branch’ settings in your dbt Cloud environment. This allows you to run your job on a different branch than the default one (usually main). To do this, you need to:\nGo to an environment and select Settings to edit it\nSelect Only run on a custom branch in General settings\nEnter the name of your custom branch (e.g. HW)\nClick Save\nCould not parse the dbt project. please check that the repository contains a valid dbt project\nRunning the Environment on the master branch causes this error, you must activate “Only run on a custom branch” checkbox and specify the branch you are  working when Environment is setup.",
        "question": "How can you change the branch for your dbt job in the Cloud environment?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 294,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Database Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\nAccess Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.\ncompiled Code at target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\nIn my case, I was set up in a different branch, so always check the branch you are working on. Change the 04-analytics-engineering/taxi_rides_ny/models/staging/schema.yml file in the\nsources:\n- name: staging\ndatabase: your_database_name\nIf this error will continue when running dbt job, As for changing the branch for your job, you can use the ‘Custom Branch’ settings in your dbt Cloud environment. This allows you to run your job on a different branch than the default one (usually main). To do this, you need to:\nGo to an environment and select Settings to edit it\nSelect Only run on a custom branch in General settings\nEnter the name of your custom branch (e.g. HW)\nClick Save\nCould not parse the dbt project. please check that the repository contains a valid dbt project\nRunning the Environment on the master branch causes this error, you must activate “Only run on a custom branch” checkbox and specify the branch you are  working when Environment is setup.",
        "question": "What file needs to be changed to update the database name in the staging model?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 294,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Database Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\nAccess Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.\ncompiled Code at target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\nIn my case, I was set up in a different branch, so always check the branch you are working on. Change the 04-analytics-engineering/taxi_rides_ny/models/staging/schema.yml file in the\nsources:\n- name: staging\ndatabase: your_database_name\nIf this error will continue when running dbt job, As for changing the branch for your job, you can use the ‘Custom Branch’ settings in your dbt Cloud environment. This allows you to run your job on a different branch than the default one (usually main). To do this, you need to:\nGo to an environment and select Settings to edit it\nSelect Only run on a custom branch in General settings\nEnter the name of your custom branch (e.g. HW)\nClick Save\nCould not parse the dbt project. please check that the repository contains a valid dbt project\nRunning the Environment on the master branch causes this error, you must activate “Only run on a custom branch” checkbox and specify the branch you are  working when Environment is setup.",
        "question": "What should you do if the dbt project cannot be parsed?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 294,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change to main branch, make a pull request from the development branch.\nNote: this will take you to github.\nApprove the merging and rerun you job, it would work as planned now",
        "question": "What is the first step in the process described in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 295,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change to main branch, make a pull request from the development branch.\nNote: this will take you to github.\nApprove the merging and rerun you job, it would work as planned now",
        "question": "What should you do after changing to the main branch?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 295,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change to main branch, make a pull request from the development branch.\nNote: this will take you to github.\nApprove the merging and rerun you job, it would work as planned now",
        "question": "Which platform will you be directed to when making a pull request?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 295,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change to main branch, make a pull request from the development branch.\nNote: this will take you to github.\nApprove the merging and rerun you job, it would work as planned now",
        "question": "What action must you take after approving the merging?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 295,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change to main branch, make a pull request from the development branch.\nNote: this will take you to github.\nApprove the merging and rerun you job, it would work as planned now",
        "question": "What is expected to happen after rerunning your job?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 295,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Before you can develop some data model on dbt, you should create development environment and set some parameter on it. After the model being developed, we should also create deployment environment to create and run some jobs.",
        "question": "What is the first step before developing a data model on dbt?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 296,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Before you can develop some data model on dbt, you should create development environment and set some parameter on it. After the model being developed, we should also create deployment environment to create and run some jobs.",
        "question": "What type of environment should you create for development in dbt?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 296,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Before you can develop some data model on dbt, you should create development environment and set some parameter on it. After the model being developed, we should also create deployment environment to create and run some jobs.",
        "question": "What should be configured in the development environment before starting the data model?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 296,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Before you can develop some data model on dbt, you should create development environment and set some parameter on it. After the model being developed, we should also create deployment environment to create and run some jobs.",
        "question": "What is the purpose of creating a deployment environment?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 296,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Before you can develop some data model on dbt, you should create development environment and set some parameter on it. After the model being developed, we should also create deployment environment to create and run some jobs.",
        "question": "What actions can be performed in the deployment environment after developing the model?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 296,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error Message:\nInvestigate Sentry error: ProtocolError \"Invalid input ConnectionInputs.SEND_HEADERS in state ConnectionState.CLOSED\"\nSolution:\nreference\nRun it again because it happens sometimes. Or wait a few minutes, it will continue.",
        "question": "What is the error message mentioned in the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 297,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error Message:\nInvestigate Sentry error: ProtocolError \"Invalid input ConnectionInputs.SEND_HEADERS in state ConnectionState.CLOSED\"\nSolution:\nreference\nRun it again because it happens sometimes. Or wait a few minutes, it will continue.",
        "question": "What is the specific protocol error identified?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 297,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error Message:\nInvestigate Sentry error: ProtocolError \"Invalid input ConnectionInputs.SEND_HEADERS in state ConnectionState.CLOSED\"\nSolution:\nreference\nRun it again because it happens sometimes. Or wait a few minutes, it will continue.",
        "question": "What is the recommended solution for the error?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 297,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error Message:\nInvestigate Sentry error: ProtocolError \"Invalid input ConnectionInputs.SEND_HEADERS in state ConnectionState.CLOSED\"\nSolution:\nreference\nRun it again because it happens sometimes. Or wait a few minutes, it will continue.",
        "question": "How often does the error occur according to the text?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 297,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error Message:\nInvestigate Sentry error: ProtocolError \"Invalid input ConnectionInputs.SEND_HEADERS in state ConnectionState.CLOSED\"\nSolution:\nreference\nRun it again because it happens sometimes. Or wait a few minutes, it will continue.",
        "question": "What action can be taken if the error persists?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 297,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data.\nWhen I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.",
        "question": "What script was used to load taxi data into GCS?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 298,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data.\nWhen I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.",
        "question": "What data format was the taxi data converted into?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 298,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data.\nWhen I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.",
        "question": "What error message was received when executing dbt run?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 298,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data.\nWhen I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.",
        "question": "What transformation was applied to resolve the error regarding column types?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 298,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data.\nWhen I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.",
        "question": "Where can more information about the encountered error be found?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 298,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use the syntax below instead if the code in the tutorial is not working.\ndbt run --select stg_green_tripdata --vars '{\"is_test_run\": false}'",
        "question": "What command should be used if the tutorial code is not functioning properly?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 299,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use the syntax below instead if the code in the tutorial is not working.\ndbt run --select stg_green_tripdata --vars '{\"is_test_run\": false}'",
        "question": "What does the '--select' option specify in the dbt run command?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 299,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use the syntax below instead if the code in the tutorial is not working.\ndbt run --select stg_green_tripdata --vars '{\"is_test_run\": false}'",
        "question": "What is the purpose of the '--vars' option in the dbt run command?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 299,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use the syntax below instead if the code in the tutorial is not working.\ndbt run --select stg_green_tripdata --vars '{\"is_test_run\": false}'",
        "question": "What value is being assigned to 'is_test_run' in the provided code?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 299,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use the syntax below instead if the code in the tutorial is not working.\ndbt run --select stg_green_tripdata --vars '{\"is_test_run\": false}'",
        "question": "What is the significance of setting 'is_test_run' to false?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 299,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\nSolution:\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`",
        "question": "What command is used to build the Docker containers?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 300,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\nSolution:\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`",
        "question": "What error is encountered after running the dbt-bq-dtc init command?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 300,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\nSolution:\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`",
        "question": "Which Python module needs to be installed to resolve the encountered error?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 300,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\nSolution:\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`",
        "question": "Where should the installation command for the missing module be added in the Dockerfile?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 300,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\nSolution:\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`",
        "question": "What is the base image used in the Dockerfile for this setup?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 300,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you have problems editing dbt_project.yml when using Docker after ‘docker-compose run dbt-bq-dtc init’, to change profile ‘taxi_rides_ny’ to 'bq-dbt-workshop’, just run:\nsudo chown -R username path\nDBT - Internal Error: Profile should not be None if loading is completed\nWhen  running dbt debug, change the directory to the newly created subdirectory (e.g: the newly created `taxi_rides_ny` directory, which contains the dbt project).",
        "question": "What command is suggested to change the profile from 'taxi_rides_ny' to 'bq-dbt-workshop'?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 301,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you have problems editing dbt_project.yml when using Docker after ‘docker-compose run dbt-bq-dtc init’, to change profile ‘taxi_rides_ny’ to 'bq-dbt-workshop’, just run:\nsudo chown -R username path\nDBT - Internal Error: Profile should not be None if loading is completed\nWhen  running dbt debug, change the directory to the newly created subdirectory (e.g: the newly created `taxi_rides_ny` directory, which contains the dbt project).",
        "question": "What error occurs if the profile is None after loading is completed?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 301,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you have problems editing dbt_project.yml when using Docker after ‘docker-compose run dbt-bq-dtc init’, to change profile ‘taxi_rides_ny’ to 'bq-dbt-workshop’, just run:\nsudo chown -R username path\nDBT - Internal Error: Profile should not be None if loading is completed\nWhen  running dbt debug, change the directory to the newly created subdirectory (e.g: the newly created `taxi_rides_ny` directory, which contains the dbt project).",
        "question": "Which command should you use to change ownership of a directory in Docker?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 301,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you have problems editing dbt_project.yml when using Docker after ‘docker-compose run dbt-bq-dtc init’, to change profile ‘taxi_rides_ny’ to 'bq-dbt-workshop’, just run:\nsudo chown -R username path\nDBT - Internal Error: Profile should not be None if loading is completed\nWhen  running dbt debug, change the directory to the newly created subdirectory (e.g: the newly created `taxi_rides_ny` directory, which contains the dbt project).",
        "question": "What must you do before running 'dbt debug' in the newly created subdirectory?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 301,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you have problems editing dbt_project.yml when using Docker after ‘docker-compose run dbt-bq-dtc init’, to change profile ‘taxi_rides_ny’ to 'bq-dbt-workshop’, just run:\nsudo chown -R username path\nDBT - Internal Error: Profile should not be None if loading is completed\nWhen  running dbt debug, change the directory to the newly created subdirectory (e.g: the newly created `taxi_rides_ny` directory, which contains the dbt project).",
        "question": "What is the name of the directory created during the dbt project initialization?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 301,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When running a query on BigQuery sometimes could appear a this table is not on the specified location error.\nFor this problem there is not a straightforward solution, you need to dig a little, but the problem could be one of these:\nCheck the locations of your bucket, datasets and tables. Make sure they are all on the same one.\nChange the query settings to the location you are in: on the query window select more -> query settings -> select the location\nCheck if all the paths you are using in your query to your tables are correct: you can click on the table -> details -> and copy the path.",
        "question": "What error might occur when running a query on BigQuery?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 302,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When running a query on BigQuery sometimes could appear a this table is not on the specified location error.\nFor this problem there is not a straightforward solution, you need to dig a little, but the problem could be one of these:\nCheck the locations of your bucket, datasets and tables. Make sure they are all on the same one.\nChange the query settings to the location you are in: on the query window select more -> query settings -> select the location\nCheck if all the paths you are using in your query to your tables are correct: you can click on the table -> details -> and copy the path.",
        "question": "What are some potential causes of the 'this table is not on the specified location' error?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 302,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When running a query on BigQuery sometimes could appear a this table is not on the specified location error.\nFor this problem there is not a straightforward solution, you need to dig a little, but the problem could be one of these:\nCheck the locations of your bucket, datasets and tables. Make sure they are all on the same one.\nChange the query settings to the location you are in: on the query window select more -> query settings -> select the location\nCheck if all the paths you are using in your query to your tables are correct: you can click on the table -> details -> and copy the path.",
        "question": "How can you check the locations of your bucket, datasets, and tables in BigQuery?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 302,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When running a query on BigQuery sometimes could appear a this table is not on the specified location error.\nFor this problem there is not a straightforward solution, you need to dig a little, but the problem could be one of these:\nCheck the locations of your bucket, datasets and tables. Make sure they are all on the same one.\nChange the query settings to the location you are in: on the query window select more -> query settings -> select the location\nCheck if all the paths you are using in your query to your tables are correct: you can click on the table -> details -> and copy the path.",
        "question": "Where can you find the option to change the query settings location in BigQuery?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 302,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When running a query on BigQuery sometimes could appear a this table is not on the specified location error.\nFor this problem there is not a straightforward solution, you need to dig a little, but the problem could be one of these:\nCheck the locations of your bucket, datasets and tables. Make sure they are all on the same one.\nChange the query settings to the location you are in: on the query window select more -> query settings -> select the location\nCheck if all the paths you are using in your query to your tables are correct: you can click on the table -> details -> and copy the path.",
        "question": "What steps should you take to verify the paths used in your query to your tables?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 302,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens because we have moved the dbt project to another directory on our repo.\nOr might be that you’re on a different branch than is expected to be merged from / to.\nSolution:\nGo to the projects window on dbt cloud -> settings -> edit -> and add directory (the extra path to the dbt project)\nFor example:\n/week5/taxi_rides_ny\nMake sure your file explorer path and this Project settings path matches and there’s no files waiting to be committed to github if you’re running the job to deploy to PROD.\nAnd that you had setup the PROD environment to check in the main branch, or whichever you specified.\nIn the picture below, I had set it to ella2024 to be checked as “production-ready” by the “freshness” check mark at the PROD environment settings. So each time I merge a branch from something else into ella2024 and then trigger the PR, the CI check job would kick-in. But we still do need to Merge and close the PR manually, I believe, that part is not automated.\nYou set up the PROD custom branch (if not default main) in the Environment setup screen.",
        "question": "What is one possible reason for issues after moving a dbt project?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 303,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens because we have moved the dbt project to another directory on our repo.\nOr might be that you’re on a different branch than is expected to be merged from / to.\nSolution:\nGo to the projects window on dbt cloud -> settings -> edit -> and add directory (the extra path to the dbt project)\nFor example:\n/week5/taxi_rides_ny\nMake sure your file explorer path and this Project settings path matches and there’s no files waiting to be committed to github if you’re running the job to deploy to PROD.\nAnd that you had setup the PROD environment to check in the main branch, or whichever you specified.\nIn the picture below, I had set it to ella2024 to be checked as “production-ready” by the “freshness” check mark at the PROD environment settings. So each time I merge a branch from something else into ella2024 and then trigger the PR, the CI check job would kick-in. But we still do need to Merge and close the PR manually, I believe, that part is not automated.\nYou set up the PROD custom branch (if not default main) in the Environment setup screen.",
        "question": "How can you add a directory to the dbt project in dbt cloud?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 303,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens because we have moved the dbt project to another directory on our repo.\nOr might be that you’re on a different branch than is expected to be merged from / to.\nSolution:\nGo to the projects window on dbt cloud -> settings -> edit -> and add directory (the extra path to the dbt project)\nFor example:\n/week5/taxi_rides_ny\nMake sure your file explorer path and this Project settings path matches and there’s no files waiting to be committed to github if you’re running the job to deploy to PROD.\nAnd that you had setup the PROD environment to check in the main branch, or whichever you specified.\nIn the picture below, I had set it to ella2024 to be checked as “production-ready” by the “freshness” check mark at the PROD environment settings. So each time I merge a branch from something else into ella2024 and then trigger the PR, the CI check job would kick-in. But we still do need to Merge and close the PR manually, I believe, that part is not automated.\nYou set up the PROD custom branch (if not default main) in the Environment setup screen.",
        "question": "What must be ensured regarding the file explorer path and the Project settings path?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 303,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens because we have moved the dbt project to another directory on our repo.\nOr might be that you’re on a different branch than is expected to be merged from / to.\nSolution:\nGo to the projects window on dbt cloud -> settings -> edit -> and add directory (the extra path to the dbt project)\nFor example:\n/week5/taxi_rides_ny\nMake sure your file explorer path and this Project settings path matches and there’s no files waiting to be committed to github if you’re running the job to deploy to PROD.\nAnd that you had setup the PROD environment to check in the main branch, or whichever you specified.\nIn the picture below, I had set it to ella2024 to be checked as “production-ready” by the “freshness” check mark at the PROD environment settings. So each time I merge a branch from something else into ella2024 and then trigger the PR, the CI check job would kick-in. But we still do need to Merge and close the PR manually, I believe, that part is not automated.\nYou set up the PROD custom branch (if not default main) in the Environment setup screen.",
        "question": "What does the 'freshness' check mark indicate in the PROD environment settings?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 303,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This happens because we have moved the dbt project to another directory on our repo.\nOr might be that you’re on a different branch than is expected to be merged from / to.\nSolution:\nGo to the projects window on dbt cloud -> settings -> edit -> and add directory (the extra path to the dbt project)\nFor example:\n/week5/taxi_rides_ny\nMake sure your file explorer path and this Project settings path matches and there’s no files waiting to be committed to github if you’re running the job to deploy to PROD.\nAnd that you had setup the PROD environment to check in the main branch, or whichever you specified.\nIn the picture below, I had set it to ella2024 to be checked as “production-ready” by the “freshness” check mark at the PROD environment settings. So each time I merge a branch from something else into ella2024 and then trigger the PR, the CI check job would kick-in. But we still do need to Merge and close the PR manually, I believe, that part is not automated.\nYou set up the PROD custom branch (if not default main) in the Environment setup screen.",
        "question": "Is the process of merging and closing a pull request automated in the described setup?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 303,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you are creating the pull request and running the CI, dbt is creating a new schema on BIgQuery. By default that new schema will be created on ‘US’ location, if you have your dataset, schemas and tables on ‘EU’ that will generate an error and the pull request will not be accepted. To change that location to ‘EU’ on the connection to BigQuery from dbt we need to add the location ‘EU’ on the connection optional settings:\nDbt -> project -> settings -> connection BIgQuery -> OPtional Settings -> Location -> EU",
        "question": "What happens when a new schema is created on BigQuery during a pull request in dbt?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 304,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you are creating the pull request and running the CI, dbt is creating a new schema on BIgQuery. By default that new schema will be created on ‘US’ location, if you have your dataset, schemas and tables on ‘EU’ that will generate an error and the pull request will not be accepted. To change that location to ‘EU’ on the connection to BigQuery from dbt we need to add the location ‘EU’ on the connection optional settings:\nDbt -> project -> settings -> connection BIgQuery -> OPtional Settings -> Location -> EU",
        "question": "What is the default location for new schemas created by dbt on BigQuery?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 304,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you are creating the pull request and running the CI, dbt is creating a new schema on BIgQuery. By default that new schema will be created on ‘US’ location, if you have your dataset, schemas and tables on ‘EU’ that will generate an error and the pull request will not be accepted. To change that location to ‘EU’ on the connection to BigQuery from dbt we need to add the location ‘EU’ on the connection optional settings:\nDbt -> project -> settings -> connection BIgQuery -> OPtional Settings -> Location -> EU",
        "question": "What error may occur if your dataset is on 'EU' when the schema is created in 'US'?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 304,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you are creating the pull request and running the CI, dbt is creating a new schema on BIgQuery. By default that new schema will be created on ‘US’ location, if you have your dataset, schemas and tables on ‘EU’ that will generate an error and the pull request will not be accepted. To change that location to ‘EU’ on the connection to BigQuery from dbt we need to add the location ‘EU’ on the connection optional settings:\nDbt -> project -> settings -> connection BIgQuery -> OPtional Settings -> Location -> EU",
        "question": "How can you change the location of the BigQuery connection in dbt to 'EU'?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 304,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When you are creating the pull request and running the CI, dbt is creating a new schema on BIgQuery. By default that new schema will be created on ‘US’ location, if you have your dataset, schemas and tables on ‘EU’ that will generate an error and the pull request will not be accepted. To change that location to ‘EU’ on the connection to BigQuery from dbt we need to add the location ‘EU’ on the connection optional settings:\nDbt -> project -> settings -> connection BIgQuery -> OPtional Settings -> Location -> EU",
        "question": "Where in the dbt project settings can you find the option to change the BigQuery connection location?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 304,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When running trying to run the dbt project on prod there is some things you need to do and check on your own:\nFirst Make the pull request and Merge the branch into the main.\nMake sure you have the latest version, if you made changes to the repo in another place.\nCheck if the dbt_project.yml file is accessible to the project, if not check this solution (Dbt: This dbt Cloud run was cancelled because a valid dbt project was not found.).\nCheck if the name you gave to the dataset on BigQuery is the same you put on the dataset spot on the production environment created on dbt cloud.",
        "question": "What is the first step to run the dbt project on production?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 305,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When running trying to run the dbt project on prod there is some things you need to do and check on your own:\nFirst Make the pull request and Merge the branch into the main.\nMake sure you have the latest version, if you made changes to the repo in another place.\nCheck if the dbt_project.yml file is accessible to the project, if not check this solution (Dbt: This dbt Cloud run was cancelled because a valid dbt project was not found.).\nCheck if the name you gave to the dataset on BigQuery is the same you put on the dataset spot on the production environment created on dbt cloud.",
        "question": "Why is it important to check if the dbt_project.yml file is accessible?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 305,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When running trying to run the dbt project on prod there is some things you need to do and check on your own:\nFirst Make the pull request and Merge the branch into the main.\nMake sure you have the latest version, if you made changes to the repo in another place.\nCheck if the dbt_project.yml file is accessible to the project, if not check this solution (Dbt: This dbt Cloud run was cancelled because a valid dbt project was not found.).\nCheck if the name you gave to the dataset on BigQuery is the same you put on the dataset spot on the production environment created on dbt cloud.",
        "question": "What should you do if you made changes to the repo in another place?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 305,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When running trying to run the dbt project on prod there is some things you need to do and check on your own:\nFirst Make the pull request and Merge the branch into the main.\nMake sure you have the latest version, if you made changes to the repo in another place.\nCheck if the dbt_project.yml file is accessible to the project, if not check this solution (Dbt: This dbt Cloud run was cancelled because a valid dbt project was not found.).\nCheck if the name you gave to the dataset on BigQuery is the same you put on the dataset spot on the production environment created on dbt cloud.",
        "question": "How can you ensure the dataset name on BigQuery matches with dbt cloud?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 305,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When running trying to run the dbt project on prod there is some things you need to do and check on your own:\nFirst Make the pull request and Merge the branch into the main.\nMake sure you have the latest version, if you made changes to the repo in another place.\nCheck if the dbt_project.yml file is accessible to the project, if not check this solution (Dbt: This dbt Cloud run was cancelled because a valid dbt project was not found.).\nCheck if the name you gave to the dataset on BigQuery is the same you put on the dataset spot on the production environment created on dbt cloud.",
        "question": "What might cause a dbt Cloud run to be cancelled?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 305,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the step in this video (DE Zoomcamp 4.3.1 - Build the First dbt Models), after creating `stg_green_tripdata.sql` and clicking `build`, I encountered an error saying dataset not found in location EU. The default location for dbt Bigquery is the US, so when generating the new Bigquery schema for dbt, unless specified, the schema locates in the US.\nSolution:\nTurns out I forgot to specify Location to be `EU` when adding connection details.\nDevelop -> Configure Cloud CLI -> Projects -> taxi_rides_ny -> (connection) Bigquery -> Edit -> Location (Optional) -> type `EU` -> Save",
        "question": "What error did the author encounter after clicking 'build' on the dbt model?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 306,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the step in this video (DE Zoomcamp 4.3.1 - Build the First dbt Models), after creating `stg_green_tripdata.sql` and clicking `build`, I encountered an error saying dataset not found in location EU. The default location for dbt Bigquery is the US, so when generating the new Bigquery schema for dbt, unless specified, the schema locates in the US.\nSolution:\nTurns out I forgot to specify Location to be `EU` when adding connection details.\nDevelop -> Configure Cloud CLI -> Projects -> taxi_rides_ny -> (connection) Bigquery -> Edit -> Location (Optional) -> type `EU` -> Save",
        "question": "What is the default location for dbt Bigquery schemas?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 306,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the step in this video (DE Zoomcamp 4.3.1 - Build the First dbt Models), after creating `stg_green_tripdata.sql` and clicking `build`, I encountered an error saying dataset not found in location EU. The default location for dbt Bigquery is the US, so when generating the new Bigquery schema for dbt, unless specified, the schema locates in the US.\nSolution:\nTurns out I forgot to specify Location to be `EU` when adding connection details.\nDevelop -> Configure Cloud CLI -> Projects -> taxi_rides_ny -> (connection) Bigquery -> Edit -> Location (Optional) -> type `EU` -> Save",
        "question": "What specific location did the author need to specify to resolve the error?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 306,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the step in this video (DE Zoomcamp 4.3.1 - Build the First dbt Models), after creating `stg_green_tripdata.sql` and clicking `build`, I encountered an error saying dataset not found in location EU. The default location for dbt Bigquery is the US, so when generating the new Bigquery schema for dbt, unless specified, the schema locates in the US.\nSolution:\nTurns out I forgot to specify Location to be `EU` when adding connection details.\nDevelop -> Configure Cloud CLI -> Projects -> taxi_rides_ny -> (connection) Bigquery -> Edit -> Location (Optional) -> type `EU` -> Save",
        "question": "How can one change the connection details for the Bigquery in dbt?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 306,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the step in this video (DE Zoomcamp 4.3.1 - Build the First dbt Models), after creating `stg_green_tripdata.sql` and clicking `build`, I encountered an error saying dataset not found in location EU. The default location for dbt Bigquery is the US, so when generating the new Bigquery schema for dbt, unless specified, the schema locates in the US.\nSolution:\nTurns out I forgot to specify Location to be `EU` when adding connection details.\nDevelop -> Configure Cloud CLI -> Projects -> taxi_rides_ny -> (connection) Bigquery -> Edit -> Location (Optional) -> type `EU` -> Save",
        "question": "What steps are involved in configuring the Cloud CLI to set the location to EU?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 306,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue: If you’re having problems loading the FHV_20?? data from the github repo into GCS and then into BQ (input file not of type parquet), you need to do two things. First, append the URL Template link with ‘?raw=true’ like so:\nURL_TEMPLATE = URL_PREFIX + \"/fhv_tripdata_{{ execution_date.strftime(\\'%Y-%m\\') }}.parquet?raw=true\"\nSecond, update make sure the URL_PREFIX is set to the following value:\n\nURL_PREFIX = \"https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv\"\nIt is critical that you use this link with the keyword blob. If your link has ‘tree’ here, replace it. Everything else can stay the same, including the curl -sSLf command. ‘",
        "question": "What should you append to the URL Template link to resolve loading issues?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 307,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue: If you’re having problems loading the FHV_20?? data from the github repo into GCS and then into BQ (input file not of type parquet), you need to do two things. First, append the URL Template link with ‘?raw=true’ like so:\nURL_TEMPLATE = URL_PREFIX + \"/fhv_tripdata_{{ execution_date.strftime(\\'%Y-%m\\') }}.parquet?raw=true\"\nSecond, update make sure the URL_PREFIX is set to the following value:\n\nURL_PREFIX = \"https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv\"\nIt is critical that you use this link with the keyword blob. If your link has ‘tree’ here, replace it. Everything else can stay the same, including the curl -sSLf command. ‘",
        "question": "What value should the URL_PREFIX be set to for the FHV_20?? data?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 307,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue: If you’re having problems loading the FHV_20?? data from the github repo into GCS and then into BQ (input file not of type parquet), you need to do two things. First, append the URL Template link with ‘?raw=true’ like so:\nURL_TEMPLATE = URL_PREFIX + \"/fhv_tripdata_{{ execution_date.strftime(\\'%Y-%m\\') }}.parquet?raw=true\"\nSecond, update make sure the URL_PREFIX is set to the following value:\n\nURL_PREFIX = \"https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv\"\nIt is critical that you use this link with the keyword blob. If your link has ‘tree’ here, replace it. Everything else can stay the same, including the curl -sSLf command. ‘",
        "question": "Why is it important to use the keyword 'blob' in the URL?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 307,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue: If you’re having problems loading the FHV_20?? data from the github repo into GCS and then into BQ (input file not of type parquet), you need to do two things. First, append the URL Template link with ‘?raw=true’ like so:\nURL_TEMPLATE = URL_PREFIX + \"/fhv_tripdata_{{ execution_date.strftime(\\'%Y-%m\\') }}.parquet?raw=true\"\nSecond, update make sure the URL_PREFIX is set to the following value:\n\nURL_PREFIX = \"https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv\"\nIt is critical that you use this link with the keyword blob. If your link has ‘tree’ here, replace it. Everything else can stay the same, including the curl -sSLf command. ‘",
        "question": "What command can remain unchanged when adjusting the URL for loading data?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 307,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Issue: If you’re having problems loading the FHV_20?? data from the github repo into GCS and then into BQ (input file not of type parquet), you need to do two things. First, append the URL Template link with ‘?raw=true’ like so:\nURL_TEMPLATE = URL_PREFIX + \"/fhv_tripdata_{{ execution_date.strftime(\\'%Y-%m\\') }}.parquet?raw=true\"\nSecond, update make sure the URL_PREFIX is set to the following value:\n\nURL_PREFIX = \"https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv\"\nIt is critical that you use this link with the keyword blob. If your link has ‘tree’ here, replace it. Everything else can stay the same, including the curl -sSLf command. ‘",
        "question": "What is the consequence of using 'tree' instead of 'blob' in the URL?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 307,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I found out that the easies way to upload datasets form github for the homework is utilising this script git_csv_to_gcs.py. Thank you Lidia!!\nIt is similar to a script that Alexey provided us in 03-data-warehouse/extras/web_to_gcs.py",
        "question": "What is the main purpose of the script git_csv_to_gcs.py?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 308,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I found out that the easies way to upload datasets form github for the homework is utilising this script git_csv_to_gcs.py. Thank you Lidia!!\nIt is similar to a script that Alexey provided us in 03-data-warehouse/extras/web_to_gcs.py",
        "question": "Who helped the author with uploading datasets from GitHub?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 308,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I found out that the easies way to upload datasets form github for the homework is utilising this script git_csv_to_gcs.py. Thank you Lidia!!\nIt is similar to a script that Alexey provided us in 03-data-warehouse/extras/web_to_gcs.py",
        "question": "How does the git_csv_to_gcs.py script relate to the script provided by Alexey?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 308,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I found out that the easies way to upload datasets form github for the homework is utilising this script git_csv_to_gcs.py. Thank you Lidia!!\nIt is similar to a script that Alexey provided us in 03-data-warehouse/extras/web_to_gcs.py",
        "question": "Where can the script web_to_gcs.py be found?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 308,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I found out that the easies way to upload datasets form github for the homework is utilising this script git_csv_to_gcs.py. Thank you Lidia!!\nIt is similar to a script that Alexey provided us in 03-data-warehouse/extras/web_to_gcs.py",
        "question": "What does the author express gratitude for?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 308,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you have to securely put your credentials for a project and, probably, push it to a git repository then the best option is to use an environment variable\nFor example for web_to_gcs.py or git_csv_to_gcs.py we have to set these variables:\nGOOGLE_APPLICATION_CREDENTIALS\nGCP_GCS_BUCKET\nThe easises option to do it  is to use .env  (dotenv).\nInstall it and add a few lines of code that inject these variables for your project\npip install python-dotenv\nfrom dotenv import load_dotenv\nimport os\n# Load environment variables from .env file\nload_dotenv()\n# Now you can access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS\ncredentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\")",
        "question": "What are the two environment variables mentioned that need to be set for web_to_gcs.py or git_csv_to_gcs.py?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 309,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you have to securely put your credentials for a project and, probably, push it to a git repository then the best option is to use an environment variable\nFor example for web_to_gcs.py or git_csv_to_gcs.py we have to set these variables:\nGOOGLE_APPLICATION_CREDENTIALS\nGCP_GCS_BUCKET\nThe easises option to do it  is to use .env  (dotenv).\nInstall it and add a few lines of code that inject these variables for your project\npip install python-dotenv\nfrom dotenv import load_dotenv\nimport os\n# Load environment variables from .env file\nload_dotenv()\n# Now you can access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS\ncredentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\")",
        "question": "What is the purpose of using environment variables for project credentials?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 309,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you have to securely put your credentials for a project and, probably, push it to a git repository then the best option is to use an environment variable\nFor example for web_to_gcs.py or git_csv_to_gcs.py we have to set these variables:\nGOOGLE_APPLICATION_CREDENTIALS\nGCP_GCS_BUCKET\nThe easises option to do it  is to use .env  (dotenv).\nInstall it and add a few lines of code that inject these variables for your project\npip install python-dotenv\nfrom dotenv import load_dotenv\nimport os\n# Load environment variables from .env file\nload_dotenv()\n# Now you can access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS\ncredentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\")",
        "question": "How can you install the dotenv package in Python?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 309,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you have to securely put your credentials for a project and, probably, push it to a git repository then the best option is to use an environment variable\nFor example for web_to_gcs.py or git_csv_to_gcs.py we have to set these variables:\nGOOGLE_APPLICATION_CREDENTIALS\nGCP_GCS_BUCKET\nThe easises option to do it  is to use .env  (dotenv).\nInstall it and add a few lines of code that inject these variables for your project\npip install python-dotenv\nfrom dotenv import load_dotenv\nimport os\n# Load environment variables from .env file\nload_dotenv()\n# Now you can access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS\ncredentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\")",
        "question": "What is the code snippet needed to load environment variables from a .env file?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 309,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you have to securely put your credentials for a project and, probably, push it to a git repository then the best option is to use an environment variable\nFor example for web_to_gcs.py or git_csv_to_gcs.py we have to set these variables:\nGOOGLE_APPLICATION_CREDENTIALS\nGCP_GCS_BUCKET\nThe easises option to do it  is to use .env  (dotenv).\nInstall it and add a few lines of code that inject these variables for your project\npip install python-dotenv\nfrom dotenv import load_dotenv\nimport os\n# Load environment variables from .env file\nload_dotenv()\n# Now you can access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS\ncredentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\")",
        "question": "How can you access the value of the GCP_GCS_BUCKET environment variable in your code?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 309,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you uploaded manually the fvh 2019 csv files, you may face errors regarding date types. Try to create an the external table in bigquery but define the pickup_datetime and dropoff_datetime to be strings\nCREATE OR REPLACE EXTERNAL TABLE `gcp_project.trips_data_all.fhv_tripdata`  (\ndispatching_base_num STRING,\npickup_datetime STRING,\ndropoff_datetime STRING,\nPUlocationID STRING,\nDOlocationID STRING,\nSR_Flag STRING,\nAffiliated_base_number STRING\n)\nOPTIONS (\nformat = 'csv',\nuris = ['gs://bucket/*.csv']\n);\nThen when creating the fhv core model in dbt, use TIMESTAMP(CAST(()) to ensure it first parses as a string and then convert it to timestamp.\nwith fhv_tripdata as (\nselect * from {{ ref('stg_fhv_tripdata') }}\n),\ndim_zones as (\nselect * from {{ ref('dim_zones') }}\nwhere borough != 'Unknown'\n)\nselect fhv_tripdata.dispatching_base_num,\nTIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS STRING)) AS pickup_datetime,\nTIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS STRING)) AS dropoff_datetime,",
        "question": "What issues might arise when manually uploading the fvh 2019 csv files?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 310,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you uploaded manually the fvh 2019 csv files, you may face errors regarding date types. Try to create an the external table in bigquery but define the pickup_datetime and dropoff_datetime to be strings\nCREATE OR REPLACE EXTERNAL TABLE `gcp_project.trips_data_all.fhv_tripdata`  (\ndispatching_base_num STRING,\npickup_datetime STRING,\ndropoff_datetime STRING,\nPUlocationID STRING,\nDOlocationID STRING,\nSR_Flag STRING,\nAffiliated_base_number STRING\n)\nOPTIONS (\nformat = 'csv',\nuris = ['gs://bucket/*.csv']\n);\nThen when creating the fhv core model in dbt, use TIMESTAMP(CAST(()) to ensure it first parses as a string and then convert it to timestamp.\nwith fhv_tripdata as (\nselect * from {{ ref('stg_fhv_tripdata') }}\n),\ndim_zones as (\nselect * from {{ ref('dim_zones') }}\nwhere borough != 'Unknown'\n)\nselect fhv_tripdata.dispatching_base_num,\nTIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS STRING)) AS pickup_datetime,\nTIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS STRING)) AS dropoff_datetime,",
        "question": "How should the pickup_datetime and dropoff_datetime be defined when creating an external table in BigQuery?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 310,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you uploaded manually the fvh 2019 csv files, you may face errors regarding date types. Try to create an the external table in bigquery but define the pickup_datetime and dropoff_datetime to be strings\nCREATE OR REPLACE EXTERNAL TABLE `gcp_project.trips_data_all.fhv_tripdata`  (\ndispatching_base_num STRING,\npickup_datetime STRING,\ndropoff_datetime STRING,\nPUlocationID STRING,\nDOlocationID STRING,\nSR_Flag STRING,\nAffiliated_base_number STRING\n)\nOPTIONS (\nformat = 'csv',\nuris = ['gs://bucket/*.csv']\n);\nThen when creating the fhv core model in dbt, use TIMESTAMP(CAST(()) to ensure it first parses as a string and then convert it to timestamp.\nwith fhv_tripdata as (\nselect * from {{ ref('stg_fhv_tripdata') }}\n),\ndim_zones as (\nselect * from {{ ref('dim_zones') }}\nwhere borough != 'Unknown'\n)\nselect fhv_tripdata.dispatching_base_num,\nTIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS STRING)) AS pickup_datetime,\nTIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS STRING)) AS dropoff_datetime,",
        "question": "What SQL command is used to create an external table for fhv_tripdata?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 310,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you uploaded manually the fvh 2019 csv files, you may face errors regarding date types. Try to create an the external table in bigquery but define the pickup_datetime and dropoff_datetime to be strings\nCREATE OR REPLACE EXTERNAL TABLE `gcp_project.trips_data_all.fhv_tripdata`  (\ndispatching_base_num STRING,\npickup_datetime STRING,\ndropoff_datetime STRING,\nPUlocationID STRING,\nDOlocationID STRING,\nSR_Flag STRING,\nAffiliated_base_number STRING\n)\nOPTIONS (\nformat = 'csv',\nuris = ['gs://bucket/*.csv']\n);\nThen when creating the fhv core model in dbt, use TIMESTAMP(CAST(()) to ensure it first parses as a string and then convert it to timestamp.\nwith fhv_tripdata as (\nselect * from {{ ref('stg_fhv_tripdata') }}\n),\ndim_zones as (\nselect * from {{ ref('dim_zones') }}\nwhere borough != 'Unknown'\n)\nselect fhv_tripdata.dispatching_base_num,\nTIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS STRING)) AS pickup_datetime,\nTIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS STRING)) AS dropoff_datetime,",
        "question": "What does the TIMESTAMP function do when creating the fhv core model in dbt?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 310,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you uploaded manually the fvh 2019 csv files, you may face errors regarding date types. Try to create an the external table in bigquery but define the pickup_datetime and dropoff_datetime to be strings\nCREATE OR REPLACE EXTERNAL TABLE `gcp_project.trips_data_all.fhv_tripdata`  (\ndispatching_base_num STRING,\npickup_datetime STRING,\ndropoff_datetime STRING,\nPUlocationID STRING,\nDOlocationID STRING,\nSR_Flag STRING,\nAffiliated_base_number STRING\n)\nOPTIONS (\nformat = 'csv',\nuris = ['gs://bucket/*.csv']\n);\nThen when creating the fhv core model in dbt, use TIMESTAMP(CAST(()) to ensure it first parses as a string and then convert it to timestamp.\nwith fhv_tripdata as (\nselect * from {{ ref('stg_fhv_tripdata') }}\n),\ndim_zones as (\nselect * from {{ ref('dim_zones') }}\nwhere borough != 'Unknown'\n)\nselect fhv_tripdata.dispatching_base_num,\nTIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS STRING)) AS pickup_datetime,\nTIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS STRING)) AS dropoff_datetime,",
        "question": "What filtering condition is applied to the dim_zones table in the provided SQL query?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 310,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you uploaded manually the fvh 2019 parquet files manually after downloading from https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-*.parquet you may face errors regarding date types while loading the data in a landing table (say fhv_tripdata). Try to create an the external table with the schema defines as following and load each month in a loop.\n-----Correct load with schema defination----will not throw error----------------------\nCREATE OR REPLACE EXTERNAL TABLE `dw-bigquery-week-3.trips_data_all.external_tlc_fhv_trips_2019` (\ndispatching_base_num STRING,\npickup_datetime TIMESTAMP,\ndropoff_datetime TIMESTAMP,\nPUlocationID FLOAT64,\nDOlocationID FLOAT64,\nSR_Flag FLOAT64,\nAffiliated_base_number STRING\n)\nOPTIONS (\nformat = 'PARQUET',\nuris = ['gs://project id/fhv_2019_8.parquet']\n);\nCan Also USE  uris = ['gs://project id/fhv_2019_*.parquet'] (THIS WILL remove the need for the loop and can be done for all month in single RUN )\n– THANKYOU FOR THIS –",
        "question": "What is the main issue encountered when uploading fvh 2019 parquet files manually?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 311,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you uploaded manually the fvh 2019 parquet files manually after downloading from https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-*.parquet you may face errors regarding date types while loading the data in a landing table (say fhv_tripdata). Try to create an the external table with the schema defines as following and load each month in a loop.\n-----Correct load with schema defination----will not throw error----------------------\nCREATE OR REPLACE EXTERNAL TABLE `dw-bigquery-week-3.trips_data_all.external_tlc_fhv_trips_2019` (\ndispatching_base_num STRING,\npickup_datetime TIMESTAMP,\ndropoff_datetime TIMESTAMP,\nPUlocationID FLOAT64,\nDOlocationID FLOAT64,\nSR_Flag FLOAT64,\nAffiliated_base_number STRING\n)\nOPTIONS (\nformat = 'PARQUET',\nuris = ['gs://project id/fhv_2019_8.parquet']\n);\nCan Also USE  uris = ['gs://project id/fhv_2019_*.parquet'] (THIS WILL remove the need for the loop and can be done for all month in single RUN )\n– THANKYOU FOR THIS –",
        "question": "What schema should be used to create an external table for loading the fhv_tripdata?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 311,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you uploaded manually the fvh 2019 parquet files manually after downloading from https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-*.parquet you may face errors regarding date types while loading the data in a landing table (say fhv_tripdata). Try to create an the external table with the schema defines as following and load each month in a loop.\n-----Correct load with schema defination----will not throw error----------------------\nCREATE OR REPLACE EXTERNAL TABLE `dw-bigquery-week-3.trips_data_all.external_tlc_fhv_trips_2019` (\ndispatching_base_num STRING,\npickup_datetime TIMESTAMP,\ndropoff_datetime TIMESTAMP,\nPUlocationID FLOAT64,\nDOlocationID FLOAT64,\nSR_Flag FLOAT64,\nAffiliated_base_number STRING\n)\nOPTIONS (\nformat = 'PARQUET',\nuris = ['gs://project id/fhv_2019_8.parquet']\n);\nCan Also USE  uris = ['gs://project id/fhv_2019_*.parquet'] (THIS WILL remove the need for the loop and can be done for all month in single RUN )\n– THANKYOU FOR THIS –",
        "question": "How can you load each month's data without encountering errors?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 311,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you uploaded manually the fvh 2019 parquet files manually after downloading from https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-*.parquet you may face errors regarding date types while loading the data in a landing table (say fhv_tripdata). Try to create an the external table with the schema defines as following and load each month in a loop.\n-----Correct load with schema defination----will not throw error----------------------\nCREATE OR REPLACE EXTERNAL TABLE `dw-bigquery-week-3.trips_data_all.external_tlc_fhv_trips_2019` (\ndispatching_base_num STRING,\npickup_datetime TIMESTAMP,\ndropoff_datetime TIMESTAMP,\nPUlocationID FLOAT64,\nDOlocationID FLOAT64,\nSR_Flag FLOAT64,\nAffiliated_base_number STRING\n)\nOPTIONS (\nformat = 'PARQUET',\nuris = ['gs://project id/fhv_2019_8.parquet']\n);\nCan Also USE  uris = ['gs://project id/fhv_2019_*.parquet'] (THIS WILL remove the need for the loop and can be done for all month in single RUN )\n– THANKYOU FOR THIS –",
        "question": "What are the two options for the 'uris' parameter when creating the external table?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 311,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you uploaded manually the fvh 2019 parquet files manually after downloading from https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-*.parquet you may face errors regarding date types while loading the data in a landing table (say fhv_tripdata). Try to create an the external table with the schema defines as following and load each month in a loop.\n-----Correct load with schema defination----will not throw error----------------------\nCREATE OR REPLACE EXTERNAL TABLE `dw-bigquery-week-3.trips_data_all.external_tlc_fhv_trips_2019` (\ndispatching_base_num STRING,\npickup_datetime TIMESTAMP,\ndropoff_datetime TIMESTAMP,\nPUlocationID FLOAT64,\nDOlocationID FLOAT64,\nSR_Flag FLOAT64,\nAffiliated_base_number STRING\n)\nOPTIONS (\nformat = 'PARQUET',\nuris = ['gs://project id/fhv_2019_8.parquet']\n);\nCan Also USE  uris = ['gs://project id/fhv_2019_*.parquet'] (THIS WILL remove the need for the loop and can be done for all month in single RUN )\n– THANKYOU FOR THIS –",
        "question": "What is the benefit of using a wildcard in the 'uris' option for loading the data?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 311,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When accessing Looker Studio through the Google Cloud Project console, you may be prompted to subscribe to the Pro version and receive the following errors:\nInstead, navigate to https://lookerstudio.google.com/navigation/reporting which will take you to the free version.",
        "question": "What may happen when accessing Looker Studio through the Google Cloud Project console?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 312,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When accessing Looker Studio through the Google Cloud Project console, you may be prompted to subscribe to the Pro version and receive the following errors:\nInstead, navigate to https://lookerstudio.google.com/navigation/reporting which will take you to the free version.",
        "question": "What errors might you receive when prompted to subscribe to the Pro version?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 312,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When accessing Looker Studio through the Google Cloud Project console, you may be prompted to subscribe to the Pro version and receive the following errors:\nInstead, navigate to https://lookerstudio.google.com/navigation/reporting which will take you to the free version.",
        "question": "How can one access the free version of Looker Studio?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 312,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When accessing Looker Studio through the Google Cloud Project console, you may be prompted to subscribe to the Pro version and receive the following errors:\nInstead, navigate to https://lookerstudio.google.com/navigation/reporting which will take you to the free version.",
        "question": "What is the URL for accessing Looker Studio's reporting feature?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 312,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When accessing Looker Studio through the Google Cloud Project console, you may be prompted to subscribe to the Pro version and receive the following errors:\nInstead, navigate to https://lookerstudio.google.com/navigation/reporting which will take you to the free version.",
        "question": "What is the difference between the Pro version and the free version of Looker Studio?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 312,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: Dbt provides a mechanism called \"ref\" to manage dependencies between models. By referencing other models using the \"ref\" keyword in SQL, dbt automatically understands the dependencies and ensures the correct execution order.\nLoading FHV Data goes into slumber using Mage?\nTry loading the data using jupyter notebooks in a local environment. There might be bandwidth issues with Mage.\nLoad the data into a pandas dataframe using the urls, make necessary transformations, upload the gcp bucket / alternatively download the parquet/csv files locally and then upload to GCP manually.\nRegion Mismatch in DBT and BigQuery\nIf you are using the datasets copied into BigQuery from BigQuery public datasets, the region will be set as US by default and hence it is much easier to set your dbt profile location as US while transforming the tables and views. \nYou can change the location as follows:",
        "question": "What mechanism does dbt provide to manage dependencies between models?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 313,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: Dbt provides a mechanism called \"ref\" to manage dependencies between models. By referencing other models using the \"ref\" keyword in SQL, dbt automatically understands the dependencies and ensures the correct execution order.\nLoading FHV Data goes into slumber using Mage?\nTry loading the data using jupyter notebooks in a local environment. There might be bandwidth issues with Mage.\nLoad the data into a pandas dataframe using the urls, make necessary transformations, upload the gcp bucket / alternatively download the parquet/csv files locally and then upload to GCP manually.\nRegion Mismatch in DBT and BigQuery\nIf you are using the datasets copied into BigQuery from BigQuery public datasets, the region will be set as US by default and hence it is much easier to set your dbt profile location as US while transforming the tables and views. \nYou can change the location as follows:",
        "question": "How does dbt ensure the correct execution order of models?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 313,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: Dbt provides a mechanism called \"ref\" to manage dependencies between models. By referencing other models using the \"ref\" keyword in SQL, dbt automatically understands the dependencies and ensures the correct execution order.\nLoading FHV Data goes into slumber using Mage?\nTry loading the data using jupyter notebooks in a local environment. There might be bandwidth issues with Mage.\nLoad the data into a pandas dataframe using the urls, make necessary transformations, upload the gcp bucket / alternatively download the parquet/csv files locally and then upload to GCP manually.\nRegion Mismatch in DBT and BigQuery\nIf you are using the datasets copied into BigQuery from BigQuery public datasets, the region will be set as US by default and hence it is much easier to set your dbt profile location as US while transforming the tables and views. \nYou can change the location as follows:",
        "question": "What issues might arise when loading FHV Data using Mage?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 313,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: Dbt provides a mechanism called \"ref\" to manage dependencies between models. By referencing other models using the \"ref\" keyword in SQL, dbt automatically understands the dependencies and ensures the correct execution order.\nLoading FHV Data goes into slumber using Mage?\nTry loading the data using jupyter notebooks in a local environment. There might be bandwidth issues with Mage.\nLoad the data into a pandas dataframe using the urls, make necessary transformations, upload the gcp bucket / alternatively download the parquet/csv files locally and then upload to GCP manually.\nRegion Mismatch in DBT and BigQuery\nIf you are using the datasets copied into BigQuery from BigQuery public datasets, the region will be set as US by default and hence it is much easier to set your dbt profile location as US while transforming the tables and views. \nYou can change the location as follows:",
        "question": "How can data be loaded into a pandas dataframe for further transformations?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 313,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: Dbt provides a mechanism called \"ref\" to manage dependencies between models. By referencing other models using the \"ref\" keyword in SQL, dbt automatically understands the dependencies and ensures the correct execution order.\nLoading FHV Data goes into slumber using Mage?\nTry loading the data using jupyter notebooks in a local environment. There might be bandwidth issues with Mage.\nLoad the data into a pandas dataframe using the urls, make necessary transformations, upload the gcp bucket / alternatively download the parquet/csv files locally and then upload to GCP manually.\nRegion Mismatch in DBT and BigQuery\nIf you are using the datasets copied into BigQuery from BigQuery public datasets, the region will be set as US by default and hence it is much easier to set your dbt profile location as US while transforming the tables and views. \nYou can change the location as follows:",
        "question": "Why is it easier to set the dbt profile location to US when using datasets copied into BigQuery?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 313,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use the PostgreSQL COPY FROM feature that is compatible with csv files\nCOPY table_name [ ( column_name [, ...] ) ]\nFROM { 'filename' | PROGRAM 'command' | STDIN }\n[ [ WITH ] ( option [, ...] ) ]\n[ WHERE condition ]",
        "question": "What is the purpose of the PostgreSQL COPY FROM feature?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 314,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use the PostgreSQL COPY FROM feature that is compatible with csv files\nCOPY table_name [ ( column_name [, ...] ) ]\nFROM { 'filename' | PROGRAM 'command' | STDIN }\n[ [ WITH ] ( option [, ...] ) ]\n[ WHERE condition ]",
        "question": "How do you specify the table name when using the COPY FROM command?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 314,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use the PostgreSQL COPY FROM feature that is compatible with csv files\nCOPY table_name [ ( column_name [, ...] ) ]\nFROM { 'filename' | PROGRAM 'command' | STDIN }\n[ [ WITH ] ( option [, ...] ) ]\n[ WHERE condition ]",
        "question": "What types of source can be used with the FROM clause in the COPY command?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 314,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use the PostgreSQL COPY FROM feature that is compatible with csv files\nCOPY table_name [ ( column_name [, ...] ) ]\nFROM { 'filename' | PROGRAM 'command' | STDIN }\n[ [ WITH ] ( option [, ...] ) ]\n[ WHERE condition ]",
        "question": "What options can be included after the WITH keyword in the COPY command?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 314,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use the PostgreSQL COPY FROM feature that is compatible with csv files\nCOPY table_name [ ( column_name [, ...] ) ]\nFROM { 'filename' | PROGRAM 'command' | STDIN }\n[ [ WITH ] ( option [, ...] ) ]\n[ WHERE condition ]",
        "question": "What is the significance of the WHERE clause in the COPY FROM statement?",
        "section": "Module 4: analytics engineering with dbt",
        "document_id": 314,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Update the line:\nWith:",
        "question": "What does it mean to update the line?",
        "section": "Module 5: pyspark",
        "document_id": 315,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Update the line:\nWith:",
        "question": "What specific line is being updated?",
        "section": "Module 5: pyspark",
        "document_id": 315,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Update the line:\nWith:",
        "question": "What process is involved in updating the line?",
        "section": "Module 5: pyspark",
        "document_id": 315,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Update the line:\nWith:",
        "question": "Are there any tools or software needed to update the line?",
        "section": "Module 5: pyspark",
        "document_id": 315,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Update the line:\nWith:",
        "question": "What are the implications of updating the line?",
        "section": "Module 5: pyspark",
        "document_id": 315,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Install SDKMAN:\ncurl -s \"https://get.sdkman.io\" | bash\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nUsing SDKMAN, install Java 11 and Spark 3.3.2:\nsdk install java 11.0.22-tem\nsdk install spark 3.3.2\nOpen a new terminal or run the following in the same shell:\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nVerify the locations and versions of Java and Spark that were installed:\necho $JAVA_HOME\njava -version\necho $SPARK_HOME\nspark-submit --version",
        "question": "What command is used to install SDKMAN?",
        "section": "Module 5: pyspark",
        "document_id": 316,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Install SDKMAN:\ncurl -s \"https://get.sdkman.io\" | bash\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nUsing SDKMAN, install Java 11 and Spark 3.3.2:\nsdk install java 11.0.22-tem\nsdk install spark 3.3.2\nOpen a new terminal or run the following in the same shell:\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nVerify the locations and versions of Java and Spark that were installed:\necho $JAVA_HOME\njava -version\necho $SPARK_HOME\nspark-submit --version",
        "question": "How can you install Java 11 using SDKMAN?",
        "section": "Module 5: pyspark",
        "document_id": 316,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Install SDKMAN:\ncurl -s \"https://get.sdkman.io\" | bash\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nUsing SDKMAN, install Java 11 and Spark 3.3.2:\nsdk install java 11.0.22-tem\nsdk install spark 3.3.2\nOpen a new terminal or run the following in the same shell:\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nVerify the locations and versions of Java and Spark that were installed:\necho $JAVA_HOME\njava -version\necho $SPARK_HOME\nspark-submit --version",
        "question": "What command is used to verify the installed version of Java?",
        "section": "Module 5: pyspark",
        "document_id": 316,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Install SDKMAN:\ncurl -s \"https://get.sdkman.io\" | bash\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nUsing SDKMAN, install Java 11 and Spark 3.3.2:\nsdk install java 11.0.22-tem\nsdk install spark 3.3.2\nOpen a new terminal or run the following in the same shell:\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nVerify the locations and versions of Java and Spark that were installed:\necho $JAVA_HOME\njava -version\necho $SPARK_HOME\nspark-submit --version",
        "question": "What is the command to check the installation location of Spark?",
        "section": "Module 5: pyspark",
        "document_id": 316,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Install SDKMAN:\ncurl -s \"https://get.sdkman.io\" | bash\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nUsing SDKMAN, install Java 11 and Spark 3.3.2:\nsdk install java 11.0.22-tem\nsdk install spark 3.3.2\nOpen a new terminal or run the following in the same shell:\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nVerify the locations and versions of Java and Spark that were installed:\necho $JAVA_HOME\njava -version\necho $SPARK_HOME\nspark-submit --version",
        "question": "What must be done after installing SDKMAN for the changes to take effect?",
        "section": "Module 5: pyspark",
        "document_id": 316,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re seriously struggling to set things up \"locally\" (here locally meaning non/partly-managed environment like own laptop, a VM or Codespaces) you can use the following guide to use Spark in Google Colab:\nhttps://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304\nStarter notebook:\nhttps://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb\nIt’s advisable to spend some time setting things up locally rather than jumping right into this solution.",
        "question": "What are some local environments mentioned for setting up Spark?",
        "section": "Module 5: pyspark",
        "document_id": 317,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re seriously struggling to set things up \"locally\" (here locally meaning non/partly-managed environment like own laptop, a VM or Codespaces) you can use the following guide to use Spark in Google Colab:\nhttps://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304\nStarter notebook:\nhttps://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb\nIt’s advisable to spend some time setting things up locally rather than jumping right into this solution.",
        "question": "What is the recommended guide for using Spark in Google Colab?",
        "section": "Module 5: pyspark",
        "document_id": 317,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re seriously struggling to set things up \"locally\" (here locally meaning non/partly-managed environment like own laptop, a VM or Codespaces) you can use the following guide to use Spark in Google Colab:\nhttps://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304\nStarter notebook:\nhttps://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb\nIt’s advisable to spend some time setting things up locally rather than jumping right into this solution.",
        "question": "Where can I find the starter notebook for Spark in Google Colab?",
        "section": "Module 5: pyspark",
        "document_id": 317,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re seriously struggling to set things up \"locally\" (here locally meaning non/partly-managed environment like own laptop, a VM or Codespaces) you can use the following guide to use Spark in Google Colab:\nhttps://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304\nStarter notebook:\nhttps://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb\nIt’s advisable to spend some time setting things up locally rather than jumping right into this solution.",
        "question": "Why is it advisable to spend time setting things up locally?",
        "section": "Module 5: pyspark",
        "document_id": 317,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re seriously struggling to set things up \"locally\" (here locally meaning non/partly-managed environment like own laptop, a VM or Codespaces) you can use the following guide to use Spark in Google Colab:\nhttps://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304\nStarter notebook:\nhttps://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb\nIt’s advisable to spend some time setting things up locally rather than jumping right into this solution.",
        "question": "What types of environments does 'locally' refer to in the context of this text?",
        "section": "Module 5: pyspark",
        "document_id": 317,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:\njava.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed\nmodule @0x3c947bc5\nSolution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file.",
        "question": "What error may occur when running spark-shell after installing Java, Hadoop, and Spark?",
        "section": "Module 5: pyspark",
        "document_id": 318,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:\njava.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed\nmodule @0x3c947bc5\nSolution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file.",
        "question": "Which versions of Java are supported by Spark 3.x?",
        "section": "Module 5: pyspark",
        "document_id": 318,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:\njava.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed\nmodule @0x3c947bc5\nSolution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file.",
        "question": "What is the recommended version of Java to install for compatibility with Spark?",
        "section": "Module 5: pyspark",
        "document_id": 318,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:\njava.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed\nmodule @0x3c947bc5\nSolution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file.",
        "question": "What issue arises from using Java 17 or 19 with Spark?",
        "section": "Module 5: pyspark",
        "document_id": 318,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:\njava.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed\nmodule @0x3c947bc5\nSolution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file.",
        "question": "Where can one find the website to download Java 11 for Spark setup?",
        "section": "Module 5: pyspark",
        "document_id": 318,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I found this error while executing the user defined function in Spark (crazy_stuff_udf). I am working on Windows and using conda. After following the setup instructions, I found that the PYSPARK_PYTHON environment variable was not set correctly, given that conda has different python paths for each environment.\nSolution:\npip install findspark on the command line inside proper environment\nAdd to the top of the script\nimport findspark\nfindspark.init()",
        "question": "What error was encountered while executing the user defined function in Spark?",
        "section": "Module 5: pyspark",
        "document_id": 319,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I found this error while executing the user defined function in Spark (crazy_stuff_udf). I am working on Windows and using conda. After following the setup instructions, I found that the PYSPARK_PYTHON environment variable was not set correctly, given that conda has different python paths for each environment.\nSolution:\npip install findspark on the command line inside proper environment\nAdd to the top of the script\nimport findspark\nfindspark.init()",
        "question": "What operating system is being used to run Spark?",
        "section": "Module 5: pyspark",
        "document_id": 319,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I found this error while executing the user defined function in Spark (crazy_stuff_udf). I am working on Windows and using conda. After following the setup instructions, I found that the PYSPARK_PYTHON environment variable was not set correctly, given that conda has different python paths for each environment.\nSolution:\npip install findspark on the command line inside proper environment\nAdd to the top of the script\nimport findspark\nfindspark.init()",
        "question": "What command is suggested to install findspark?",
        "section": "Module 5: pyspark",
        "document_id": 319,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I found this error while executing the user defined function in Spark (crazy_stuff_udf). I am working on Windows and using conda. After following the setup instructions, I found that the PYSPARK_PYTHON environment variable was not set correctly, given that conda has different python paths for each environment.\nSolution:\npip install findspark on the command line inside proper environment\nAdd to the top of the script\nimport findspark\nfindspark.init()",
        "question": "What must be added to the top of the script to use findspark?",
        "section": "Module 5: pyspark",
        "document_id": 319,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I found this error while executing the user defined function in Spark (crazy_stuff_udf). I am working on Windows and using conda. After following the setup instructions, I found that the PYSPARK_PYTHON environment variable was not set correctly, given that conda has different python paths for each environment.\nSolution:\npip install findspark on the command line inside proper environment\nAdd to the top of the script\nimport findspark\nfindspark.init()",
        "question": "Why was the PYSPARK_PYTHON environment variable not set correctly?",
        "section": "Module 5: pyspark",
        "document_id": 319,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is because Python 3.11 has some inconsistencies with such an old version of Spark. The solution is a downgrade in the Python version. Python 3.9 using a conda environment takes care of it. Or install newer PySpark >= 3.5.1 works for me (Ella) [source].",
        "question": "What version of Python is recommended for compatibility with older versions of Spark?",
        "section": "Module 5: pyspark",
        "document_id": 320,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is because Python 3.11 has some inconsistencies with such an old version of Spark. The solution is a downgrade in the Python version. Python 3.9 using a conda environment takes care of it. Or install newer PySpark >= 3.5.1 works for me (Ella) [source].",
        "question": "What specific version of Python resolved the inconsistencies mentioned in the text?",
        "section": "Module 5: pyspark",
        "document_id": 320,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is because Python 3.11 has some inconsistencies with such an old version of Spark. The solution is a downgrade in the Python version. Python 3.9 using a conda environment takes care of it. Or install newer PySpark >= 3.5.1 works for me (Ella) [source].",
        "question": "What is one potential solution to the compatibility issue between Python and Spark?",
        "section": "Module 5: pyspark",
        "document_id": 320,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is because Python 3.11 has some inconsistencies with such an old version of Spark. The solution is a downgrade in the Python version. Python 3.9 using a conda environment takes care of it. Or install newer PySpark >= 3.5.1 works for me (Ella) [source].",
        "question": "Which version of PySpark is mentioned as working for the author, Ella?",
        "section": "Module 5: pyspark",
        "document_id": 320,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is because Python 3.11 has some inconsistencies with such an old version of Spark. The solution is a downgrade in the Python version. Python 3.9 using a conda environment takes care of it. Or install newer PySpark >= 3.5.1 works for me (Ella) [source].",
        "question": "How did the author suggest managing the Python environment to address compatibility issues?",
        "section": "Module 5: pyspark",
        "document_id": 320,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook\nimport pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n.master(\"local[*]\") \\\n.appName('test') \\\n.getOrCreate()\ndf = spark.read \\\n.option(\"header\", \"true\") \\\n.csv('taxi+_zone_lookup.csv')\ndf.show()\nit gives the error:\nRuntimeError: Java gateway process exited before sending its port number\n✅The solution (for me) was:\npip install findspark on the command line and then\nAdd\nimport findspark\nfindspark.init()\nto the top of the script.\nAnother possible solution is:\nCheck that pyspark is pointing to the correct location.\nRun pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos.\nIf it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content.\nTo add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial: Install Apache PySpark on Windows PC | Apache Spark Installation Guide\nOnce everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.",
        "question": "What error occurs when running the PySpark script in the Jupyter notebook?",
        "section": "Module 5: pyspark",
        "document_id": 322,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook\nimport pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n.master(\"local[*]\") \\\n.appName('test') \\\n.getOrCreate()\ndf = spark.read \\\n.option(\"header\", \"true\") \\\n.csv('taxi+_zone_lookup.csv')\ndf.show()\nit gives the error:\nRuntimeError: Java gateway process exited before sending its port number\n✅The solution (for me) was:\npip install findspark on the command line and then\nAdd\nimport findspark\nfindspark.init()\nto the top of the script.\nAnother possible solution is:\nCheck that pyspark is pointing to the correct location.\nRun pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos.\nIf it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content.\nTo add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial: Install Apache PySpark on Windows PC | Apache Spark Installation Guide\nOnce everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.",
        "question": "What package needs to be installed to resolve the 'Java gateway process exited' error?",
        "section": "Module 5: pyspark",
        "document_id": 322,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook\nimport pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n.master(\"local[*]\") \\\n.appName('test') \\\n.getOrCreate()\ndf = spark.read \\\n.option(\"header\", \"true\") \\\n.csv('taxi+_zone_lookup.csv')\ndf.show()\nit gives the error:\nRuntimeError: Java gateway process exited before sending its port number\n✅The solution (for me) was:\npip install findspark on the command line and then\nAdd\nimport findspark\nfindspark.init()\nto the top of the script.\nAnother possible solution is:\nCheck that pyspark is pointing to the correct location.\nRun pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos.\nIf it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content.\nTo add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial: Install Apache PySpark on Windows PC | Apache Spark Installation Guide\nOnce everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.",
        "question": "What code should be added to the top of the script after installing findspark?",
        "section": "Module 5: pyspark",
        "document_id": 322,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook\nimport pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n.master(\"local[*]\") \\\n.appName('test') \\\n.getOrCreate()\ndf = spark.read \\\n.option(\"header\", \"true\") \\\n.csv('taxi+_zone_lookup.csv')\ndf.show()\nit gives the error:\nRuntimeError: Java gateway process exited before sending its port number\n✅The solution (for me) was:\npip install findspark on the command line and then\nAdd\nimport findspark\nfindspark.init()\nto the top of the script.\nAnother possible solution is:\nCheck that pyspark is pointing to the correct location.\nRun pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos.\nIf it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content.\nTo add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial: Install Apache PySpark on Windows PC | Apache Spark Installation Guide\nOnce everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.",
        "question": "How can you check if PySpark is pointing to the correct location?",
        "section": "Module 5: pyspark",
        "document_id": 322,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook\nimport pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n.master(\"local[*]\") \\\n.appName('test') \\\n.getOrCreate()\ndf = spark.read \\\n.option(\"header\", \"true\") \\\n.csv('taxi+_zone_lookup.csv')\ndf.show()\nit gives the error:\nRuntimeError: Java gateway process exited before sending its port number\n✅The solution (for me) was:\npip install findspark on the command line and then\nAdd\nimport findspark\nfindspark.init()\nto the top of the script.\nAnother possible solution is:\nCheck that pyspark is pointing to the correct location.\nRun pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos.\nIf it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content.\nTo add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial: Install Apache PySpark on Windows PC | Apache Spark Installation Guide\nOnce everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.",
        "question": "What is suggested for setting environment variables permanently in relation to PySpark?",
        "section": "Module 5: pyspark",
        "document_id": 322,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\nThe solution which worked for me(use following in jupyter notebook) :\n!pip install findspark\nimport findspark\nfindspark.init()\nThereafter , import pyspark and create spark contex<<t as usual\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\nFilter based on conditions based on multiple columns\nfrom pyspark.sql.functions import col\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\nKrishna Anand",
        "question": "What error was encountered when trying to use pyspark in Jupyter notebook?",
        "section": "Module 5: pyspark",
        "document_id": 323,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\nThe solution which worked for me(use following in jupyter notebook) :\n!pip install findspark\nimport findspark\nfindspark.init()\nThereafter , import pyspark and create spark contex<<t as usual\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\nFilter based on conditions based on multiple columns\nfrom pyspark.sql.functions import col\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\nKrishna Anand",
        "question": "Which command solved the module not found error for pyspark in Jupyter notebook?",
        "section": "Module 5: pyspark",
        "document_id": 323,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\nThe solution which worked for me(use following in jupyter notebook) :\n!pip install findspark\nimport findspark\nfindspark.init()\nThereafter , import pyspark and create spark contex<<t as usual\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\nFilter based on conditions based on multiple columns\nfrom pyspark.sql.functions import col\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\nKrishna Anand",
        "question": "What is the purpose of the findspark library in this context?",
        "section": "Module 5: pyspark",
        "document_id": 323,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\nThe solution which worked for me(use following in jupyter notebook) :\n!pip install findspark\nimport findspark\nfindspark.init()\nThereafter , import pyspark and create spark contex<<t as usual\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\nFilter based on conditions based on multiple columns\nfrom pyspark.sql.functions import col\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\nKrishna Anand",
        "question": "What specific command was successful in installing pyspark that was different from the previous attempts?",
        "section": "Module 5: pyspark",
        "document_id": 323,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\nThe solution which worked for me(use following in jupyter notebook) :\n!pip install findspark\nimport findspark\nfindspark.init()\nThereafter , import pyspark and create spark contex<<t as usual\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\nFilter based on conditions based on multiple columns\nfrom pyspark.sql.functions import col\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\nKrishna Anand",
        "question": "How can multiple column conditions be applied when filtering data in pyspark?",
        "section": "Module 5: pyspark",
        "document_id": 323,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\nexport PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\nAdditionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.",
        "question": "How can you verify the version of the Py4J file in your installation?",
        "section": "Module 5: pyspark",
        "document_id": 324,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\nexport PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\nAdditionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.",
        "question": "What command is used to set the PYTHONPATH for Py4J?",
        "section": "Module 5: pyspark",
        "document_id": 324,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\nexport PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\nAdditionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.",
        "question": "What error might you encounter if the Py4J version does not match the expected filename?",
        "section": "Module 5: pyspark",
        "document_id": 324,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\nexport PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\nAdditionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.",
        "question": "What should you do if you encounter issues after setting the correct Py4J version?",
        "section": "Module 5: pyspark",
        "document_id": 324,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\nexport PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\nAdditionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.",
        "question": "How can you check the version of 'py4j' for the Spark you are using?",
        "section": "Module 5: pyspark",
        "document_id": 324,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If below does not work, then download the latest available py4j version with\nconda install -c conda-forge py4j\nTake care of the latest version number in the website to replace appropriately.\nNow add\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\nin your  .bashrc file.",
        "question": "What command should be used to install the latest available py4j version?",
        "section": "Module 5: pyspark",
        "document_id": 325,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If below does not work, then download the latest available py4j version with\nconda install -c conda-forge py4j\nTake care of the latest version number in the website to replace appropriately.\nNow add\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\nin your  .bashrc file.",
        "question": "Where should the py4j installation command be run?",
        "section": "Module 5: pyspark",
        "document_id": 325,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If below does not work, then download the latest available py4j version with\nconda install -c conda-forge py4j\nTake care of the latest version number in the website to replace appropriately.\nNow add\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\nin your  .bashrc file.",
        "question": "What file needs to be modified to include the PYTHONPATH for Spark?",
        "section": "Module 5: pyspark",
        "document_id": 325,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If below does not work, then download the latest available py4j version with\nconda install -c conda-forge py4j\nTake care of the latest version number in the website to replace appropriately.\nNow add\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\nin your  .bashrc file.",
        "question": "What are the two export commands that need to be added to the .bashrc file?",
        "section": "Module 5: pyspark",
        "document_id": 325,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If below does not work, then download the latest available py4j version with\nconda install -c conda-forge py4j\nTake care of the latest version number in the website to replace appropriately.\nNow add\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\nin your  .bashrc file.",
        "question": "Why is it important to check the latest version number on the website before installation?",
        "section": "Module 5: pyspark",
        "document_id": 325,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Even after we have exported our paths correctly you may find that  even though Jupyter is installed you might not have Jupyter Noteboopgak for one reason or another. Full instructions are found here (for my walkthrough) or here (where I got the original instructions from) but are included below. These instructions include setting up a virtual environment (handy if you are on your own machine doing this and not a VM):\nFull steps:\nUpdate and upgrade packages:\nsudo apt update && sudo apt -y upgrade\nInstall Python:\nsudo apt install python3-pip python3-dev\nInstall Python virtualenv:\nsudo -H pip3 install --upgrade pip\nsudo -H pip3 install virtualenv\nCreate a Python Virtual Environment:\nmkdir notebook\ncd notebook\nvirtualenv jupyterenv\nsource jupyterenv/bin/activate\nInstall Jupyter Notebook:\npip install jupyter\nRun Jupyter Notebook:\njupyter notebook",
        "question": "What are the initial commands to update and upgrade packages?",
        "section": "Module 5: pyspark",
        "document_id": 326,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Even after we have exported our paths correctly you may find that  even though Jupyter is installed you might not have Jupyter Noteboopgak for one reason or another. Full instructions are found here (for my walkthrough) or here (where I got the original instructions from) but are included below. These instructions include setting up a virtual environment (handy if you are on your own machine doing this and not a VM):\nFull steps:\nUpdate and upgrade packages:\nsudo apt update && sudo apt -y upgrade\nInstall Python:\nsudo apt install python3-pip python3-dev\nInstall Python virtualenv:\nsudo -H pip3 install --upgrade pip\nsudo -H pip3 install virtualenv\nCreate a Python Virtual Environment:\nmkdir notebook\ncd notebook\nvirtualenv jupyterenv\nsource jupyterenv/bin/activate\nInstall Jupyter Notebook:\npip install jupyter\nRun Jupyter Notebook:\njupyter notebook",
        "question": "How do you create a Python virtual environment for Jupyter Notebook?",
        "section": "Module 5: pyspark",
        "document_id": 326,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Even after we have exported our paths correctly you may find that  even though Jupyter is installed you might not have Jupyter Noteboopgak for one reason or another. Full instructions are found here (for my walkthrough) or here (where I got the original instructions from) but are included below. These instructions include setting up a virtual environment (handy if you are on your own machine doing this and not a VM):\nFull steps:\nUpdate and upgrade packages:\nsudo apt update && sudo apt -y upgrade\nInstall Python:\nsudo apt install python3-pip python3-dev\nInstall Python virtualenv:\nsudo -H pip3 install --upgrade pip\nsudo -H pip3 install virtualenv\nCreate a Python Virtual Environment:\nmkdir notebook\ncd notebook\nvirtualenv jupyterenv\nsource jupyterenv/bin/activate\nInstall Jupyter Notebook:\npip install jupyter\nRun Jupyter Notebook:\njupyter notebook",
        "question": "What command is used to install Jupyter Notebook after activating the virtual environment?",
        "section": "Module 5: pyspark",
        "document_id": 326,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Even after we have exported our paths correctly you may find that  even though Jupyter is installed you might not have Jupyter Noteboopgak for one reason or another. Full instructions are found here (for my walkthrough) or here (where I got the original instructions from) but are included below. These instructions include setting up a virtual environment (handy if you are on your own machine doing this and not a VM):\nFull steps:\nUpdate and upgrade packages:\nsudo apt update && sudo apt -y upgrade\nInstall Python:\nsudo apt install python3-pip python3-dev\nInstall Python virtualenv:\nsudo -H pip3 install --upgrade pip\nsudo -H pip3 install virtualenv\nCreate a Python Virtual Environment:\nmkdir notebook\ncd notebook\nvirtualenv jupyterenv\nsource jupyterenv/bin/activate\nInstall Jupyter Notebook:\npip install jupyter\nRun Jupyter Notebook:\njupyter notebook",
        "question": "What is the purpose of using a virtual environment when installing Jupyter Notebook?",
        "section": "Module 5: pyspark",
        "document_id": 326,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Even after we have exported our paths correctly you may find that  even though Jupyter is installed you might not have Jupyter Noteboopgak for one reason or another. Full instructions are found here (for my walkthrough) or here (where I got the original instructions from) but are included below. These instructions include setting up a virtual environment (handy if you are on your own machine doing this and not a VM):\nFull steps:\nUpdate and upgrade packages:\nsudo apt update && sudo apt -y upgrade\nInstall Python:\nsudo apt install python3-pip python3-dev\nInstall Python virtualenv:\nsudo -H pip3 install --upgrade pip\nsudo -H pip3 install virtualenv\nCreate a Python Virtual Environment:\nmkdir notebook\ncd notebook\nvirtualenv jupyterenv\nsource jupyterenv/bin/activate\nInstall Jupyter Notebook:\npip install jupyter\nRun Jupyter Notebook:\njupyter notebook",
        "question": "What should you do if Jupyter is installed but you can't find Jupyter Notebook?",
        "section": "Module 5: pyspark",
        "document_id": 326,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Code executed:\ndf = spark.read.parquet(pq_path)\n… some operations on df …\ndf.write.parquet(pq_path, mode=\"overwrite\")\njava.io.FileNotFoundException: File file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet does not exist\nThe problem is that Sparks performs lazy transformations, so the actual action that trigger the job is df.write, which does delete the parquet files that is trying to read (mode=”overwrite”)\n✅Solution: Write to a different directorydf\ndf.write.parquet(pq_path_temp, mode=\"overwrite\")",
        "question": "What method is used to read a parquet file in the provided code?",
        "section": "Module 5: pyspark",
        "document_id": 327,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Code executed:\ndf = spark.read.parquet(pq_path)\n… some operations on df …\ndf.write.parquet(pq_path, mode=\"overwrite\")\njava.io.FileNotFoundException: File file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet does not exist\nThe problem is that Sparks performs lazy transformations, so the actual action that trigger the job is df.write, which does delete the parquet files that is trying to read (mode=”overwrite”)\n✅Solution: Write to a different directorydf\ndf.write.parquet(pq_path_temp, mode=\"overwrite\")",
        "question": "What error is encountered when executing the code, and what does it indicate?",
        "section": "Module 5: pyspark",
        "document_id": 327,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Code executed:\ndf = spark.read.parquet(pq_path)\n… some operations on df …\ndf.write.parquet(pq_path, mode=\"overwrite\")\njava.io.FileNotFoundException: File file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet does not exist\nThe problem is that Sparks performs lazy transformations, so the actual action that trigger the job is df.write, which does delete the parquet files that is trying to read (mode=”overwrite”)\n✅Solution: Write to a different directorydf\ndf.write.parquet(pq_path_temp, mode=\"overwrite\")",
        "question": "What is the reason behind the FileNotFoundException in the context of Spark operations?",
        "section": "Module 5: pyspark",
        "document_id": 327,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Code executed:\ndf = spark.read.parquet(pq_path)\n… some operations on df …\ndf.write.parquet(pq_path, mode=\"overwrite\")\njava.io.FileNotFoundException: File file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet does not exist\nThe problem is that Sparks performs lazy transformations, so the actual action that trigger the job is df.write, which does delete the parquet files that is trying to read (mode=”overwrite”)\n✅Solution: Write to a different directorydf\ndf.write.parquet(pq_path_temp, mode=\"overwrite\")",
        "question": "What solution is suggested to avoid the FileNotFoundException?",
        "section": "Module 5: pyspark",
        "document_id": 327,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Code executed:\ndf = spark.read.parquet(pq_path)\n… some operations on df …\ndf.write.parquet(pq_path, mode=\"overwrite\")\njava.io.FileNotFoundException: File file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet does not exist\nThe problem is that Sparks performs lazy transformations, so the actual action that trigger the job is df.write, which does delete the parquet files that is trying to read (mode=”overwrite”)\n✅Solution: Write to a different directorydf\ndf.write.parquet(pq_path_temp, mode=\"overwrite\")",
        "question": "What is the effect of using mode='overwrite' when writing parquet files in Spark?",
        "section": "Module 5: pyspark",
        "document_id": 327,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to create the Hadoop /bin directory manually and add the downloaded files in there, since the shell script provided for Windows installation just puts them in /c/tools/hadoop-3.2.0/ .",
        "question": "What directory needs to be created manually for Hadoop installation?",
        "section": "Module 5: pyspark",
        "document_id": 328,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to create the Hadoop /bin directory manually and add the downloaded files in there, since the shell script provided for Windows installation just puts them in /c/tools/hadoop-3.2.0/ .",
        "question": "Where does the Windows installation script place the downloaded Hadoop files?",
        "section": "Module 5: pyspark",
        "document_id": 328,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to create the Hadoop /bin directory manually and add the downloaded files in there, since the shell script provided for Windows installation just puts them in /c/tools/hadoop-3.2.0/ .",
        "question": "What version of Hadoop is mentioned in the installation instructions?",
        "section": "Module 5: pyspark",
        "document_id": 328,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to create the Hadoop /bin directory manually and add the downloaded files in there, since the shell script provided for Windows installation just puts them in /c/tools/hadoop-3.2.0/ .",
        "question": "Why is it necessary to manually create the Hadoop /bin directory?",
        "section": "Module 5: pyspark",
        "document_id": 328,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to create the Hadoop /bin directory manually and add the downloaded files in there, since the shell script provided for Windows installation just puts them in /c/tools/hadoop-3.2.0/ .",
        "question": "What is the path where the Windows installation script puts the Hadoop files?",
        "section": "Module 5: pyspark",
        "document_id": 328,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Actually Spark SQL is one independent “type” of SQL - Spark SQL.\nThe several SQL providers are very similar:\nSELECT [attributes]\nFROM [table]\nWHERE [filter]\nGROUP BY [grouping attributes]\nHAVING [filtering the groups]\nORDER BY [attribute to order]\n(INNER/FULL/LEFT/RIGHT) JOIN [table2]\nON [attributes table joining table2] (...)\nWhat differs the most between several SQL providers are built-in functions.\nFor Built-in Spark SQL function check this link: https://spark.apache.org/docs/latest/api/sql/index.html\nExtra information on SPARK SQL :\nhttps://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data.",
        "question": "What is Spark SQL?",
        "section": "Module 5: pyspark",
        "document_id": 329,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Actually Spark SQL is one independent “type” of SQL - Spark SQL.\nThe several SQL providers are very similar:\nSELECT [attributes]\nFROM [table]\nWHERE [filter]\nGROUP BY [grouping attributes]\nHAVING [filtering the groups]\nORDER BY [attribute to order]\n(INNER/FULL/LEFT/RIGHT) JOIN [table2]\nON [attributes table joining table2] (...)\nWhat differs the most between several SQL providers are built-in functions.\nFor Built-in Spark SQL function check this link: https://spark.apache.org/docs/latest/api/sql/index.html\nExtra information on SPARK SQL :\nhttps://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data.",
        "question": "How does Spark SQL differ from other SQL providers?",
        "section": "Module 5: pyspark",
        "document_id": 329,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Actually Spark SQL is one independent “type” of SQL - Spark SQL.\nThe several SQL providers are very similar:\nSELECT [attributes]\nFROM [table]\nWHERE [filter]\nGROUP BY [grouping attributes]\nHAVING [filtering the groups]\nORDER BY [attribute to order]\n(INNER/FULL/LEFT/RIGHT) JOIN [table2]\nON [attributes table joining table2] (...)\nWhat differs the most between several SQL providers are built-in functions.\nFor Built-in Spark SQL function check this link: https://spark.apache.org/docs/latest/api/sql/index.html\nExtra information on SPARK SQL :\nhttps://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data.",
        "question": "What are the key components of a typical SQL query as outlined in the text?",
        "section": "Module 5: pyspark",
        "document_id": 329,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Actually Spark SQL is one independent “type” of SQL - Spark SQL.\nThe several SQL providers are very similar:\nSELECT [attributes]\nFROM [table]\nWHERE [filter]\nGROUP BY [grouping attributes]\nHAVING [filtering the groups]\nORDER BY [attribute to order]\n(INNER/FULL/LEFT/RIGHT) JOIN [table2]\nON [attributes table joining table2] (...)\nWhat differs the most between several SQL providers are built-in functions.\nFor Built-in Spark SQL function check this link: https://spark.apache.org/docs/latest/api/sql/index.html\nExtra information on SPARK SQL :\nhttps://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data.",
        "question": "Where can one find information about built-in functions for Spark SQL?",
        "section": "Module 5: pyspark",
        "document_id": 329,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Actually Spark SQL is one independent “type” of SQL - Spark SQL.\nThe several SQL providers are very similar:\nSELECT [attributes]\nFROM [table]\nWHERE [filter]\nGROUP BY [grouping attributes]\nHAVING [filtering the groups]\nORDER BY [attribute to order]\n(INNER/FULL/LEFT/RIGHT) JOIN [table2]\nON [attributes table joining table2] (...)\nWhat differs the most between several SQL providers are built-in functions.\nFor Built-in Spark SQL function check this link: https://spark.apache.org/docs/latest/api/sql/index.html\nExtra information on SPARK SQL :\nhttps://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data.",
        "question": "What resources are provided for extra information on Spark SQL?",
        "section": "Module 5: pyspark",
        "document_id": 329,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅Solution: I had two notebooks running, and the one I wanted to look at had opened a port on localhost:4041.\nIf a port is in use, then Spark uses the next available port number. It can be even 4044. Clean up after yourself when a port does not work or a container does not run.\nYou can run spark.sparkContext.uiWebUrl\nand result will be some like\n'http://172.19.10.61:4041'",
        "question": "What should you do if a port is in use when running Spark?",
        "section": "Module 5: pyspark",
        "document_id": 330,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅Solution: I had two notebooks running, and the one I wanted to look at had opened a port on localhost:4041.\nIf a port is in use, then Spark uses the next available port number. It can be even 4044. Clean up after yourself when a port does not work or a container does not run.\nYou can run spark.sparkContext.uiWebUrl\nand result will be some like\n'http://172.19.10.61:4041'",
        "question": "How can you determine the web UI URL for Spark's context?",
        "section": "Module 5: pyspark",
        "document_id": 330,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅Solution: I had two notebooks running, and the one I wanted to look at had opened a port on localhost:4041.\nIf a port is in use, then Spark uses the next available port number. It can be even 4044. Clean up after yourself when a port does not work or a container does not run.\nYou can run spark.sparkContext.uiWebUrl\nand result will be some like\n'http://172.19.10.61:4041'",
        "question": "What does Spark do when a port number is already occupied?",
        "section": "Module 5: pyspark",
        "document_id": 330,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅Solution: I had two notebooks running, and the one I wanted to look at had opened a port on localhost:4041.\nIf a port is in use, then Spark uses the next available port number. It can be even 4044. Clean up after yourself when a port does not work or a container does not run.\nYou can run spark.sparkContext.uiWebUrl\nand result will be some like\n'http://172.19.10.61:4041'",
        "question": "What is the significance of localhost:4041 in the text?",
        "section": "Module 5: pyspark",
        "document_id": 330,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅Solution: I had two notebooks running, and the one I wanted to look at had opened a port on localhost:4041.\nIf a port is in use, then Spark uses the next available port number. It can be even 4044. Clean up after yourself when a port does not work or a container does not run.\nYou can run spark.sparkContext.uiWebUrl\nand result will be some like\n'http://172.19.10.61:4041'",
        "question": "What is implied by the instruction to 'clean up after yourself' in relation to ports?",
        "section": "Module 5: pyspark",
        "document_id": 330,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅Solution: replace Java Developer Kit 11 with Java Developer Kit 8.\nJava - RuntimeError: Java gateway process exited before sending its port number\nShows java_home is not set on the notebook log\nhttps://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/\nhttps://twitter.com/drkrishnaanand/status/1765423415878463839",
        "question": "What version of the Java Developer Kit should be replaced according to the solution?",
        "section": "Module 5: pyspark",
        "document_id": 331,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅Solution: replace Java Developer Kit 11 with Java Developer Kit 8.\nJava - RuntimeError: Java gateway process exited before sending its port number\nShows java_home is not set on the notebook log\nhttps://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/\nhttps://twitter.com/drkrishnaanand/status/1765423415878463839",
        "question": "What error is mentioned regarding the Java gateway process?",
        "section": "Module 5: pyspark",
        "document_id": 331,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅Solution: replace Java Developer Kit 11 with Java Developer Kit 8.\nJava - RuntimeError: Java gateway process exited before sending its port number\nShows java_home is not set on the notebook log\nhttps://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/\nhttps://twitter.com/drkrishnaanand/status/1765423415878463839",
        "question": "What issue does the notebook log indicate about 'java_home'?",
        "section": "Module 5: pyspark",
        "document_id": 331,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅Solution: replace Java Developer Kit 11 with Java Developer Kit 8.\nJava - RuntimeError: Java gateway process exited before sending its port number\nShows java_home is not set on the notebook log\nhttps://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/\nhttps://twitter.com/drkrishnaanand/status/1765423415878463839",
        "question": "Where can one find information about the PySpark exception related to the Java gateway process?",
        "section": "Module 5: pyspark",
        "document_id": 331,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅Solution: replace Java Developer Kit 11 with Java Developer Kit 8.\nJava - RuntimeError: Java gateway process exited before sending its port number\nShows java_home is not set on the notebook log\nhttps://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/\nhttps://twitter.com/drkrishnaanand/status/1765423415878463839",
        "question": "What platform is referenced to discuss the Java gateway process error?",
        "section": "Module 5: pyspark",
        "document_id": 331,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1\nI also added the google_credentials.json and .p12 to auth with gcs. These files are downloadable from GCP Service account.\nTo create the SparkSession:\nspark = SparkSession.builder.master('local[*]') \\\n.appName('spark-read-from-bigquery') \\\n.config('BigQueryProjectId','razor-project-xxxxxxx) \\\n.config('BigQueryDatasetLocation','de_final_data') \\\n.config('parentProject','razor-project-xxxxxxx) \\\n.config(\"google.cloud.auth.service.account.enable\", \"true\") \\\n.config(\"credentialsFile\", \"google_credentials.json\") \\\n.config(\"GcpJsonKeyFile\", \"google_credentials.json\") \\\n.config(\"spark.driver.memory\", \"4g\") \\\n.config(\"spark.executor.memory\", \"2g\") \\\n.config(\"spark.memory.offHeap.enabled\",True) \\\n.config(\"spark.memory.offHeap.size\",\"5g\") \\\n.config('google.cloud.auth.service.account.json.keyfile', \"google_credentials.json\") \\\n.config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") \\\n.config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n.config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n.getOrCreate()",
        "question": "What version of the gcs-connector was used?",
        "section": "Module 5: pyspark",
        "document_id": 332,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1\nI also added the google_credentials.json and .p12 to auth with gcs. These files are downloadable from GCP Service account.\nTo create the SparkSession:\nspark = SparkSession.builder.master('local[*]') \\\n.appName('spark-read-from-bigquery') \\\n.config('BigQueryProjectId','razor-project-xxxxxxx) \\\n.config('BigQueryDatasetLocation','de_final_data') \\\n.config('parentProject','razor-project-xxxxxxx) \\\n.config(\"google.cloud.auth.service.account.enable\", \"true\") \\\n.config(\"credentialsFile\", \"google_credentials.json\") \\\n.config(\"GcpJsonKeyFile\", \"google_credentials.json\") \\\n.config(\"spark.driver.memory\", \"4g\") \\\n.config(\"spark.executor.memory\", \"2g\") \\\n.config(\"spark.memory.offHeap.enabled\",True) \\\n.config(\"spark.memory.offHeap.size\",\"5g\") \\\n.config('google.cloud.auth.service.account.json.keyfile', \"google_credentials.json\") \\\n.config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") \\\n.config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n.config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n.getOrCreate()",
        "question": "What files are necessary for authentication with GCS?",
        "section": "Module 5: pyspark",
        "document_id": 332,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1\nI also added the google_credentials.json and .p12 to auth with gcs. These files are downloadable from GCP Service account.\nTo create the SparkSession:\nspark = SparkSession.builder.master('local[*]') \\\n.appName('spark-read-from-bigquery') \\\n.config('BigQueryProjectId','razor-project-xxxxxxx) \\\n.config('BigQueryDatasetLocation','de_final_data') \\\n.config('parentProject','razor-project-xxxxxxx) \\\n.config(\"google.cloud.auth.service.account.enable\", \"true\") \\\n.config(\"credentialsFile\", \"google_credentials.json\") \\\n.config(\"GcpJsonKeyFile\", \"google_credentials.json\") \\\n.config(\"spark.driver.memory\", \"4g\") \\\n.config(\"spark.executor.memory\", \"2g\") \\\n.config(\"spark.memory.offHeap.enabled\",True) \\\n.config(\"spark.memory.offHeap.size\",\"5g\") \\\n.config('google.cloud.auth.service.account.json.keyfile', \"google_credentials.json\") \\\n.config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") \\\n.config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n.config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n.getOrCreate()",
        "question": "What is the purpose of the 'spark.memory.offHeap.enabled' configuration?",
        "section": "Module 5: pyspark",
        "document_id": 332,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1\nI also added the google_credentials.json and .p12 to auth with gcs. These files are downloadable from GCP Service account.\nTo create the SparkSession:\nspark = SparkSession.builder.master('local[*]') \\\n.appName('spark-read-from-bigquery') \\\n.config('BigQueryProjectId','razor-project-xxxxxxx) \\\n.config('BigQueryDatasetLocation','de_final_data') \\\n.config('parentProject','razor-project-xxxxxxx) \\\n.config(\"google.cloud.auth.service.account.enable\", \"true\") \\\n.config(\"credentialsFile\", \"google_credentials.json\") \\\n.config(\"GcpJsonKeyFile\", \"google_credentials.json\") \\\n.config(\"spark.driver.memory\", \"4g\") \\\n.config(\"spark.executor.memory\", \"2g\") \\\n.config(\"spark.memory.offHeap.enabled\",True) \\\n.config(\"spark.memory.offHeap.size\",\"5g\") \\\n.config('google.cloud.auth.service.account.json.keyfile', \"google_credentials.json\") \\\n.config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") \\\n.config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n.config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n.getOrCreate()",
        "question": "How is the SparkSession configured to read from BigQuery?",
        "section": "Module 5: pyspark",
        "document_id": 332,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1\nI also added the google_credentials.json and .p12 to auth with gcs. These files are downloadable from GCP Service account.\nTo create the SparkSession:\nspark = SparkSession.builder.master('local[*]') \\\n.appName('spark-read-from-bigquery') \\\n.config('BigQueryProjectId','razor-project-xxxxxxx) \\\n.config('BigQueryDatasetLocation','de_final_data') \\\n.config('parentProject','razor-project-xxxxxxx) \\\n.config(\"google.cloud.auth.service.account.enable\", \"true\") \\\n.config(\"credentialsFile\", \"google_credentials.json\") \\\n.config(\"GcpJsonKeyFile\", \"google_credentials.json\") \\\n.config(\"spark.driver.memory\", \"4g\") \\\n.config(\"spark.executor.memory\", \"2g\") \\\n.config(\"spark.memory.offHeap.enabled\",True) \\\n.config(\"spark.memory.offHeap.size\",\"5g\") \\\n.config('google.cloud.auth.service.account.json.keyfile', \"google_credentials.json\") \\\n.config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") \\\n.config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n.config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n.getOrCreate()",
        "question": "What parameters are set for Spark driver and executor memory in the configuration?",
        "section": "Module 5: pyspark",
        "document_id": 332,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "While creating a SparkSession using the config spark.jars.packages as com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\nspark = SparkSession.builder.master('local').appName('bq').config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\").getOrCreate()\nautomatically downloads the required dependency jars and configures the connector, removing the need to manage this dependency. More details available here",
        "question": "What is the purpose of the config spark.jars.packages in a SparkSession?",
        "section": "Module 5: pyspark",
        "document_id": 333,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "While creating a SparkSession using the config spark.jars.packages as com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\nspark = SparkSession.builder.master('local').appName('bq').config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\").getOrCreate()\nautomatically downloads the required dependency jars and configures the connector, removing the need to manage this dependency. More details available here",
        "question": "Which version of the spark-bigquery-with-dependencies package is mentioned in the text?",
        "section": "Module 5: pyspark",
        "document_id": 333,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "While creating a SparkSession using the config spark.jars.packages as com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\nspark = SparkSession.builder.master('local').appName('bq').config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\").getOrCreate()\nautomatically downloads the required dependency jars and configures the connector, removing the need to manage this dependency. More details available here",
        "question": "What does the SparkSession.builder.master('local') method do?",
        "section": "Module 5: pyspark",
        "document_id": 333,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "While creating a SparkSession using the config spark.jars.packages as com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\nspark = SparkSession.builder.master('local').appName('bq').config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\").getOrCreate()\nautomatically downloads the required dependency jars and configures the connector, removing the need to manage this dependency. More details available here",
        "question": "How does the configuration of spark.jars.packages simplify dependency management?",
        "section": "Module 5: pyspark",
        "document_id": 333,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "While creating a SparkSession using the config spark.jars.packages as com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\nspark = SparkSession.builder.master('local').appName('bq').config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\").getOrCreate()\nautomatically downloads the required dependency jars and configures the connector, removing the need to manage this dependency. More details available here",
        "question": "What is the significance of the appName parameter in the SparkSession builder?",
        "section": "Module 5: pyspark",
        "document_id": 333,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\nThere’s a few extra steps to go into reading from GCS with PySpark\n1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\nAs the name implies, this .jar file is what essentially connects PySpark with your GCS\n2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\n3.) In your Python script, there are a few extra classes you’ll have to import:\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.conf import SparkConf\nfrom pyspark.context import SparkContext\n4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\nconf = SparkConf() \\\n.setMaster('local[*]') \\\n.setAppName('test') \\\n.set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\n.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\nsc = SparkContext(conf=conf)\nsc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:\nspark = SparkSession.builder \\\n.config(conf=sc.getConf()) \\\n.getOrCreate()\n6.) Finally, you’re able to read your files straight from GCS!\ndf_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")",
        "question": "What is the main topic discussed in the Slack thread?",
        "section": "Module 5: pyspark",
        "document_id": 334,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\nThere’s a few extra steps to go into reading from GCS with PySpark\n1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\nAs the name implies, this .jar file is what essentially connects PySpark with your GCS\n2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\n3.) In your Python script, there are a few extra classes you’ll have to import:\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.conf import SparkConf\nfrom pyspark.context import SparkContext\n4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\nconf = SparkConf() \\\n.setMaster('local[*]') \\\n.setAppName('test') \\\n.set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\n.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\nsc = SparkContext(conf=conf)\nsc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:\nspark = SparkSession.builder \\\n.config(conf=sc.getConf()) \\\n.getOrCreate()\n6.) Finally, you’re able to read your files straight from GCS!\ndf_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")",
        "question": "What is the first step to read from GCP data lake using PySpark?",
        "section": "Module 5: pyspark",
        "document_id": 334,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\nThere’s a few extra steps to go into reading from GCS with PySpark\n1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\nAs the name implies, this .jar file is what essentially connects PySpark with your GCS\n2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\n3.) In your Python script, there are a few extra classes you’ll have to import:\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.conf import SparkConf\nfrom pyspark.context import SparkContext\n4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\nconf = SparkConf() \\\n.setMaster('local[*]') \\\n.setAppName('test') \\\n.set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\n.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\nsc = SparkContext(conf=conf)\nsc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:\nspark = SparkSession.builder \\\n.config(conf=sc.getConf()) \\\n.getOrCreate()\n6.) Finally, you’re able to read your files straight from GCS!\ndf_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")",
        "question": "Where should the Cloud Storage connector .jar file be moved after downloading it?",
        "section": "Module 5: pyspark",
        "document_id": 334,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\nThere’s a few extra steps to go into reading from GCS with PySpark\n1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\nAs the name implies, this .jar file is what essentially connects PySpark with your GCS\n2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\n3.) In your Python script, there are a few extra classes you’ll have to import:\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.conf import SparkConf\nfrom pyspark.context import SparkContext\n4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\nconf = SparkConf() \\\n.setMaster('local[*]') \\\n.setAppName('test') \\\n.set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\n.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\nsc = SparkContext(conf=conf)\nsc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:\nspark = SparkSession.builder \\\n.config(conf=sc.getConf()) \\\n.getOrCreate()\n6.) Finally, you’re able to read your files straight from GCS!\ndf_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")",
        "question": "What are some of the key classes that need to be imported in the Python script for PySpark?",
        "section": "Module 5: pyspark",
        "document_id": 334,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\nThere’s a few extra steps to go into reading from GCS with PySpark\n1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\nAs the name implies, this .jar file is what essentially connects PySpark with your GCS\n2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\n3.) In your Python script, there are a few extra classes you’ll have to import:\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.conf import SparkConf\nfrom pyspark.context import SparkContext\n4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\nconf = SparkConf() \\\n.setMaster('local[*]') \\\n.setAppName('test') \\\n.set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\n.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\nsc = SparkContext(conf=conf)\nsc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:\nspark = SparkSession.builder \\\n.config(conf=sc.getConf()) \\\n.getOrCreate()\n6.) Finally, you’re able to read your files straight from GCS!\ndf_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")",
        "question": "What code snippet is given for setting up configurations before building the SparkSession?",
        "section": "Module 5: pyspark",
        "document_id": 334,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "from pyarrow.parquet import ParquetFile\npf = ParquetFile('fhvhv_tripdata_2021-01.parquet')\n#pyarrow builds tables, not dataframes\ntbl_small = next(pf.iter_batches(batch_size = 1000))\n#this function converts the table to a dataframe of manageable size\ndf = tbl_small.to_pandas()\nAlternatively without PyArrow:\ndf = spark.read.parquet('fhvhv_tripdata_2021-01.parquet')\ndf1 = df.sort('DOLocationID').limit(1000)\npdf = df1.select(\"*\").toPandas()\ngcsu",
        "question": "What is the purpose of the ParquetFile function in the provided code?",
        "section": "Module 5: pyspark",
        "document_id": 335,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "from pyarrow.parquet import ParquetFile\npf = ParquetFile('fhvhv_tripdata_2021-01.parquet')\n#pyarrow builds tables, not dataframes\ntbl_small = next(pf.iter_batches(batch_size = 1000))\n#this function converts the table to a dataframe of manageable size\ndf = tbl_small.to_pandas()\nAlternatively without PyArrow:\ndf = spark.read.parquet('fhvhv_tripdata_2021-01.parquet')\ndf1 = df.sort('DOLocationID').limit(1000)\npdf = df1.select(\"*\").toPandas()\ngcsu",
        "question": "How does the code retrieve a manageable size dataframe from the Parquet file using PyArrow?",
        "section": "Module 5: pyspark",
        "document_id": 335,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "from pyarrow.parquet import ParquetFile\npf = ParquetFile('fhvhv_tripdata_2021-01.parquet')\n#pyarrow builds tables, not dataframes\ntbl_small = next(pf.iter_batches(batch_size = 1000))\n#this function converts the table to a dataframe of manageable size\ndf = tbl_small.to_pandas()\nAlternatively without PyArrow:\ndf = spark.read.parquet('fhvhv_tripdata_2021-01.parquet')\ndf1 = df.sort('DOLocationID').limit(1000)\npdf = df1.select(\"*\").toPandas()\ngcsu",
        "question": "What is the significance of the batch_size parameter in the iter_batches method?",
        "section": "Module 5: pyspark",
        "document_id": 335,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "from pyarrow.parquet import ParquetFile\npf = ParquetFile('fhvhv_tripdata_2021-01.parquet')\n#pyarrow builds tables, not dataframes\ntbl_small = next(pf.iter_batches(batch_size = 1000))\n#this function converts the table to a dataframe of manageable size\ndf = tbl_small.to_pandas()\nAlternatively without PyArrow:\ndf = spark.read.parquet('fhvhv_tripdata_2021-01.parquet')\ndf1 = df.sort('DOLocationID').limit(1000)\npdf = df1.select(\"*\").toPandas()\ngcsu",
        "question": "What alternative method is shown for reading Parquet files without using PyArrow?",
        "section": "Module 5: pyspark",
        "document_id": 335,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "from pyarrow.parquet import ParquetFile\npf = ParquetFile('fhvhv_tripdata_2021-01.parquet')\n#pyarrow builds tables, not dataframes\ntbl_small = next(pf.iter_batches(batch_size = 1000))\n#this function converts the table to a dataframe of manageable size\ndf = tbl_small.to_pandas()\nAlternatively without PyArrow:\ndf = spark.read.parquet('fhvhv_tripdata_2021-01.parquet')\ndf1 = df.sort('DOLocationID').limit(1000)\npdf = df1.select(\"*\").toPandas()\ngcsu",
        "question": "How does the code sort the dataframe based on the DOLocationID and limit the results?",
        "section": "Module 5: pyspark",
        "document_id": 335,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Probably you’ll encounter this if you followed the video ‘5.3.1 - First Look at Spark/PySpark’ and used the parquet file from the TLC website (csv was used in the video).\nWhen defining the schema, the PULocation and DOLocationID are defined as IntegerType. This will cause an error because the Parquet file is INT64 and you’ll get an error like:\nParquet column cannot be converted in file [...] Column [...] Expected: int, Found: INT64\nChange the schema definition from IntegerType to LongType and it should work",
        "question": "What file format was used in the video '5.3.1 - First Look at Spark/PySpark'?",
        "section": "Module 5: pyspark",
        "document_id": 336,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Probably you’ll encounter this if you followed the video ‘5.3.1 - First Look at Spark/PySpark’ and used the parquet file from the TLC website (csv was used in the video).\nWhen defining the schema, the PULocation and DOLocationID are defined as IntegerType. This will cause an error because the Parquet file is INT64 and you’ll get an error like:\nParquet column cannot be converted in file [...] Column [...] Expected: int, Found: INT64\nChange the schema definition from IntegerType to LongType and it should work",
        "question": "What data type is expected for PULocation and DOLocationID in the schema?",
        "section": "Module 5: pyspark",
        "document_id": 336,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Probably you’ll encounter this if you followed the video ‘5.3.1 - First Look at Spark/PySpark’ and used the parquet file from the TLC website (csv was used in the video).\nWhen defining the schema, the PULocation and DOLocationID are defined as IntegerType. This will cause an error because the Parquet file is INT64 and you’ll get an error like:\nParquet column cannot be converted in file [...] Column [...] Expected: int, Found: INT64\nChange the schema definition from IntegerType to LongType and it should work",
        "question": "What error occurs when using IntegerType for the schema definition with the Parquet file?",
        "section": "Module 5: pyspark",
        "document_id": 336,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Probably you’ll encounter this if you followed the video ‘5.3.1 - First Look at Spark/PySpark’ and used the parquet file from the TLC website (csv was used in the video).\nWhen defining the schema, the PULocation and DOLocationID are defined as IntegerType. This will cause an error because the Parquet file is INT64 and you’ll get an error like:\nParquet column cannot be converted in file [...] Column [...] Expected: int, Found: INT64\nChange the schema definition from IntegerType to LongType and it should work",
        "question": "What is the correct data type to use in the schema definition for the Parquet file?",
        "section": "Module 5: pyspark",
        "document_id": 336,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Probably you’ll encounter this if you followed the video ‘5.3.1 - First Look at Spark/PySpark’ and used the parquet file from the TLC website (csv was used in the video).\nWhen defining the schema, the PULocation and DOLocationID are defined as IntegerType. This will cause an error because the Parquet file is INT64 and you’ll get an error like:\nParquet column cannot be converted in file [...] Column [...] Expected: int, Found: INT64\nChange the schema definition from IntegerType to LongType and it should work",
        "question": "What should be changed in the schema to avoid the error related to INT64?",
        "section": "Module 5: pyspark",
        "document_id": 336,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "df_finalx=df_finalw.select([col(x).alias(x.replace(\" \",\"\")) for x in df_finalw.columns])\nKrishna Anand",
        "question": "What is the purpose of the `select` method in the given code?",
        "section": "Module 5: pyspark",
        "document_id": 337,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "df_finalx=df_finalw.select([col(x).alias(x.replace(\" \",\"\")) for x in df_finalw.columns])\nKrishna Anand",
        "question": "How is aliasing achieved in the code snippet?",
        "section": "Module 5: pyspark",
        "document_id": 337,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "df_finalx=df_finalw.select([col(x).alias(x.replace(\" \",\"\")) for x in df_finalw.columns])\nKrishna Anand",
        "question": "What does the `col` function do in the context of the code?",
        "section": "Module 5: pyspark",
        "document_id": 337,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "df_finalx=df_finalw.select([col(x).alias(x.replace(\" \",\"\")) for x in df_finalw.columns])\nKrishna Anand",
        "question": "What is the significance of the `replace` method in this code?",
        "section": "Module 5: pyspark",
        "document_id": 337,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "df_finalx=df_finalw.select([col(x).alias(x.replace(\" \",\"\")) for x in df_finalw.columns])\nKrishna Anand",
        "question": "What type of data structure is `df_finalw` likely to be based on the context?",
        "section": "Module 5: pyspark",
        "document_id": 337,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\nbecause as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\nSo when you run the command spark.createDataFrame(df1_pandas).show(),\nYou get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\nAnother option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\npd.DataFrame.iteritems = pd.DataFrame.items\nNote that this problem is solved with Spark versions from 3.4.1",
        "question": "What error is encountered when running the command spark.createDataFrame(df1_pandas).show()?",
        "section": "Module 5: pyspark",
        "document_id": 338,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\nbecause as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\nSo when you run the command spark.createDataFrame(df1_pandas).show(),\nYou get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\nAnother option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\npd.DataFrame.iteritems = pd.DataFrame.items\nNote that this problem is solved with Spark versions from 3.4.1",
        "question": "What version of pandas is required to avoid the Attribute error in Spark 3.3.2?",
        "section": "Module 5: pyspark",
        "document_id": 338,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\nbecause as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\nSo when you run the command spark.createDataFrame(df1_pandas).show(),\nYou get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\nAnother option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\npd.DataFrame.iteritems = pd.DataFrame.items\nNote that this problem is solved with Spark versions from 3.4.1",
        "question": "What is an alternative to downgrading pandas if the user wants to maintain the current version?",
        "section": "Module 5: pyspark",
        "document_id": 338,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\nbecause as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\nSo when you run the command spark.createDataFrame(df1_pandas).show(),\nYou get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\nAnother option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\npd.DataFrame.iteritems = pd.DataFrame.items\nNote that this problem is solved with Spark versions from 3.4.1",
        "question": "Which version of Spark resolves the compatibility issue with pandas version 2.0.0?",
        "section": "Module 5: pyspark",
        "document_id": 338,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\nbecause as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\nSo when you run the command spark.createDataFrame(df1_pandas).show(),\nYou get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\nAnother option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\npd.DataFrame.iteritems = pd.DataFrame.items\nNote that this problem is solved with Spark versions from 3.4.1",
        "question": "What command can be used to downgrade pandas to the required version?",
        "section": "Module 5: pyspark",
        "document_id": 338,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Another alternative is to install pandas 2.0.1 (it worked well as at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"",
        "question": "What version of pandas is mentioned in the text?",
        "section": "Module 5: pyspark",
        "document_id": 339,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Another alternative is to install pandas 2.0.1 (it worked well as at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"",
        "question": "Which version of Pyspark is compatible with pandas 2.0.1?",
        "section": "Module 5: pyspark",
        "document_id": 339,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Another alternative is to install pandas 2.0.1 (it worked well as at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"",
        "question": "What command is given to set the SPARK_HOME environment variable?",
        "section": "Module 5: pyspark",
        "document_id": 339,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Another alternative is to install pandas 2.0.1 (it worked well as at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"",
        "question": "How should the PATH environment variable be modified according to the text?",
        "section": "Module 5: pyspark",
        "document_id": 339,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Another alternative is to install pandas 2.0.1 (it worked well as at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"",
        "question": "What is the directory structure indicated for the SPARK_HOME variable?",
        "section": "Module 5: pyspark",
        "document_id": 339,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Open a CMD terminal in administrator mode\ncd %SPARK_HOME%\nStart a master node: bin\\spark-class org.apache.spark.deploy.master.Master\nStart a worker node: bin\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\nbin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\nspark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\nUse --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\nNow you can access Spark UI through localhost:8080\nHomework for Module 5:\nDo not refer to the homework file located under /05-batch/code/. The correct file is located under\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md",
        "question": "What command is used to start a master node in Spark?",
        "section": "Module 5: pyspark",
        "document_id": 340,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Open a CMD terminal in administrator mode\ncd %SPARK_HOME%\nStart a master node: bin\\spark-class org.apache.spark.deploy.master.Master\nStart a worker node: bin\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\nbin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\nspark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\nUse --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\nNow you can access Spark UI through localhost:8080\nHomework for Module 5:\nDo not refer to the homework file located under /05-batch/code/. The correct file is located under\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md",
        "question": "How do you start a worker node in Spark?",
        "section": "Module 5: pyspark",
        "document_id": 340,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Open a CMD terminal in administrator mode\ncd %SPARK_HOME%\nStart a master node: bin\\spark-class org.apache.spark.deploy.master.Master\nStart a worker node: bin\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\nbin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\nspark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\nUse --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\nNow you can access Spark UI through localhost:8080\nHomework for Module 5:\nDo not refer to the homework file located under /05-batch/code/. The correct file is located under\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md",
        "question": "What is the default address for accessing Spark UI?",
        "section": "Module 5: pyspark",
        "document_id": 340,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Open a CMD terminal in administrator mode\ncd %SPARK_HOME%\nStart a master node: bin\\spark-class org.apache.spark.deploy.master.Master\nStart a worker node: bin\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\nbin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\nspark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\nUse --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\nNow you can access Spark UI through localhost:8080\nHomework for Module 5:\nDo not refer to the homework file located under /05-batch/code/. The correct file is located under\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md",
        "question": "Where can the correct homework file for Module 5 be found?",
        "section": "Module 5: pyspark",
        "document_id": 340,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Open a CMD terminal in administrator mode\ncd %SPARK_HOME%\nStart a master node: bin\\spark-class org.apache.spark.deploy.master.Master\nStart a worker node: bin\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\nbin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\nspark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\nUse --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\nNow you can access Spark UI through localhost:8080\nHomework for Module 5:\nDo not refer to the homework file located under /05-batch/code/. The correct file is located under\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md",
        "question": "What should be done if you want to run the worker on a different machine?",
        "section": "Module 5: pyspark",
        "document_id": 340,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can either type the export command every time you run a new session, add it to the .bashrc/ which you can find in /home or run this command at the beginning of your homebook:\nimport findspark\nfindspark.init()",
        "question": "What is the purpose of the export command in a new session?",
        "section": "Module 5: pyspark",
        "document_id": 341,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can either type the export command every time you run a new session, add it to the .bashrc/ which you can find in /home or run this command at the beginning of your homebook:\nimport findspark\nfindspark.init()",
        "question": "Where can you find the .bashrc file?",
        "section": "Module 5: pyspark",
        "document_id": 341,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can either type the export command every time you run a new session, add it to the .bashrc/ which you can find in /home or run this command at the beginning of your homebook:\nimport findspark\nfindspark.init()",
        "question": "What command needs to be run to initialize findspark?",
        "section": "Module 5: pyspark",
        "document_id": 341,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can either type the export command every time you run a new session, add it to the .bashrc/ which you can find in /home or run this command at the beginning of your homebook:\nimport findspark\nfindspark.init()",
        "question": "What is findspark used for?",
        "section": "Module 5: pyspark",
        "document_id": 341,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can either type the export command every time you run a new session, add it to the .bashrc/ which you can find in /home or run this command at the beginning of your homebook:\nimport findspark\nfindspark.init()",
        "question": "What does adding the export command to .bashrc achieve?",
        "section": "Module 5: pyspark",
        "document_id": 341,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I solved this issue: unzip the file with:\nf\nbefore creating head.csv",
        "question": "What file was needed to be unzipped?",
        "section": "Module 5: pyspark",
        "document_id": 342,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I solved this issue: unzip the file with:\nf\nbefore creating head.csv",
        "question": "What is the purpose of creating head.csv?",
        "section": "Module 5: pyspark",
        "document_id": 342,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I solved this issue: unzip the file with:\nf\nbefore creating head.csv",
        "question": "What action was taken to solve the issue?",
        "section": "Module 5: pyspark",
        "document_id": 342,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I solved this issue: unzip the file with:\nf\nbefore creating head.csv",
        "question": "Which command was mentioned for unzipping the file?",
        "section": "Module 5: pyspark",
        "document_id": 342,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I solved this issue: unzip the file with:\nf\nbefore creating head.csv",
        "question": "What was the sequence of actions in solving the issue?",
        "section": "Module 5: pyspark",
        "document_id": 342,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the code along from Video 5.3.3 Alexey downloads the CSV files from the NYT website and gzips them in their bash script. If we now (2023) follow along but download the data from the GH course Repo, it will already be zippes as csv.gz files. Therefore we zip it again if we follow the code from the video exactly. This then leads to gibberish outcome when we then try to cat the contents or count the lines with zcat, because the file is zipped twitch and zcat only unzips it once.\n✅solution: do not gzip the files downloaded from the course repo. Just wget them and save them as they are as csv.gz files. Then the zcat command and the showSchema command will also work\nURL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\nLOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\nLOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\nLOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\necho \"downloading ${URL} to ${LOCAL_PATH}\"\nmkdir -p ${LOCAL_PREFIX}\nwget ${URL} -O ${LOCAL_PATH}\necho \"compressing ${LOCAL_PATH}\"\n# gzip ${LOCAL_PATH} <- uncomment this line",
        "question": "What is the significance of downloading CSV files from the NYT website in the context of Video 5.3.3?",
        "section": "Module 5: pyspark",
        "document_id": 343,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the code along from Video 5.3.3 Alexey downloads the CSV files from the NYT website and gzips them in their bash script. If we now (2023) follow along but download the data from the GH course Repo, it will already be zippes as csv.gz files. Therefore we zip it again if we follow the code from the video exactly. This then leads to gibberish outcome when we then try to cat the contents or count the lines with zcat, because the file is zipped twitch and zcat only unzips it once.\n✅solution: do not gzip the files downloaded from the course repo. Just wget them and save them as they are as csv.gz files. Then the zcat command and the showSchema command will also work\nURL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\nLOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\nLOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\nLOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\necho \"downloading ${URL} to ${LOCAL_PATH}\"\nmkdir -p ${LOCAL_PREFIX}\nwget ${URL} -O ${LOCAL_PATH}\necho \"compressing ${LOCAL_PATH}\"\n# gzip ${LOCAL_PATH} <- uncomment this line",
        "question": "What issue arises from gzipping the already zipped csv.gz files downloaded from the GH course Repo?",
        "section": "Module 5: pyspark",
        "document_id": 343,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the code along from Video 5.3.3 Alexey downloads the CSV files from the NYT website and gzips them in their bash script. If we now (2023) follow along but download the data from the GH course Repo, it will already be zippes as csv.gz files. Therefore we zip it again if we follow the code from the video exactly. This then leads to gibberish outcome when we then try to cat the contents or count the lines with zcat, because the file is zipped twitch and zcat only unzips it once.\n✅solution: do not gzip the files downloaded from the course repo. Just wget them and save them as they are as csv.gz files. Then the zcat command and the showSchema command will also work\nURL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\nLOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\nLOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\nLOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\necho \"downloading ${URL} to ${LOCAL_PATH}\"\nmkdir -p ${LOCAL_PREFIX}\nwget ${URL} -O ${LOCAL_PATH}\necho \"compressing ${LOCAL_PATH}\"\n# gzip ${LOCAL_PATH} <- uncomment this line",
        "question": "What is the recommended solution for handling files downloaded from the course repo?",
        "section": "Module 5: pyspark",
        "document_id": 343,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the code along from Video 5.3.3 Alexey downloads the CSV files from the NYT website and gzips them in their bash script. If we now (2023) follow along but download the data from the GH course Repo, it will already be zippes as csv.gz files. Therefore we zip it again if we follow the code from the video exactly. This then leads to gibberish outcome when we then try to cat the contents or count the lines with zcat, because the file is zipped twitch and zcat only unzips it once.\n✅solution: do not gzip the files downloaded from the course repo. Just wget them and save them as they are as csv.gz files. Then the zcat command and the showSchema command will also work\nURL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\nLOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\nLOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\nLOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\necho \"downloading ${URL} to ${LOCAL_PATH}\"\nmkdir -p ${LOCAL_PREFIX}\nwget ${URL} -O ${LOCAL_PATH}\necho \"compressing ${LOCAL_PATH}\"\n# gzip ${LOCAL_PATH} <- uncomment this line",
        "question": "What is the purpose of the wget command in the provided bash script?",
        "section": "Module 5: pyspark",
        "document_id": 343,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the code along from Video 5.3.3 Alexey downloads the CSV files from the NYT website and gzips them in their bash script. If we now (2023) follow along but download the data from the GH course Repo, it will already be zippes as csv.gz files. Therefore we zip it again if we follow the code from the video exactly. This then leads to gibberish outcome when we then try to cat the contents or count the lines with zcat, because the file is zipped twitch and zcat only unzips it once.\n✅solution: do not gzip the files downloaded from the course repo. Just wget them and save them as they are as csv.gz files. Then the zcat command and the showSchema command will also work\nURL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\nLOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\nLOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\nLOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\necho \"downloading ${URL} to ${LOCAL_PATH}\"\nmkdir -p ${LOCAL_PREFIX}\nwget ${URL} -O ${LOCAL_PATH}\necho \"compressing ${LOCAL_PATH}\"\n# gzip ${LOCAL_PATH} <- uncomment this line",
        "question": "What happens when the zcat command is used on a file that has been zipped twice?",
        "section": "Module 5: pyspark",
        "document_id": 343,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Occurred while running : spark.createDataFrame(df_pandas).show()\nThis error is usually due to the python version, since spark till date of 2 march 2023 doesn’t support python 3.11, try creating a new env with python version 3.8 and then run this command.\nOn the virtual machine, you can create a conda environment (here called myenv) with python 3.10 installed:\nconda create -n myenv python=3.10 anaconda\nThen you must run conda activate myenv to run python 3.10. Otherwise you’ll still be running version 3.11. You can deactivate by typing conda deactivate.",
        "question": "What command was being run when the error occurred?",
        "section": "Module 5: pyspark",
        "document_id": 344,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Occurred while running : spark.createDataFrame(df_pandas).show()\nThis error is usually due to the python version, since spark till date of 2 march 2023 doesn’t support python 3.11, try creating a new env with python version 3.8 and then run this command.\nOn the virtual machine, you can create a conda environment (here called myenv) with python 3.10 installed:\nconda create -n myenv python=3.10 anaconda\nThen you must run conda activate myenv to run python 3.10. Otherwise you’ll still be running version 3.11. You can deactivate by typing conda deactivate.",
        "question": "What is the main reason for the error discussed in the text?",
        "section": "Module 5: pyspark",
        "document_id": 344,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Occurred while running : spark.createDataFrame(df_pandas).show()\nThis error is usually due to the python version, since spark till date of 2 march 2023 doesn’t support python 3.11, try creating a new env with python version 3.8 and then run this command.\nOn the virtual machine, you can create a conda environment (here called myenv) with python 3.10 installed:\nconda create -n myenv python=3.10 anaconda\nThen you must run conda activate myenv to run python 3.10. Otherwise you’ll still be running version 3.11. You can deactivate by typing conda deactivate.",
        "question": "Which version of Python does Spark not support as of March 2, 2023?",
        "section": "Module 5: pyspark",
        "document_id": 344,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Occurred while running : spark.createDataFrame(df_pandas).show()\nThis error is usually due to the python version, since spark till date of 2 march 2023 doesn’t support python 3.11, try creating a new env with python version 3.8 and then run this command.\nOn the virtual machine, you can create a conda environment (here called myenv) with python 3.10 installed:\nconda create -n myenv python=3.10 anaconda\nThen you must run conda activate myenv to run python 3.10. Otherwise you’ll still be running version 3.11. You can deactivate by typing conda deactivate.",
        "question": "How can you create a new conda environment with Python 3.10?",
        "section": "Module 5: pyspark",
        "document_id": 344,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Occurred while running : spark.createDataFrame(df_pandas).show()\nThis error is usually due to the python version, since spark till date of 2 march 2023 doesn’t support python 3.11, try creating a new env with python version 3.8 and then run this command.\nOn the virtual machine, you can create a conda environment (here called myenv) with python 3.10 installed:\nconda create -n myenv python=3.10 anaconda\nThen you must run conda activate myenv to run python 3.10. Otherwise you’ll still be running version 3.11. You can deactivate by typing conda deactivate.",
        "question": "What command is used to deactivate a conda environment?",
        "section": "Module 5: pyspark",
        "document_id": 344,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure you have your credentials of your GCP in your VM under the location defined in the script.",
        "question": "What are the required credentials for GCP?",
        "section": "Module 5: pyspark",
        "document_id": 345,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure you have your credentials of your GCP in your VM under the location defined in the script.",
        "question": "Where should the GCP credentials be located in the VM?",
        "section": "Module 5: pyspark",
        "document_id": 345,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure you have your credentials of your GCP in your VM under the location defined in the script.",
        "question": "What does GCP stand for?",
        "section": "Module 5: pyspark",
        "document_id": 345,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure you have your credentials of your GCP in your VM under the location defined in the script.",
        "question": "What is the significance of the script in relation to credentials?",
        "section": "Module 5: pyspark",
        "document_id": 345,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure you have your credentials of your GCP in your VM under the location defined in the script.",
        "question": "How can I verify if my GCP credentials are correctly set up in the VM?",
        "section": "Module 5: pyspark",
        "document_id": 345,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To run spark in docker setup\n1. Build bitnami spark docker\na. clone bitnami repo using command\ngit clone https://github.com/bitnami/containers.git\n(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\nb. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update java and spark version as following\n\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\n\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\nreference: https://github.com/bitnami/containers/issues/13409\nc. build docker image by navigating to above directory and running docker build command\nnavigate cd bitnami/spark/3.3/debian-11/\nbuild command docker build -t spark:3.3-java-17 .\n2. run docker compose\nusing following file\n```yaml docker-compose.yml\nversion: '2'\nservices:\nspark:\nimage: spark:3.3-java-17\nenvironment:\n- SPARK_MODE=master\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\n- SPARK_RPC_ENCRYPTION_ENABLED=no\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n- SPARK_SSL_ENABLED=no\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8080:8080'\n- '7077:7077'\nspark-worker:\nimage: spark:3.3-java-17\nenvironment:\n- SPARK_MODE=worker\n- SPARK_MASTER_URL=spark://spark:7077\n- SPARK_WORKER_MEMORY=1G\n- SPARK_WORKER_CORES=1\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\n- SPARK_RPC_ENCRYPTION_ENABLED=no\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n- SPARK_SSL_ENABLED=no\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8081:8081'\nspark-nb:\nimage: jupyter/pyspark-notebook:java-17.0.5\nenvironment:\n- SPARK_MASTER_URL=spark://spark:7077\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8888:8888'\n- '4040:4040'\n```\nrun command to deploy docker compose\ndocker-compose up\nAccess jupyter notebook using link logged in docker compose logs\nSpark master url is spark://spark:7077",
        "question": "What is the first step to run Spark in a Docker setup?",
        "section": "Module 5: pyspark",
        "document_id": 346,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To run spark in docker setup\n1. Build bitnami spark docker\na. clone bitnami repo using command\ngit clone https://github.com/bitnami/containers.git\n(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\nb. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update java and spark version as following\n\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\n\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\nreference: https://github.com/bitnami/containers/issues/13409\nc. build docker image by navigating to above directory and running docker build command\nnavigate cd bitnami/spark/3.3/debian-11/\nbuild command docker build -t spark:3.3-java-17 .\n2. run docker compose\nusing following file\n```yaml docker-compose.yml\nversion: '2'\nservices:\nspark:\nimage: spark:3.3-java-17\nenvironment:\n- SPARK_MODE=master\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\n- SPARK_RPC_ENCRYPTION_ENABLED=no\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n- SPARK_SSL_ENABLED=no\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8080:8080'\n- '7077:7077'\nspark-worker:\nimage: spark:3.3-java-17\nenvironment:\n- SPARK_MODE=worker\n- SPARK_MASTER_URL=spark://spark:7077\n- SPARK_WORKER_MEMORY=1G\n- SPARK_WORKER_CORES=1\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\n- SPARK_RPC_ENCRYPTION_ENABLED=no\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n- SPARK_SSL_ENABLED=no\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8081:8081'\nspark-nb:\nimage: jupyter/pyspark-notebook:java-17.0.5\nenvironment:\n- SPARK_MASTER_URL=spark://spark:7077\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8888:8888'\n- '4040:4040'\n```\nrun command to deploy docker compose\ndocker-compose up\nAccess jupyter notebook using link logged in docker compose logs\nSpark master url is spark://spark:7077",
        "question": "Which command is used to clone the Bitnami repo for Spark?",
        "section": "Module 5: pyspark",
        "document_id": 346,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To run spark in docker setup\n1. Build bitnami spark docker\na. clone bitnami repo using command\ngit clone https://github.com/bitnami/containers.git\n(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\nb. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update java and spark version as following\n\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\n\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\nreference: https://github.com/bitnami/containers/issues/13409\nc. build docker image by navigating to above directory and running docker build command\nnavigate cd bitnami/spark/3.3/debian-11/\nbuild command docker build -t spark:3.3-java-17 .\n2. run docker compose\nusing following file\n```yaml docker-compose.yml\nversion: '2'\nservices:\nspark:\nimage: spark:3.3-java-17\nenvironment:\n- SPARK_MODE=master\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\n- SPARK_RPC_ENCRYPTION_ENABLED=no\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n- SPARK_SSL_ENABLED=no\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8080:8080'\n- '7077:7077'\nspark-worker:\nimage: spark:3.3-java-17\nenvironment:\n- SPARK_MODE=worker\n- SPARK_MASTER_URL=spark://spark:7077\n- SPARK_WORKER_MEMORY=1G\n- SPARK_WORKER_CORES=1\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\n- SPARK_RPC_ENCRYPTION_ENABLED=no\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n- SPARK_SSL_ENABLED=no\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8081:8081'\nspark-nb:\nimage: jupyter/pyspark-notebook:java-17.0.5\nenvironment:\n- SPARK_MASTER_URL=spark://spark:7077\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8888:8888'\n- '4040:4040'\n```\nrun command to deploy docker compose\ndocker-compose up\nAccess jupyter notebook using link logged in docker compose logs\nSpark master url is spark://spark:7077",
        "question": "What file needs to be edited to update the Java and Spark version?",
        "section": "Module 5: pyspark",
        "document_id": 346,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To run spark in docker setup\n1. Build bitnami spark docker\na. clone bitnami repo using command\ngit clone https://github.com/bitnami/containers.git\n(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\nb. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update java and spark version as following\n\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\n\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\nreference: https://github.com/bitnami/containers/issues/13409\nc. build docker image by navigating to above directory and running docker build command\nnavigate cd bitnami/spark/3.3/debian-11/\nbuild command docker build -t spark:3.3-java-17 .\n2. run docker compose\nusing following file\n```yaml docker-compose.yml\nversion: '2'\nservices:\nspark:\nimage: spark:3.3-java-17\nenvironment:\n- SPARK_MODE=master\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\n- SPARK_RPC_ENCRYPTION_ENABLED=no\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n- SPARK_SSL_ENABLED=no\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8080:8080'\n- '7077:7077'\nspark-worker:\nimage: spark:3.3-java-17\nenvironment:\n- SPARK_MODE=worker\n- SPARK_MASTER_URL=spark://spark:7077\n- SPARK_WORKER_MEMORY=1G\n- SPARK_WORKER_CORES=1\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\n- SPARK_RPC_ENCRYPTION_ENABLED=no\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n- SPARK_SSL_ENABLED=no\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8081:8081'\nspark-nb:\nimage: jupyter/pyspark-notebook:java-17.0.5\nenvironment:\n- SPARK_MASTER_URL=spark://spark:7077\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8888:8888'\n- '4040:4040'\n```\nrun command to deploy docker compose\ndocker-compose up\nAccess jupyter notebook using link logged in docker compose logs\nSpark master url is spark://spark:7077",
        "question": "What Docker image is created with the build command mentioned in the text?",
        "section": "Module 5: pyspark",
        "document_id": 346,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To run spark in docker setup\n1. Build bitnami spark docker\na. clone bitnami repo using command\ngit clone https://github.com/bitnami/containers.git\n(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\nb. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update java and spark version as following\n\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\n\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\nreference: https://github.com/bitnami/containers/issues/13409\nc. build docker image by navigating to above directory and running docker build command\nnavigate cd bitnami/spark/3.3/debian-11/\nbuild command docker build -t spark:3.3-java-17 .\n2. run docker compose\nusing following file\n```yaml docker-compose.yml\nversion: '2'\nservices:\nspark:\nimage: spark:3.3-java-17\nenvironment:\n- SPARK_MODE=master\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\n- SPARK_RPC_ENCRYPTION_ENABLED=no\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n- SPARK_SSL_ENABLED=no\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8080:8080'\n- '7077:7077'\nspark-worker:\nimage: spark:3.3-java-17\nenvironment:\n- SPARK_MODE=worker\n- SPARK_MASTER_URL=spark://spark:7077\n- SPARK_WORKER_MEMORY=1G\n- SPARK_WORKER_CORES=1\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\n- SPARK_RPC_ENCRYPTION_ENABLED=no\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n- SPARK_SSL_ENABLED=no\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8081:8081'\nspark-nb:\nimage: jupyter/pyspark-notebook:java-17.0.5\nenvironment:\n- SPARK_MASTER_URL=spark://spark:7077\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8888:8888'\n- '4040:4040'\n```\nrun command to deploy docker compose\ndocker-compose up\nAccess jupyter notebook using link logged in docker compose logs\nSpark master url is spark://spark:7077",
        "question": "How can you access the Jupyter notebook after deploying the Docker compose?",
        "section": "Module 5: pyspark",
        "document_id": 346,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To do this\npip install gcsfs,\nThereafter copy the uri path to the file and use \ndf = pandas.read_csc(gs://path)",
        "question": "What command is used to install the gcsfs package?",
        "section": "Module 5: pyspark",
        "document_id": 347,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To do this\npip install gcsfs,\nThereafter copy the uri path to the file and use \ndf = pandas.read_csc(gs://path)",
        "question": "How do you copy the URI path to a file after installing gcsfs?",
        "section": "Module 5: pyspark",
        "document_id": 347,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To do this\npip install gcsfs,\nThereafter copy the uri path to the file and use \ndf = pandas.read_csc(gs://path)",
        "question": "What is the correct format to read a CSV file using pandas from Google Cloud Storage?",
        "section": "Module 5: pyspark",
        "document_id": 347,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To do this\npip install gcsfs,\nThereafter copy the uri path to the file and use \ndf = pandas.read_csc(gs://path)",
        "question": "What does 'gs://' signify in the context of file paths?",
        "section": "Module 5: pyspark",
        "document_id": 347,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "To do this\npip install gcsfs,\nThereafter copy the uri path to the file and use \ndf = pandas.read_csc(gs://path)",
        "question": "Which library is being utilized to read the CSV file in the provided text?",
        "section": "Module 5: pyspark",
        "document_id": 347,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error:\nspark.createDataFrame(df_pandas).schema\nTypeError: field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>\nSolution:\nAffiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don’t have to take out any data from your dataset. Something like this can help:\ndf = spark.read \\\n.options(\nheader = \"true\", \\\ninferSchema = \"true\", \\\n) \\\n.csv('path/to/your/csv/file/')\nSolution B:\nIt's because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the 'Affiliated_base_number' column. Then you will be able to apply the pyspark function createDataFrame.\n# Only take rows that have no null values\npandas_df= pandas_df[pandas_df.notnull().all(1)]",
        "question": "What error occurs when attempting to create a DataFrame from the provided pandas DataFrame?",
        "section": "Module 5: pyspark",
        "document_id": 348,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error:\nspark.createDataFrame(df_pandas).schema\nTypeError: field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>\nSolution:\nAffiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don’t have to take out any data from your dataset. Something like this can help:\ndf = spark.read \\\n.options(\nheader = \"true\", \\\ninferSchema = \"true\", \\\n) \\\n.csv('path/to/your/csv/file/')\nSolution B:\nIt's because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the 'Affiliated_base_number' column. Then you will be able to apply the pyspark function createDataFrame.\n# Only take rows that have no null values\npandas_df= pandas_df[pandas_df.notnull().all(1)]",
        "question": "Why can't the Affiliated_base_number field be set to DoubleType?",
        "section": "Module 5: pyspark",
        "document_id": 348,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error:\nspark.createDataFrame(df_pandas).schema\nTypeError: field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>\nSolution:\nAffiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don’t have to take out any data from your dataset. Something like this can help:\ndf = spark.read \\\n.options(\nheader = \"true\", \\\ninferSchema = \"true\", \\\n) \\\n.csv('path/to/your/csv/file/')\nSolution B:\nIt's because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the 'Affiliated_base_number' column. Then you will be able to apply the pyspark function createDataFrame.\n# Only take rows that have no null values\npandas_df= pandas_df[pandas_df.notnull().all(1)]",
        "question": "How does Spark's inferSchema compare to Pandas' infer type method in this context?",
        "section": "Module 5: pyspark",
        "document_id": 348,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error:\nspark.createDataFrame(df_pandas).schema\nTypeError: field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>\nSolution:\nAffiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don’t have to take out any data from your dataset. Something like this can help:\ndf = spark.read \\\n.options(\nheader = \"true\", \\\ninferSchema = \"true\", \\\n) \\\n.csv('path/to/your/csv/file/')\nSolution B:\nIt's because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the 'Affiliated_base_number' column. Then you will be able to apply the pyspark function createDataFrame.\n# Only take rows that have no null values\npandas_df= pandas_df[pandas_df.notnull().all(1)]",
        "question": "What steps should be taken to handle rows with null values in the Affiliated_base_number column?",
        "section": "Module 5: pyspark",
        "document_id": 348,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Error:\nspark.createDataFrame(df_pandas).schema\nTypeError: field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>\nSolution:\nAffiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don’t have to take out any data from your dataset. Something like this can help:\ndf = spark.read \\\n.options(\nheader = \"true\", \\\ninferSchema = \"true\", \\\n) \\\n.csv('path/to/your/csv/file/')\nSolution B:\nIt's because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the 'Affiliated_base_number' column. Then you will be able to apply the pyspark function createDataFrame.\n# Only take rows that have no null values\npandas_df= pandas_df[pandas_df.notnull().all(1)]",
        "question": "What options can be set when reading a CSV to ensure accurate schema inference in Spark?",
        "section": "Module 5: pyspark",
        "document_id": 348,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Default executor memory is 1gb. This error appeared when working with the homework dataset.\nError: MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\nScaling row group sizes to 95.00% for 8 writers\nSolution:\nIncrease the memory of the executor when creating the Spark session like this:\nRemember to restart the Jupyter session (ie. close the Spark session) or the config won’t take effect.",
        "question": "What is the default executor memory mentioned in the text?",
        "section": "Module 5: pyspark",
        "document_id": 349,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Default executor memory is 1gb. This error appeared when working with the homework dataset.\nError: MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\nScaling row group sizes to 95.00% for 8 writers\nSolution:\nIncrease the memory of the executor when creating the Spark session like this:\nRemember to restart the Jupyter session (ie. close the Spark session) or the config won’t take effect.",
        "question": "What error message is encountered when working with the homework dataset?",
        "section": "Module 5: pyspark",
        "document_id": 349,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Default executor memory is 1gb. This error appeared when working with the homework dataset.\nError: MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\nScaling row group sizes to 95.00% for 8 writers\nSolution:\nIncrease the memory of the executor when creating the Spark session like this:\nRemember to restart the Jupyter session (ie. close the Spark session) or the config won’t take effect.",
        "question": "What percentage of heap memory is exceeded according to the error?",
        "section": "Module 5: pyspark",
        "document_id": 349,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Default executor memory is 1gb. This error appeared when working with the homework dataset.\nError: MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\nScaling row group sizes to 95.00% for 8 writers\nSolution:\nIncrease the memory of the executor when creating the Spark session like this:\nRemember to restart the Jupyter session (ie. close the Spark session) or the config won’t take effect.",
        "question": "What is the suggested solution for addressing the memory error?",
        "section": "Module 5: pyspark",
        "document_id": 349,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Default executor memory is 1gb. This error appeared when working with the homework dataset.\nError: MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\nScaling row group sizes to 95.00% for 8 writers\nSolution:\nIncrease the memory of the executor when creating the Spark session like this:\nRemember to restart the Jupyter session (ie. close the Spark session) or the config won’t take effect.",
        "question": "What must be done after increasing the executor memory to ensure the changes take effect?",
        "section": "Module 5: pyspark",
        "document_id": 349,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change the working directory to the spark directory:\nif you have setup up your SPARK_HOME variable, use the following;\ncd %SPARK_HOME%\nif not, use the following;\ncd <path to spark installation>\nCreating a Local Spark Cluster\nTo start Spark Master:\nbin\\spark-class org.apache.spark.deploy.master.Master --host localhost\nStarting up a cluster:\nbin\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost",
        "question": "How do you change the working directory to the spark directory?",
        "section": "Module 5: pyspark",
        "document_id": 350,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change the working directory to the spark directory:\nif you have setup up your SPARK_HOME variable, use the following;\ncd %SPARK_HOME%\nif not, use the following;\ncd <path to spark installation>\nCreating a Local Spark Cluster\nTo start Spark Master:\nbin\\spark-class org.apache.spark.deploy.master.Master --host localhost\nStarting up a cluster:\nbin\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost",
        "question": "What command should you use if you have set up your SPARK_HOME variable?",
        "section": "Module 5: pyspark",
        "document_id": 350,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change the working directory to the spark directory:\nif you have setup up your SPARK_HOME variable, use the following;\ncd %SPARK_HOME%\nif not, use the following;\ncd <path to spark installation>\nCreating a Local Spark Cluster\nTo start Spark Master:\nbin\\spark-class org.apache.spark.deploy.master.Master --host localhost\nStarting up a cluster:\nbin\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost",
        "question": "What is the command to start the Spark Master?",
        "section": "Module 5: pyspark",
        "document_id": 350,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change the working directory to the spark directory:\nif you have setup up your SPARK_HOME variable, use the following;\ncd %SPARK_HOME%\nif not, use the following;\ncd <path to spark installation>\nCreating a Local Spark Cluster\nTo start Spark Master:\nbin\\spark-class org.apache.spark.deploy.master.Master --host localhost\nStarting up a cluster:\nbin\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost",
        "question": "How do you start up a local Spark cluster?",
        "section": "Module 5: pyspark",
        "document_id": 350,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change the working directory to the spark directory:\nif you have setup up your SPARK_HOME variable, use the following;\ncd %SPARK_HOME%\nif not, use the following;\ncd <path to spark installation>\nCreating a Local Spark Cluster\nTo start Spark Master:\nbin\\spark-class org.apache.spark.deploy.master.Master --host localhost\nStarting up a cluster:\nbin\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost",
        "question": "What host should be used when starting the Spark Worker?",
        "section": "Module 5: pyspark",
        "document_id": 350,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I added PYTHONPATH, JAVA_HOME and SPARK_HOME to ~/.bashrc, import pyspark worked ok in iPython in terminal, but couldn’t be found in .ipynb opened in VS Code\nAfter adding new lines to ~/.bashrc, need to restart the shell to activate the new lines, do either\nsource ~/.bashrc\nexec bash\nInstead of configuring paths in ~/.bashrc, I created .env file in the root of my workspace:",
        "question": "What environment variables were added to ~/.bashrc?",
        "section": "Module 5: pyspark",
        "document_id": 351,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I added PYTHONPATH, JAVA_HOME and SPARK_HOME to ~/.bashrc, import pyspark worked ok in iPython in terminal, but couldn’t be found in .ipynb opened in VS Code\nAfter adding new lines to ~/.bashrc, need to restart the shell to activate the new lines, do either\nsource ~/.bashrc\nexec bash\nInstead of configuring paths in ~/.bashrc, I created .env file in the root of my workspace:",
        "question": "What command can be used to activate the new lines added to ~/.bashrc?",
        "section": "Module 5: pyspark",
        "document_id": 351,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I added PYTHONPATH, JAVA_HOME and SPARK_HOME to ~/.bashrc, import pyspark worked ok in iPython in terminal, but couldn’t be found in .ipynb opened in VS Code\nAfter adding new lines to ~/.bashrc, need to restart the shell to activate the new lines, do either\nsource ~/.bashrc\nexec bash\nInstead of configuring paths in ~/.bashrc, I created .env file in the root of my workspace:",
        "question": "Why was 'import pyspark' not found in the .ipynb opened in VS Code?",
        "section": "Module 5: pyspark",
        "document_id": 351,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I added PYTHONPATH, JAVA_HOME and SPARK_HOME to ~/.bashrc, import pyspark worked ok in iPython in terminal, but couldn’t be found in .ipynb opened in VS Code\nAfter adding new lines to ~/.bashrc, need to restart the shell to activate the new lines, do either\nsource ~/.bashrc\nexec bash\nInstead of configuring paths in ~/.bashrc, I created .env file in the root of my workspace:",
        "question": "What alternative method was used instead of configuring paths in ~/.bashrc?",
        "section": "Module 5: pyspark",
        "document_id": 351,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I added PYTHONPATH, JAVA_HOME and SPARK_HOME to ~/.bashrc, import pyspark worked ok in iPython in terminal, but couldn’t be found in .ipynb opened in VS Code\nAfter adding new lines to ~/.bashrc, need to restart the shell to activate the new lines, do either\nsource ~/.bashrc\nexec bash\nInstead of configuring paths in ~/.bashrc, I created .env file in the root of my workspace:",
        "question": "What does the .env file in the root of the workspace do?",
        "section": "Module 5: pyspark",
        "document_id": 351,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I don’t use visual studio, so I did it the old fashioned way: ssh -L 8888:localhost:8888 <my user>@<VM IP> (replace user and IP with the ones used by the GCP VM, e.g. : ssh -L 8888:localhost:8888 myuser@34.140.188.1",
        "question": "What method does the author use instead of Visual Studio?",
        "section": "Module 5: pyspark",
        "document_id": 352,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I don’t use visual studio, so I did it the old fashioned way: ssh -L 8888:localhost:8888 <my user>@<VM IP> (replace user and IP with the ones used by the GCP VM, e.g. : ssh -L 8888:localhost:8888 myuser@34.140.188.1",
        "question": "What command does the author provide for SSH tunneling?",
        "section": "Module 5: pyspark",
        "document_id": 352,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I don’t use visual studio, so I did it the old fashioned way: ssh -L 8888:localhost:8888 <my user>@<VM IP> (replace user and IP with the ones used by the GCP VM, e.g. : ssh -L 8888:localhost:8888 myuser@34.140.188.1",
        "question": "What do you need to replace in the SSH command?",
        "section": "Module 5: pyspark",
        "document_id": 352,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I don’t use visual studio, so I did it the old fashioned way: ssh -L 8888:localhost:8888 <my user>@<VM IP> (replace user and IP with the ones used by the GCP VM, e.g. : ssh -L 8888:localhost:8888 myuser@34.140.188.1",
        "question": "What does the '-L' option in the SSH command signify?",
        "section": "Module 5: pyspark",
        "document_id": 352,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "I don’t use visual studio, so I did it the old fashioned way: ssh -L 8888:localhost:8888 <my user>@<VM IP> (replace user and IP with the ones used by the GCP VM, e.g. : ssh -L 8888:localhost:8888 myuser@34.140.188.1",
        "question": "What is the example VM IP address mentioned in the text?",
        "section": "Module 5: pyspark",
        "document_id": 352,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are doing wc -l fhvhv_tripdata_2021-01.csv.gz  with the gzip file as the file argument, you will get a different result, obviously! Since the file is compressed.\nUnzip the file and then do wc -l fhvhv_tripdata_2021-01.csv to get the right results.",
        "question": "What command is suggested to count the lines in the file fhvhv_tripdata_2021-01.csv?",
        "section": "Module 5: pyspark",
        "document_id": 353,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are doing wc -l fhvhv_tripdata_2021-01.csv.gz  with the gzip file as the file argument, you will get a different result, obviously! Since the file is compressed.\nUnzip the file and then do wc -l fhvhv_tripdata_2021-01.csv to get the right results.",
        "question": "Why does the command wc -l give different results when used on a gzip file?",
        "section": "Module 5: pyspark",
        "document_id": 353,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are doing wc -l fhvhv_tripdata_2021-01.csv.gz  with the gzip file as the file argument, you will get a different result, obviously! Since the file is compressed.\nUnzip the file and then do wc -l fhvhv_tripdata_2021-01.csv to get the right results.",
        "question": "What must be done to fhvhv_tripdata_2021-01.csv.gz before using wc -l?",
        "section": "Module 5: pyspark",
        "document_id": 353,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are doing wc -l fhvhv_tripdata_2021-01.csv.gz  with the gzip file as the file argument, you will get a different result, obviously! Since the file is compressed.\nUnzip the file and then do wc -l fhvhv_tripdata_2021-01.csv to get the right results.",
        "question": "What is the expected outcome of running wc -l on a compressed file?",
        "section": "Module 5: pyspark",
        "document_id": 353,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are doing wc -l fhvhv_tripdata_2021-01.csv.gz  with the gzip file as the file argument, you will get a different result, obviously! Since the file is compressed.\nUnzip the file and then do wc -l fhvhv_tripdata_2021-01.csv to get the right results.",
        "question": "What file extension is associated with the compressed version of the trip data file?",
        "section": "Module 5: pyspark",
        "document_id": 353,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\nFor Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\bin” to the PATH variable.\nAdditional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io",
        "question": "What error might indicate an issue with path variables when writing to parquet?",
        "section": "Module 5: pyspark",
        "document_id": 355,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\nFor Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\bin” to the PATH variable.\nAdditional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io",
        "question": "How can Windows users set the HADOOP_HOME User Variable?",
        "section": "Module 5: pyspark",
        "document_id": 355,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\nFor Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\bin” to the PATH variable.\nAdditional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io",
        "question": "What should be added to the PATH variable after setting HADOOP_HOME?",
        "section": "Module 5: pyspark",
        "document_id": 355,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\nFor Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\bin” to the PATH variable.\nAdditional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io",
        "question": "Where can additional tips for resolving the error be found?",
        "section": "Module 5: pyspark",
        "document_id": 355,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\nFor Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\bin” to the PATH variable.\nAdditional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io",
        "question": "What is the significance of the HADOOP_HOME environment variable in relation to Hadoop?",
        "section": "Module 5: pyspark",
        "document_id": 355,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change the hadoop version to 3.0.1.Replace all the files in the local hadoop bin folder with the files in this repo:  winutils/hadoop-3.0.1/bin at master · cdarlint/winutils (github.com)\nIf this does not work try to change other versions found in this repository.\nFor more information please see this link: This version of %1 is not compatible with the version of Windows you're running · Issue #20 · cdarlint/winutils (github.com)",
        "question": "What is the specific Hadoop version that needs to be changed to?",
        "section": "Module 5: pyspark",
        "document_id": 356,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change the hadoop version to 3.0.1.Replace all the files in the local hadoop bin folder with the files in this repo:  winutils/hadoop-3.0.1/bin at master · cdarlint/winutils (github.com)\nIf this does not work try to change other versions found in this repository.\nFor more information please see this link: This version of %1 is not compatible with the version of Windows you're running · Issue #20 · cdarlint/winutils (github.com)",
        "question": "Where can the files needed to replace in the local Hadoop bin folder be found?",
        "section": "Module 5: pyspark",
        "document_id": 356,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change the hadoop version to 3.0.1.Replace all the files in the local hadoop bin folder with the files in this repo:  winutils/hadoop-3.0.1/bin at master · cdarlint/winutils (github.com)\nIf this does not work try to change other versions found in this repository.\nFor more information please see this link: This version of %1 is not compatible with the version of Windows you're running · Issue #20 · cdarlint/winutils (github.com)",
        "question": "What should be done if changing to Hadoop version 3.0.1 does not work?",
        "section": "Module 5: pyspark",
        "document_id": 356,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change the hadoop version to 3.0.1.Replace all the files in the local hadoop bin folder with the files in this repo:  winutils/hadoop-3.0.1/bin at master · cdarlint/winutils (github.com)\nIf this does not work try to change other versions found in this repository.\nFor more information please see this link: This version of %1 is not compatible with the version of Windows you're running · Issue #20 · cdarlint/winutils (github.com)",
        "question": "What type of issues might arise according to the provided link?",
        "section": "Module 5: pyspark",
        "document_id": 356,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Change the hadoop version to 3.0.1.Replace all the files in the local hadoop bin folder with the files in this repo:  winutils/hadoop-3.0.1/bin at master · cdarlint/winutils (github.com)\nIf this does not work try to change other versions found in this repository.\nFor more information please see this link: This version of %1 is not compatible with the version of Windows you're running · Issue #20 · cdarlint/winutils (github.com)",
        "question": "Which repository is mentioned for finding different versions of Hadoop?",
        "section": "Module 5: pyspark",
        "document_id": 356,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Fix is to set the flag like the error states. Get your project ID from your dashboard and set it like so:\ngcloud dataproc jobs submit pyspark \\\n--cluster=my_cluster \\\n--region=us-central1 \\\n--project=my-dtc-project-1010101 \\\ngs://my-dtc-bucket-id/code/06_spark_sql.py\n-- \\\n…",
        "question": "What command is used to submit a pyspark job in Google Cloud Dataproc?",
        "section": "Module 5: pyspark",
        "document_id": 357,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Fix is to set the flag like the error states. Get your project ID from your dashboard and set it like so:\ngcloud dataproc jobs submit pyspark \\\n--cluster=my_cluster \\\n--region=us-central1 \\\n--project=my-dtc-project-1010101 \\\ngs://my-dtc-bucket-id/code/06_spark_sql.py\n-- \\\n…",
        "question": "How do you specify the cluster when submitting a job in Dataproc?",
        "section": "Module 5: pyspark",
        "document_id": 357,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Fix is to set the flag like the error states. Get your project ID from your dashboard and set it like so:\ngcloud dataproc jobs submit pyspark \\\n--cluster=my_cluster \\\n--region=us-central1 \\\n--project=my-dtc-project-1010101 \\\ngs://my-dtc-bucket-id/code/06_spark_sql.py\n-- \\\n…",
        "question": "What is the purpose of the 'project' flag in the gcloud command?",
        "section": "Module 5: pyspark",
        "document_id": 357,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Fix is to set the flag like the error states. Get your project ID from your dashboard and set it like so:\ngcloud dataproc jobs submit pyspark \\\n--cluster=my_cluster \\\n--region=us-central1 \\\n--project=my-dtc-project-1010101 \\\ngs://my-dtc-bucket-id/code/06_spark_sql.py\n-- \\\n…",
        "question": "From where can you obtain your project ID for the Dataproc job submission?",
        "section": "Module 5: pyspark",
        "document_id": 357,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Fix is to set the flag like the error states. Get your project ID from your dashboard and set it like so:\ngcloud dataproc jobs submit pyspark \\\n--cluster=my_cluster \\\n--region=us-central1 \\\n--project=my-dtc-project-1010101 \\\ngs://my-dtc-bucket-id/code/06_spark_sql.py\n-- \\\n…",
        "question": "What is the format of the gsutil path specified in the command?",
        "section": "Module 5: pyspark",
        "document_id": 357,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Go to %SPARK_HOME%\\bin\nRun spark-class org.apache.spark.deploy.master.Master to run the master. This will give you a URL of the form spark://ip:port\nRun spark-class org.apache.spark.deploy.worker.Worker spark://ip:port to run the worker. Make sure you use the URL you obtained in step 2.\nCreate a new Jupyter notebook:\nspark = SparkSession.builder \\\n.master(\"spark://{ip}:7077\") \\\n.appName('test') \\\n.getOrCreate()\nCheck on Spark UI the master, worker and app.",
        "question": "What command is used to run the Spark master?",
        "section": "Module 5: pyspark",
        "document_id": 358,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Go to %SPARK_HOME%\\bin\nRun spark-class org.apache.spark.deploy.master.Master to run the master. This will give you a URL of the form spark://ip:port\nRun spark-class org.apache.spark.deploy.worker.Worker spark://ip:port to run the worker. Make sure you use the URL you obtained in step 2.\nCreate a new Jupyter notebook:\nspark = SparkSession.builder \\\n.master(\"spark://{ip}:7077\") \\\n.appName('test') \\\n.getOrCreate()\nCheck on Spark UI the master, worker and app.",
        "question": "What format does the URL provided by the Spark master follow?",
        "section": "Module 5: pyspark",
        "document_id": 358,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Go to %SPARK_HOME%\\bin\nRun spark-class org.apache.spark.deploy.master.Master to run the master. This will give you a URL of the form spark://ip:port\nRun spark-class org.apache.spark.deploy.worker.Worker spark://ip:port to run the worker. Make sure you use the URL you obtained in step 2.\nCreate a new Jupyter notebook:\nspark = SparkSession.builder \\\n.master(\"spark://{ip}:7077\") \\\n.appName('test') \\\n.getOrCreate()\nCheck on Spark UI the master, worker and app.",
        "question": "How do you start a Spark worker after running the master?",
        "section": "Module 5: pyspark",
        "document_id": 358,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Go to %SPARK_HOME%\\bin\nRun spark-class org.apache.spark.deploy.master.Master to run the master. This will give you a URL of the form spark://ip:port\nRun spark-class org.apache.spark.deploy.worker.Worker spark://ip:port to run the worker. Make sure you use the URL you obtained in step 2.\nCreate a new Jupyter notebook:\nspark = SparkSession.builder \\\n.master(\"spark://{ip}:7077\") \\\n.appName('test') \\\n.getOrCreate()\nCheck on Spark UI the master, worker and app.",
        "question": "What is the purpose of the SparkSession builder in creating a Jupyter notebook?",
        "section": "Module 5: pyspark",
        "document_id": 358,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Go to %SPARK_HOME%\\bin\nRun spark-class org.apache.spark.deploy.master.Master to run the master. This will give you a URL of the form spark://ip:port\nRun spark-class org.apache.spark.deploy.worker.Worker spark://ip:port to run the worker. Make sure you use the URL you obtained in step 2.\nCreate a new Jupyter notebook:\nspark = SparkSession.builder \\\n.master(\"spark://{ip}:7077\") \\\n.appName('test') \\\n.getOrCreate()\nCheck on Spark UI the master, worker and app.",
        "question": "Where can you check the status of the master, worker, and application?",
        "section": "Module 5: pyspark",
        "document_id": 358,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This occurs because you are not logged in “gcloud auth login” and maybe the project id is not settled. Then type in a terminal:\ngcloud auth login\nThis will open a tab in the browser, accept the terms, after that close the tab if you want. Then set the project is like:\ngcloud config set project <YOUR PROJECT_ID>\nThen you can run the command to upload the pq dir to a GCS Bucket:\ngsutil -m cp -r pq/ <YOUR URI from gsutil>/pq",
        "question": "What command is used to log in to gcloud?",
        "section": "Module 5: pyspark",
        "document_id": 359,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This occurs because you are not logged in “gcloud auth login” and maybe the project id is not settled. Then type in a terminal:\ngcloud auth login\nThis will open a tab in the browser, accept the terms, after that close the tab if you want. Then set the project is like:\ngcloud config set project <YOUR PROJECT_ID>\nThen you can run the command to upload the pq dir to a GCS Bucket:\ngsutil -m cp -r pq/ <YOUR URI from gsutil>/pq",
        "question": "What should you do after logging in to gcloud in the browser?",
        "section": "Module 5: pyspark",
        "document_id": 359,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This occurs because you are not logged in “gcloud auth login” and maybe the project id is not settled. Then type in a terminal:\ngcloud auth login\nThis will open a tab in the browser, accept the terms, after that close the tab if you want. Then set the project is like:\ngcloud config set project <YOUR PROJECT_ID>\nThen you can run the command to upload the pq dir to a GCS Bucket:\ngsutil -m cp -r pq/ <YOUR URI from gsutil>/pq",
        "question": "How can you set the project ID in gcloud config?",
        "section": "Module 5: pyspark",
        "document_id": 359,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This occurs because you are not logged in “gcloud auth login” and maybe the project id is not settled. Then type in a terminal:\ngcloud auth login\nThis will open a tab in the browser, accept the terms, after that close the tab if you want. Then set the project is like:\ngcloud config set project <YOUR PROJECT_ID>\nThen you can run the command to upload the pq dir to a GCS Bucket:\ngsutil -m cp -r pq/ <YOUR URI from gsutil>/pq",
        "question": "What is the command to upload the pq directory to a GCS Bucket?",
        "section": "Module 5: pyspark",
        "document_id": 359,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This occurs because you are not logged in “gcloud auth login” and maybe the project id is not settled. Then type in a terminal:\ngcloud auth login\nThis will open a tab in the browser, accept the terms, after that close the tab if you want. Then set the project is like:\ngcloud config set project <YOUR PROJECT_ID>\nThen you can run the command to upload the pq dir to a GCS Bucket:\ngsutil -m cp -r pq/ <YOUR URI from gsutil>/pq",
        "question": "What flag is used with gsutil to copy files recursively?",
        "section": "Module 5: pyspark",
        "document_id": 359,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When submit a job, it might throw an error about Java in log panel within Dataproc. I changed the Versioning Control when I created a cluster, so it means that I delete the cluster and created a new one, and instead of choosing Debian-Hadoop-Spark, I switch to Ubuntu 20.02-Hadoop3.3-Spark3.3 for Versioning Control feature, the main reason to choose this is because I have the same Ubuntu version in mi laptop, I tried to find documentation to sustent this but unfortunately I couldn't nevertheless it works for me.",
        "question": "What error might occur when submitting a job in Dataproc?",
        "section": "Module 5: pyspark",
        "document_id": 360,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When submit a job, it might throw an error about Java in log panel within Dataproc. I changed the Versioning Control when I created a cluster, so it means that I delete the cluster and created a new one, and instead of choosing Debian-Hadoop-Spark, I switch to Ubuntu 20.02-Hadoop3.3-Spark3.3 for Versioning Control feature, the main reason to choose this is because I have the same Ubuntu version in mi laptop, I tried to find documentation to sustent this but unfortunately I couldn't nevertheless it works for me.",
        "question": "What action did the author take to resolve the Java error in Dataproc?",
        "section": "Module 5: pyspark",
        "document_id": 360,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When submit a job, it might throw an error about Java in log panel within Dataproc. I changed the Versioning Control when I created a cluster, so it means that I delete the cluster and created a new one, and instead of choosing Debian-Hadoop-Spark, I switch to Ubuntu 20.02-Hadoop3.3-Spark3.3 for Versioning Control feature, the main reason to choose this is because I have the same Ubuntu version in mi laptop, I tried to find documentation to sustent this but unfortunately I couldn't nevertheless it works for me.",
        "question": "Which version of Ubuntu did the author choose for the cluster instead of Debian-Hadoop-Spark?",
        "section": "Module 5: pyspark",
        "document_id": 360,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When submit a job, it might throw an error about Java in log panel within Dataproc. I changed the Versioning Control when I created a cluster, so it means that I delete the cluster and created a new one, and instead of choosing Debian-Hadoop-Spark, I switch to Ubuntu 20.02-Hadoop3.3-Spark3.3 for Versioning Control feature, the main reason to choose this is because I have the same Ubuntu version in mi laptop, I tried to find documentation to sustent this but unfortunately I couldn't nevertheless it works for me.",
        "question": "Why did the author prefer Ubuntu 20.02 for the Versioning Control feature?",
        "section": "Module 5: pyspark",
        "document_id": 360,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "When submit a job, it might throw an error about Java in log panel within Dataproc. I changed the Versioning Control when I created a cluster, so it means that I delete the cluster and created a new one, and instead of choosing Debian-Hadoop-Spark, I switch to Ubuntu 20.02-Hadoop3.3-Spark3.3 for Versioning Control feature, the main reason to choose this is because I have the same Ubuntu version in mi laptop, I tried to find documentation to sustent this but unfortunately I couldn't nevertheless it works for me.",
        "question": "Was the author able to find documentation to support their choice of Ubuntu version?",
        "section": "Module 5: pyspark",
        "document_id": 360,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use both repartition and coalesce, like so:\ndf = df.repartition(6)\ndf = df.coalesce(6)\ndf.write.parquet('fhv/2019/10', mode='overwrite')",
        "question": "What is the purpose of using repartition in this code?",
        "section": "Module 5: pyspark",
        "document_id": 361,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use both repartition and coalesce, like so:\ndf = df.repartition(6)\ndf = df.coalesce(6)\ndf.write.parquet('fhv/2019/10', mode='overwrite')",
        "question": "How many partitions does the DataFrame have after calling coalesce?",
        "section": "Module 5: pyspark",
        "document_id": 361,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use both repartition and coalesce, like so:\ndf = df.repartition(6)\ndf = df.coalesce(6)\ndf.write.parquet('fhv/2019/10', mode='overwrite')",
        "question": "What file format is being used to write the DataFrame?",
        "section": "Module 5: pyspark",
        "document_id": 361,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use both repartition and coalesce, like so:\ndf = df.repartition(6)\ndf = df.coalesce(6)\ndf.write.parquet('fhv/2019/10', mode='overwrite')",
        "question": "What happens to the existing data when using mode='overwrite'?",
        "section": "Module 5: pyspark",
        "document_id": 361,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use both repartition and coalesce, like so:\ndf = df.repartition(6)\ndf = df.coalesce(6)\ndf.write.parquet('fhv/2019/10', mode='overwrite')",
        "question": "What is the significance of the path 'fhv/2019/10' in the write operation?",
        "section": "Module 5: pyspark",
        "document_id": 361,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Possible solution - Try to forward the port using ssh cli instead of vs code.\nRun > “ssh -L <local port>:<VM host/ip>:<VM port> <ssh hostname>”\nssh hostname is the name you specified in the ~/.ssh/config file\nIn case of Jupyter Notebook run\n“ssh -L 8888:localhost:8888 gcp-vm”\nfrom your local machine’s cli.\nNOTE: If you logout from the session, the connection would break. Also while creating the spark session notice the block's log because sometimes it fails to run at 4040 and then switches to 4041.\n~Abhijit Chakrborty: If you are having trouble accessing localhost ports from GCP VM consider adding the forwarding instructions to .ssh/config file as following:\n```\nHost <hostname>\nHostname <external-gcp-ip>\nUser xxxx\nIdentityFile yyyy\nLocalForward 8888 localhost:8888\nLocalForward 8080 localhost:8080\nLocalForward 5432 localhost:5432\nLocalForward 4040 localhost:4040\n```\nThis should automatically forward all ports and will enable accessing localhost ports.",
        "question": "What command is suggested for forwarding ports using SSH CLI?",
        "section": "Module 5: pyspark",
        "document_id": 362,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Possible solution - Try to forward the port using ssh cli instead of vs code.\nRun > “ssh -L <local port>:<VM host/ip>:<VM port> <ssh hostname>”\nssh hostname is the name you specified in the ~/.ssh/config file\nIn case of Jupyter Notebook run\n“ssh -L 8888:localhost:8888 gcp-vm”\nfrom your local machine’s cli.\nNOTE: If you logout from the session, the connection would break. Also while creating the spark session notice the block's log because sometimes it fails to run at 4040 and then switches to 4041.\n~Abhijit Chakrborty: If you are having trouble accessing localhost ports from GCP VM consider adding the forwarding instructions to .ssh/config file as following:\n```\nHost <hostname>\nHostname <external-gcp-ip>\nUser xxxx\nIdentityFile yyyy\nLocalForward 8888 localhost:8888\nLocalForward 8080 localhost:8080\nLocalForward 5432 localhost:5432\nLocalForward 4040 localhost:4040\n```\nThis should automatically forward all ports and will enable accessing localhost ports.",
        "question": "What happens to the connection if you logout from the SSH session?",
        "section": "Module 5: pyspark",
        "document_id": 362,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Possible solution - Try to forward the port using ssh cli instead of vs code.\nRun > “ssh -L <local port>:<VM host/ip>:<VM port> <ssh hostname>”\nssh hostname is the name you specified in the ~/.ssh/config file\nIn case of Jupyter Notebook run\n“ssh -L 8888:localhost:8888 gcp-vm”\nfrom your local machine’s cli.\nNOTE: If you logout from the session, the connection would break. Also while creating the spark session notice the block's log because sometimes it fails to run at 4040 and then switches to 4041.\n~Abhijit Chakrborty: If you are having trouble accessing localhost ports from GCP VM consider adding the forwarding instructions to .ssh/config file as following:\n```\nHost <hostname>\nHostname <external-gcp-ip>\nUser xxxx\nIdentityFile yyyy\nLocalForward 8888 localhost:8888\nLocalForward 8080 localhost:8080\nLocalForward 5432 localhost:5432\nLocalForward 4040 localhost:4040\n```\nThis should automatically forward all ports and will enable accessing localhost ports.",
        "question": "How can you access localhost ports from a GCP VM according to the text?",
        "section": "Module 5: pyspark",
        "document_id": 362,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Possible solution - Try to forward the port using ssh cli instead of vs code.\nRun > “ssh -L <local port>:<VM host/ip>:<VM port> <ssh hostname>”\nssh hostname is the name you specified in the ~/.ssh/config file\nIn case of Jupyter Notebook run\n“ssh -L 8888:localhost:8888 gcp-vm”\nfrom your local machine’s cli.\nNOTE: If you logout from the session, the connection would break. Also while creating the spark session notice the block's log because sometimes it fails to run at 4040 and then switches to 4041.\n~Abhijit Chakrborty: If you are having trouble accessing localhost ports from GCP VM consider adding the forwarding instructions to .ssh/config file as following:\n```\nHost <hostname>\nHostname <external-gcp-ip>\nUser xxxx\nIdentityFile yyyy\nLocalForward 8888 localhost:8888\nLocalForward 8080 localhost:8080\nLocalForward 5432 localhost:5432\nLocalForward 4040 localhost:4040\n```\nThis should automatically forward all ports and will enable accessing localhost ports.",
        "question": "What should you notice while creating the Spark session, as mentioned in the text?",
        "section": "Module 5: pyspark",
        "document_id": 362,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Possible solution - Try to forward the port using ssh cli instead of vs code.\nRun > “ssh -L <local port>:<VM host/ip>:<VM port> <ssh hostname>”\nssh hostname is the name you specified in the ~/.ssh/config file\nIn case of Jupyter Notebook run\n“ssh -L 8888:localhost:8888 gcp-vm”\nfrom your local machine’s cli.\nNOTE: If you logout from the session, the connection would break. Also while creating the spark session notice the block's log because sometimes it fails to run at 4040 and then switches to 4041.\n~Abhijit Chakrborty: If you are having trouble accessing localhost ports from GCP VM consider adding the forwarding instructions to .ssh/config file as following:\n```\nHost <hostname>\nHostname <external-gcp-ip>\nUser xxxx\nIdentityFile yyyy\nLocalForward 8888 localhost:8888\nLocalForward 8080 localhost:8080\nLocalForward 5432 localhost:5432\nLocalForward 4040 localhost:4040\n```\nThis should automatically forward all ports and will enable accessing localhost ports.",
        "question": "What are the LocalForward instructions provided for in the .ssh/config file?",
        "section": "Module 5: pyspark",
        "document_id": 362,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "~ Abhijit Chakraborty\n`sdk list java`  to check for available java sdk versions.\n`sdk install java 11.0.22-amzn`  as  java-11.0.22-amzn was available for my codespace.\nclick on Y if prompted to change the default java version.\nCheck for java version using `java -version `.\nIf working fine great, else `sdk default java 11.0.22-amzn` or whatever version you have installed.",
        "question": "What command is used to check for available Java SDK versions?",
        "section": "Module 5: pyspark",
        "document_id": 363,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "~ Abhijit Chakraborty\n`sdk list java`  to check for available java sdk versions.\n`sdk install java 11.0.22-amzn`  as  java-11.0.22-amzn was available for my codespace.\nclick on Y if prompted to change the default java version.\nCheck for java version using `java -version `.\nIf working fine great, else `sdk default java 11.0.22-amzn` or whatever version you have installed.",
        "question": "How do you install a specific version of Java using the SDK?",
        "section": "Module 5: pyspark",
        "document_id": 363,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "~ Abhijit Chakraborty\n`sdk list java`  to check for available java sdk versions.\n`sdk install java 11.0.22-amzn`  as  java-11.0.22-amzn was available for my codespace.\nclick on Y if prompted to change the default java version.\nCheck for java version using `java -version `.\nIf working fine great, else `sdk default java 11.0.22-amzn` or whatever version you have installed.",
        "question": "What should you do if prompted to change the default Java version?",
        "section": "Module 5: pyspark",
        "document_id": 363,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "~ Abhijit Chakraborty\n`sdk list java`  to check for available java sdk versions.\n`sdk install java 11.0.22-amzn`  as  java-11.0.22-amzn was available for my codespace.\nclick on Y if prompted to change the default java version.\nCheck for java version using `java -version `.\nIf working fine great, else `sdk default java 11.0.22-amzn` or whatever version you have installed.",
        "question": "How can you verify the installed version of Java on your system?",
        "section": "Module 5: pyspark",
        "document_id": 363,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "~ Abhijit Chakraborty\n`sdk list java`  to check for available java sdk versions.\n`sdk install java 11.0.22-amzn`  as  java-11.0.22-amzn was available for my codespace.\nclick on Y if prompted to change the default java version.\nCheck for java version using `java -version `.\nIf working fine great, else `sdk default java 11.0.22-amzn` or whatever version you have installed.",
        "question": "What command can you use to set the default Java version if it's not working correctly?",
        "section": "Module 5: pyspark",
        "document_id": 363,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Sometimes while creating a dataproc cluster on GCP, the following error is encountered.\nSolution: As mentioned here, sometimes there might not be enough resources in the given region to allocate the request. Usually, gets freed up in a bit and one can create a cluster. – abhirup ghosh\nSolution 2:  Changing the type of boot-disk from PD-Balanced to PD-Standard, in terraform, helped solve the problem.- Sundara Kumar Padmanabhan",
        "question": "What error is sometimes encountered while creating a dataproc cluster on GCP?",
        "section": "Module 5: pyspark",
        "document_id": 364,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Sometimes while creating a dataproc cluster on GCP, the following error is encountered.\nSolution: As mentioned here, sometimes there might not be enough resources in the given region to allocate the request. Usually, gets freed up in a bit and one can create a cluster. – abhirup ghosh\nSolution 2:  Changing the type of boot-disk from PD-Balanced to PD-Standard, in terraform, helped solve the problem.- Sundara Kumar Padmanabhan",
        "question": "What is the first solution suggested for resolving the cluster creation error?",
        "section": "Module 5: pyspark",
        "document_id": 364,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Sometimes while creating a dataproc cluster on GCP, the following error is encountered.\nSolution: As mentioned here, sometimes there might not be enough resources in the given region to allocate the request. Usually, gets freed up in a bit and one can create a cluster. – abhirup ghosh\nSolution 2:  Changing the type of boot-disk from PD-Balanced to PD-Standard, in terraform, helped solve the problem.- Sundara Kumar Padmanabhan",
        "question": "Why might resources be unavailable in a given region when creating a dataproc cluster?",
        "section": "Module 5: pyspark",
        "document_id": 364,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Sometimes while creating a dataproc cluster on GCP, the following error is encountered.\nSolution: As mentioned here, sometimes there might not be enough resources in the given region to allocate the request. Usually, gets freed up in a bit and one can create a cluster. – abhirup ghosh\nSolution 2:  Changing the type of boot-disk from PD-Balanced to PD-Standard, in terraform, helped solve the problem.- Sundara Kumar Padmanabhan",
        "question": "What change did Sundara Kumar Padmanabhan suggest to help solve the problem?",
        "section": "Module 5: pyspark",
        "document_id": 364,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Sometimes while creating a dataproc cluster on GCP, the following error is encountered.\nSolution: As mentioned here, sometimes there might not be enough resources in the given region to allocate the request. Usually, gets freed up in a bit and one can create a cluster. – abhirup ghosh\nSolution 2:  Changing the type of boot-disk from PD-Balanced to PD-Standard, in terraform, helped solve the problem.- Sundara Kumar Padmanabhan",
        "question": "What type of boot-disk was changed to help resolve the issue?",
        "section": "Module 5: pyspark",
        "document_id": 364,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Pyspark converts the difference of two TimestampType values to Python's native datetime.timedelta object. The timedelta object only stores the duration in terms of days, seconds, and microseconds. Each of the three units of time must be manually converted into hours in order to express the total duration between the two timestamps using only hours.\nAnother way for achieving this is using the datediff (sql function). It receives this parameters\nUpper Date: the closest date you have. For example dropoff_datetime\nLower Date: the farthest date you have.  For example pickup_datetime\nAnd the result is returned in terms of days, so you could multiply the result for 24 in order to get the hours.",
        "question": "What does Pyspark convert the difference of two TimestampType values to?",
        "section": "Module 5: pyspark",
        "document_id": 365,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Pyspark converts the difference of two TimestampType values to Python's native datetime.timedelta object. The timedelta object only stores the duration in terms of days, seconds, and microseconds. Each of the three units of time must be manually converted into hours in order to express the total duration between the two timestamps using only hours.\nAnother way for achieving this is using the datediff (sql function). It receives this parameters\nUpper Date: the closest date you have. For example dropoff_datetime\nLower Date: the farthest date you have.  For example pickup_datetime\nAnd the result is returned in terms of days, so you could multiply the result for 24 in order to get the hours.",
        "question": "What are the units of time stored in Python's native datetime.timedelta object?",
        "section": "Module 5: pyspark",
        "document_id": 365,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Pyspark converts the difference of two TimestampType values to Python's native datetime.timedelta object. The timedelta object only stores the duration in terms of days, seconds, and microseconds. Each of the three units of time must be manually converted into hours in order to express the total duration between the two timestamps using only hours.\nAnother way for achieving this is using the datediff (sql function). It receives this parameters\nUpper Date: the closest date you have. For example dropoff_datetime\nLower Date: the farthest date you have.  For example pickup_datetime\nAnd the result is returned in terms of days, so you could multiply the result for 24 in order to get the hours.",
        "question": "How can one express the total duration between two timestamps using only hours?",
        "section": "Module 5: pyspark",
        "document_id": 365,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Pyspark converts the difference of two TimestampType values to Python's native datetime.timedelta object. The timedelta object only stores the duration in terms of days, seconds, and microseconds. Each of the three units of time must be manually converted into hours in order to express the total duration between the two timestamps using only hours.\nAnother way for achieving this is using the datediff (sql function). It receives this parameters\nUpper Date: the closest date you have. For example dropoff_datetime\nLower Date: the farthest date you have.  For example pickup_datetime\nAnd the result is returned in terms of days, so you could multiply the result for 24 in order to get the hours.",
        "question": "What parameters does the datediff SQL function receive?",
        "section": "Module 5: pyspark",
        "document_id": 365,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Pyspark converts the difference of two TimestampType values to Python's native datetime.timedelta object. The timedelta object only stores the duration in terms of days, seconds, and microseconds. Each of the three units of time must be manually converted into hours in order to express the total duration between the two timestamps using only hours.\nAnother way for achieving this is using the datediff (sql function). It receives this parameters\nUpper Date: the closest date you have. For example dropoff_datetime\nLower Date: the farthest date you have.  For example pickup_datetime\nAnd the result is returned in terms of days, so you could multiply the result for 24 in order to get the hours.",
        "question": "What calculation can be performed to convert the result of datediff from days to hours?",
        "section": "Module 5: pyspark",
        "document_id": 365,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This version combination worked for me:\nPySpark = 3.3.2\nPandas = 1.5.3\n\nIf it still has an error,",
        "question": "What version of PySpark is mentioned in the text?",
        "section": "Module 5: pyspark",
        "document_id": 366,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This version combination worked for me:\nPySpark = 3.3.2\nPandas = 1.5.3\n\nIf it still has an error,",
        "question": "What version of Pandas is recommended?",
        "section": "Module 5: pyspark",
        "document_id": 366,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This version combination worked for me:\nPySpark = 3.3.2\nPandas = 1.5.3\n\nIf it still has an error,",
        "question": "What should you do if there is still an error?",
        "section": "Module 5: pyspark",
        "document_id": 366,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This version combination worked for me:\nPySpark = 3.3.2\nPandas = 1.5.3\n\nIf it still has an error,",
        "question": "Is there a specific combination of software versions that worked for the author?",
        "section": "Module 5: pyspark",
        "document_id": 366,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This version combination worked for me:\nPySpark = 3.3.2\nPandas = 1.5.3\n\nIf it still has an error,",
        "question": "Which library version is listed as 1.5.3?",
        "section": "Module 5: pyspark",
        "document_id": 366,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Run this before SparkSession\nimport os\nimport sys\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable",
        "question": "What is the purpose of setting the environment variable 'PYSPARK_PYTHON'?",
        "section": "Module 5: pyspark",
        "document_id": 367,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Run this before SparkSession\nimport os\nimport sys\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable",
        "question": "Why should the script be run before SparkSession?",
        "section": "Module 5: pyspark",
        "document_id": 367,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Run this before SparkSession\nimport os\nimport sys\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable",
        "question": "What does 'sys.executable' represent in this context?",
        "section": "Module 5: pyspark",
        "document_id": 367,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Run this before SparkSession\nimport os\nimport sys\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable",
        "question": "What might happen if 'PYSPARK_DRIVER_PYTHON' is not set correctly?",
        "section": "Module 5: pyspark",
        "document_id": 367,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Run this before SparkSession\nimport os\nimport sys\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable",
        "question": "Are there any prerequisites for using the code snippet provided?",
        "section": "Module 5: pyspark",
        "document_id": 367,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "import os\nimport sys\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nDataproc Pricing: https://cloud.google.com/dataproc/pricing#on_gke_pricing",
        "question": "What environment variables are being set in the provided code?",
        "section": "Module 5: pyspark",
        "document_id": 368,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "import os\nimport sys\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nDataproc Pricing: https://cloud.google.com/dataproc/pricing#on_gke_pricing",
        "question": "What is the purpose of setting 'PYSPARK_PYTHON' and 'PYSPARK_DRIVER_PYTHON'?",
        "section": "Module 5: pyspark",
        "document_id": 368,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "import os\nimport sys\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nDataproc Pricing: https://cloud.google.com/dataproc/pricing#on_gke_pricing",
        "question": "Where can one find information about Dataproc pricing?",
        "section": "Module 5: pyspark",
        "document_id": 368,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "import os\nimport sys\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nDataproc Pricing: https://cloud.google.com/dataproc/pricing#on_gke_pricing",
        "question": "What programming languages are likely being used in the given code snippet?",
        "section": "Module 5: pyspark",
        "document_id": 368,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "import os\nimport sys\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nDataproc Pricing: https://cloud.google.com/dataproc/pricing#on_gke_pricing",
        "question": "How does the provided text relate to managing Python environments in Dataproc?",
        "section": "Module 5: pyspark",
        "document_id": 368,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: No, you can submit a job to DataProc from your local computer by installing gsutil (https://cloud.google.com/storage/docs/gsutil_install) and configuring it. Then, you can execute the following command from your local computer.\ngcloud dataproc jobs submit pyspark \\\n--cluster=de-zoomcamp-cluster \\\n--region=europe-west6 \\\ngs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py \\\n-- \\\n--input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\n--input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\n--output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020 (edited)",
        "question": "What tool do you need to install to submit a job to DataProc from your local computer?",
        "section": "Module 5: pyspark",
        "document_id": 369,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: No, you can submit a job to DataProc from your local computer by installing gsutil (https://cloud.google.com/storage/docs/gsutil_install) and configuring it. Then, you can execute the following command from your local computer.\ngcloud dataproc jobs submit pyspark \\\n--cluster=de-zoomcamp-cluster \\\n--region=europe-west6 \\\ngs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py \\\n-- \\\n--input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\n--input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\n--output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020 (edited)",
        "question": "What command is used to submit a PySpark job to DataProc?",
        "section": "Module 5: pyspark",
        "document_id": 369,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: No, you can submit a job to DataProc from your local computer by installing gsutil (https://cloud.google.com/storage/docs/gsutil_install) and configuring it. Then, you can execute the following command from your local computer.\ngcloud dataproc jobs submit pyspark \\\n--cluster=de-zoomcamp-cluster \\\n--region=europe-west6 \\\ngs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py \\\n-- \\\n--input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\n--input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\n--output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020 (edited)",
        "question": "What cluster name is specified in the command to submit a job?",
        "section": "Module 5: pyspark",
        "document_id": 369,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: No, you can submit a job to DataProc from your local computer by installing gsutil (https://cloud.google.com/storage/docs/gsutil_install) and configuring it. Then, you can execute the following command from your local computer.\ngcloud dataproc jobs submit pyspark \\\n--cluster=de-zoomcamp-cluster \\\n--region=europe-west6 \\\ngs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py \\\n-- \\\n--input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\n--input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\n--output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020 (edited)",
        "question": "Which region is the DataProc cluster located in as per the command?",
        "section": "Module 5: pyspark",
        "document_id": 369,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: No, you can submit a job to DataProc from your local computer by installing gsutil (https://cloud.google.com/storage/docs/gsutil_install) and configuring it. Then, you can execute the following command from your local computer.\ngcloud dataproc jobs submit pyspark \\\n--cluster=de-zoomcamp-cluster \\\n--region=europe-west6 \\\ngs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py \\\n-- \\\n--input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\n--input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\n--output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020 (edited)",
        "question": "What are the input paths for green and yellow taxi data in the command?",
        "section": "Module 5: pyspark",
        "document_id": 369,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "AttributeError: 'DataFrame' object has no attribute 'iteritems'\nthis is because the method inside the pyspark refers to a package that has been already deprecated\n(https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)\nYou can do this code below, which is mentioned in the stackoverflow link above:\nQ: DE Zoomcamp 5.6.3 - Setting up a Dataproc Cluster I cannot create a cluster and get this message. I tried many times as the FAQ said, but it didn't work. What can I do?\nError\nInsufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 250.0.\nRequest ID: 17942272465025572271\nA: The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\nMaster Node:\nMachine type: n2-standard-2\nPrimary disk size: 85 GB\nWorker Node:\nNumber of worker nodes: 2\nMachine type: n2-standard-2\nPrimary disk size: 80 GB\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB.",
        "question": "What is the cause of the AttributeError related to 'DataFrame' and 'iteritems'?",
        "section": "Module 5: pyspark",
        "document_id": 370,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "AttributeError: 'DataFrame' object has no attribute 'iteritems'\nthis is because the method inside the pyspark refers to a package that has been already deprecated\n(https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)\nYou can do this code below, which is mentioned in the stackoverflow link above:\nQ: DE Zoomcamp 5.6.3 - Setting up a Dataproc Cluster I cannot create a cluster and get this message. I tried many times as the FAQ said, but it didn't work. What can I do?\nError\nInsufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 250.0.\nRequest ID: 17942272465025572271\nA: The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\nMaster Node:\nMachine type: n2-standard-2\nPrimary disk size: 85 GB\nWorker Node:\nNumber of worker nodes: 2\nMachine type: n2-standard-2\nPrimary disk size: 80 GB\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB.",
        "question": "What should you do if you encounter an Insufficient 'SSD_TOTAL_GB' quota error when creating a Dataproc cluster?",
        "section": "Module 5: pyspark",
        "document_id": 370,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "AttributeError: 'DataFrame' object has no attribute 'iteritems'\nthis is because the method inside the pyspark refers to a package that has been already deprecated\n(https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)\nYou can do this code below, which is mentioned in the stackoverflow link above:\nQ: DE Zoomcamp 5.6.3 - Setting up a Dataproc Cluster I cannot create a cluster and get this message. I tried many times as the FAQ said, but it didn't work. What can I do?\nError\nInsufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 250.0.\nRequest ID: 17942272465025572271\nA: The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\nMaster Node:\nMachine type: n2-standard-2\nPrimary disk size: 85 GB\nWorker Node:\nNumber of worker nodes: 2\nMachine type: n2-standard-2\nPrimary disk size: 80 GB\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB.",
        "question": "What are the memory specifications for the master and worker nodes in the Dataproc cluster setup?",
        "section": "Module 5: pyspark",
        "document_id": 370,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "AttributeError: 'DataFrame' object has no attribute 'iteritems'\nthis is because the method inside the pyspark refers to a package that has been already deprecated\n(https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)\nYou can do this code below, which is mentioned in the stackoverflow link above:\nQ: DE Zoomcamp 5.6.3 - Setting up a Dataproc Cluster I cannot create a cluster and get this message. I tried many times as the FAQ said, but it didn't work. What can I do?\nError\nInsufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 250.0.\nRequest ID: 17942272465025572271\nA: The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\nMaster Node:\nMachine type: n2-standard-2\nPrimary disk size: 85 GB\nWorker Node:\nNumber of worker nodes: 2\nMachine type: n2-standard-2\nPrimary disk size: 80 GB\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB.",
        "question": "How much memory can be allocated to each worker node in the Dataproc cluster without exceeding the total limit?",
        "section": "Module 5: pyspark",
        "document_id": 370,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "AttributeError: 'DataFrame' object has no attribute 'iteritems'\nthis is because the method inside the pyspark refers to a package that has been already deprecated\n(https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)\nYou can do this code below, which is mentioned in the stackoverflow link above:\nQ: DE Zoomcamp 5.6.3 - Setting up a Dataproc Cluster I cannot create a cluster and get this message. I tried many times as the FAQ said, but it didn't work. What can I do?\nError\nInsufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 250.0.\nRequest ID: 17942272465025572271\nA: The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\nMaster Node:\nMachine type: n2-standard-2\nPrimary disk size: 85 GB\nWorker Node:\nNumber of worker nodes: 2\nMachine type: n2-standard-2\nPrimary disk size: 80 GB\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB.",
        "question": "Which machine type is recommended for the master and worker nodes in the Dataproc setup?",
        "section": "Module 5: pyspark",
        "document_id": 370,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The MacOS setup instruction (https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java) for setting the JAVA_HOME environment variable is for Intel-based Macs which have a default install location at /usr/local/. If you have an Apple Silicon mac, you will have to set JAVA_HOME to /opt/homebrew/, specifically in your .bashrc or .zshrc:\nexport JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\nexport PATH=\"$JAVA_HOME:$PATH\"\nConfirm that your path was correctly set by running the command: which java\nYou should expect to see the output:\n/opt/homebrew/opt/openjdk/bin/java\nReference: https://docs.brew.sh/Installation",
        "question": "What is the default install location for JAVA_HOME on Intel-based Macs?",
        "section": "Module 6: streaming with kafka",
        "document_id": 371,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The MacOS setup instruction (https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java) for setting the JAVA_HOME environment variable is for Intel-based Macs which have a default install location at /usr/local/. If you have an Apple Silicon mac, you will have to set JAVA_HOME to /opt/homebrew/, specifically in your .bashrc or .zshrc:\nexport JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\nexport PATH=\"$JAVA_HOME:$PATH\"\nConfirm that your path was correctly set by running the command: which java\nYou should expect to see the output:\n/opt/homebrew/opt/openjdk/bin/java\nReference: https://docs.brew.sh/Installation",
        "question": "Where should JAVA_HOME be set for Apple Silicon Macs?",
        "section": "Module 6: streaming with kafka",
        "document_id": 371,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The MacOS setup instruction (https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java) for setting the JAVA_HOME environment variable is for Intel-based Macs which have a default install location at /usr/local/. If you have an Apple Silicon mac, you will have to set JAVA_HOME to /opt/homebrew/, specifically in your .bashrc or .zshrc:\nexport JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\nexport PATH=\"$JAVA_HOME:$PATH\"\nConfirm that your path was correctly set by running the command: which java\nYou should expect to see the output:\n/opt/homebrew/opt/openjdk/bin/java\nReference: https://docs.brew.sh/Installation",
        "question": "What command can be used to confirm the JAVA_HOME path was correctly set?",
        "section": "Module 6: streaming with kafka",
        "document_id": 371,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The MacOS setup instruction (https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java) for setting the JAVA_HOME environment variable is for Intel-based Macs which have a default install location at /usr/local/. If you have an Apple Silicon mac, you will have to set JAVA_HOME to /opt/homebrew/, specifically in your .bashrc or .zshrc:\nexport JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\nexport PATH=\"$JAVA_HOME:$PATH\"\nConfirm that your path was correctly set by running the command: which java\nYou should expect to see the output:\n/opt/homebrew/opt/openjdk/bin/java\nReference: https://docs.brew.sh/Installation",
        "question": "What is the expected output when running the command to check the JAVA_HOME path?",
        "section": "Module 6: streaming with kafka",
        "document_id": 371,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The MacOS setup instruction (https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java) for setting the JAVA_HOME environment variable is for Intel-based Macs which have a default install location at /usr/local/. If you have an Apple Silicon mac, you will have to set JAVA_HOME to /opt/homebrew/, specifically in your .bashrc or .zshrc:\nexport JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\nexport PATH=\"$JAVA_HOME:$PATH\"\nConfirm that your path was correctly set by running the command: which java\nYou should expect to see the output:\n/opt/homebrew/opt/openjdk/bin/java\nReference: https://docs.brew.sh/Installation",
        "question": "In which files should JAVA_HOME be exported for Apple Silicon Macs?",
        "section": "Module 6: streaming with kafka",
        "document_id": 371,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check Docker Compose File:\nEnsure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\nOn Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed “docker ps.” I deleted them in docker desktop and then had no problem starting up the kafka environment.",
        "question": "What details should be checked in the docker-compose.yaml file for the 'control-center' service?",
        "section": "Module 6: streaming with kafka",
        "document_id": 372,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check Docker Compose File:\nEnsure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\nOn Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed “docker ps.” I deleted them in docker desktop and then had no problem starting up the kafka environment.",
        "question": "What issue did the user encounter when trying to start the kafka control center on Mac OSX?",
        "section": "Module 6: streaming with kafka",
        "document_id": 372,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check Docker Compose File:\nEnsure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\nOn Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed “docker ps.” I deleted them in docker desktop and then had no problem starting up the kafka environment.",
        "question": "How did the user resolve the issue with the kafka control center?",
        "section": "Module 6: streaming with kafka",
        "document_id": 372,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check Docker Compose File:\nEnsure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\nOn Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed “docker ps.” I deleted them in docker desktop and then had no problem starting up the kafka environment.",
        "question": "Which version of Mac OSX was mentioned in the text?",
        "section": "Module 6: streaming with kafka",
        "document_id": 372,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check Docker Compose File:\nEnsure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\nOn Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed “docker ps.” I deleted them in docker desktop and then had no problem starting up the kafka environment.",
        "question": "What command did the user run to check the currently running Docker containers?",
        "section": "Module 6: streaming with kafka",
        "document_id": 372,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\nTo create a virtual env and install packages (run only once)\npython -m venv env\nsource env/bin/activate\npip install -r ../requirements.txt\nTo activate it (you'll need to run it every time you need the virtual env):\nsource env/bin/activate\nTo deactivate it:\ndeactivate\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.",
        "question": "What command is used to create a virtual environment?",
        "section": "Module 6: streaming with kafka",
        "document_id": 373,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\nTo create a virtual env and install packages (run only once)\npython -m venv env\nsource env/bin/activate\npip install -r ../requirements.txt\nTo activate it (you'll need to run it every time you need the virtual env):\nsource env/bin/activate\nTo deactivate it:\ndeactivate\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.",
        "question": "How do you activate the virtual environment on MacOS and Linux?",
        "section": "Module 6: streaming with kafka",
        "document_id": 373,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\nTo create a virtual env and install packages (run only once)\npython -m venv env\nsource env/bin/activate\npip install -r ../requirements.txt\nTo activate it (you'll need to run it every time you need the virtual env):\nsource env/bin/activate\nTo deactivate it:\ndeactivate\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.",
        "question": "What is the command to install packages from a requirements.txt file?",
        "section": "Module 6: streaming with kafka",
        "document_id": 373,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\nTo create a virtual env and install packages (run only once)\npython -m venv env\nsource env/bin/activate\npip install -r ../requirements.txt\nTo activate it (you'll need to run it every time you need the virtual env):\nsource env/bin/activate\nTo deactivate it:\ndeactivate\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.",
        "question": "How do you deactivate the virtual environment?",
        "section": "Module 6: streaming with kafka",
        "document_id": 373,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\nTo create a virtual env and install packages (run only once)\npython -m venv env\nsource env/bin/activate\npip install -r ../requirements.txt\nTo activate it (you'll need to run it every time you need the virtual env):\nsource env/bin/activate\nTo deactivate it:\ndeactivate\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.",
        "question": "What is the difference in the activation command for Windows compared to MacOS and Linux?",
        "section": "Module 6: streaming with kafka",
        "document_id": 373,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ImportError: DLL load failed while importing cimpl: The specified module could not be found\nVerify Python Version:\nMake sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\n... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\nfrom ctypes import CDLL\nCDLL(\"C:\\\\Users\\\\YOUR_USER_NAME\\\\anaconda3\\\\envs\\\\dtcde\\\\Lib\\\\site-packages\\\\confluent_kafka.libs\\librdkafka-5d2e2910.dll\")\nIt seems that the error may occur depending on the OS and python version installed.\nALTERNATIVE:\nImportError: DLL load failed while importing cimpl\n✅SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\nYou need to set this DLL manually in Conda Env.\nSource: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2",
        "question": "What is the error message associated with importing cimpl?",
        "section": "Module 6: streaming with kafka",
        "document_id": 374,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ImportError: DLL load failed while importing cimpl: The specified module could not be found\nVerify Python Version:\nMake sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\n... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\nfrom ctypes import CDLL\nCDLL(\"C:\\\\Users\\\\YOUR_USER_NAME\\\\anaconda3\\\\envs\\\\dtcde\\\\Lib\\\\site-packages\\\\confluent_kafka.libs\\librdkafka-5d2e2910.dll\")\nIt seems that the error may occur depending on the OS and python version installed.\nALTERNATIVE:\nImportError: DLL load failed while importing cimpl\n✅SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\nYou need to set this DLL manually in Conda Env.\nSource: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2",
        "question": "What should you verify to ensure compatibility with the Avro library?",
        "section": "Module 6: streaming with kafka",
        "document_id": 374,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ImportError: DLL load failed while importing cimpl: The specified module could not be found\nVerify Python Version:\nMake sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\n... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\nfrom ctypes import CDLL\nCDLL(\"C:\\\\Users\\\\YOUR_USER_NAME\\\\anaconda3\\\\envs\\\\dtcde\\\\Lib\\\\site-packages\\\\confluent_kafka.libs\\librdkafka-5d2e2910.dll\")\nIt seems that the error may occur depending on the OS and python version installed.\nALTERNATIVE:\nImportError: DLL load failed while importing cimpl\n✅SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\nYou need to set this DLL manually in Conda Env.\nSource: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2",
        "question": "How can you manually load the required DLL in your code?",
        "section": "Module 6: streaming with kafka",
        "document_id": 374,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ImportError: DLL load failed while importing cimpl: The specified module could not be found\nVerify Python Version:\nMake sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\n... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\nfrom ctypes import CDLL\nCDLL(\"C:\\\\Users\\\\YOUR_USER_NAME\\\\anaconda3\\\\envs\\\\dtcde\\\\Lib\\\\site-packages\\\\confluent_kafka.libs\\librdkafka-5d2e2910.dll\")\nIt seems that the error may occur depending on the OS and python version installed.\nALTERNATIVE:\nImportError: DLL load failed while importing cimpl\n✅SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\nYou need to set this DLL manually in Conda Env.\nSource: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2",
        "question": "What PowerShell command can you use to resolve DLL load issues in Conda environments?",
        "section": "Module 6: streaming with kafka",
        "document_id": 374,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ImportError: DLL load failed while importing cimpl: The specified module could not be found\nVerify Python Version:\nMake sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\n... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\nfrom ctypes import CDLL\nCDLL(\"C:\\\\Users\\\\YOUR_USER_NAME\\\\anaconda3\\\\envs\\\\dtcde\\\\Lib\\\\site-packages\\\\confluent_kafka.libs\\librdkafka-5d2e2910.dll\")\nIt seems that the error may occur depending on the OS and python version installed.\nALTERNATIVE:\nImportError: DLL load failed while importing cimpl\n✅SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\nYou need to set this DLL manually in Conda Env.\nSource: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2",
        "question": "Where can you find more information about the issue related to importing cimpl?",
        "section": "Module 6: streaming with kafka",
        "document_id": 374,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅SOLUTION: pip install confluent-kafka[avro].\nFor some reason, Conda also doesn't include this when installing confluent-kafka via pip.\nMore sources on Anaconda and confluent-kafka issues:\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka",
        "question": "What command is suggested to install confluent-kafka with Avro support?",
        "section": "Module 6: streaming with kafka",
        "document_id": 375,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅SOLUTION: pip install confluent-kafka[avro].\nFor some reason, Conda also doesn't include this when installing confluent-kafka via pip.\nMore sources on Anaconda and confluent-kafka issues:\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka",
        "question": "Why might Conda fail to include certain components when installing confluent-kafka via pip?",
        "section": "Module 6: streaming with kafka",
        "document_id": 375,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅SOLUTION: pip install confluent-kafka[avro].\nFor some reason, Conda also doesn't include this when installing confluent-kafka via pip.\nMore sources on Anaconda and confluent-kafka issues:\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka",
        "question": "Where can one find more information regarding issues with Anaconda and confluent-kafka?",
        "section": "Module 6: streaming with kafka",
        "document_id": 375,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅SOLUTION: pip install confluent-kafka[avro].\nFor some reason, Conda also doesn't include this when installing confluent-kafka via pip.\nMore sources on Anaconda and confluent-kafka issues:\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka",
        "question": "What GitHub issues are referenced for troubleshooting confluent-kafka?",
        "section": "Module 6: streaming with kafka",
        "document_id": 375,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "✅SOLUTION: pip install confluent-kafka[avro].\nFor some reason, Conda also doesn't include this when installing confluent-kafka via pip.\nMore sources on Anaconda and confluent-kafka issues:\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka",
        "question": "Which online platform is suggested for seeking help on importing producer from confluent-kafka?",
        "section": "Module 6: streaming with kafka",
        "document_id": 375,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you get an error while running the command python3 stream.py worker\nRun pip uninstall kafka-python\nThen run pip install kafka-python==1.4.6\nWhat is the use of  Redpanda ?\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.",
        "question": "What command should you run if you encounter an error while executing python3 stream.py worker?",
        "section": "Module 6: streaming with kafka",
        "document_id": 376,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you get an error while running the command python3 stream.py worker\nRun pip uninstall kafka-python\nThen run pip install kafka-python==1.4.6\nWhat is the use of  Redpanda ?\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.",
        "question": "What specific version of kafka-python is recommended to install after uninstalling the current one?",
        "section": "Module 6: streaming with kafka",
        "document_id": 376,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you get an error while running the command python3 stream.py worker\nRun pip uninstall kafka-python\nThen run pip install kafka-python==1.4.6\nWhat is the use of  Redpanda ?\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.",
        "question": "What algorithm is Redpanda built on top of?",
        "section": "Module 6: streaming with kafka",
        "document_id": 376,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you get an error while running the command python3 stream.py worker\nRun pip uninstall kafka-python\nThen run pip install kafka-python==1.4.6\nWhat is the use of  Redpanda ?\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.",
        "question": "How does Redpanda compare to Kafka in terms of performance and complexity?",
        "section": "Module 6: streaming with kafka",
        "document_id": 376,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you get an error while running the command python3 stream.py worker\nRun pip uninstall kafka-python\nThen run pip install kafka-python==1.4.6\nWhat is the use of  Redpanda ?\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.",
        "question": "What makes Redpanda a cost-efficient option for streaming data applications?",
        "section": "Module 6: streaming with kafka",
        "document_id": 376,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Got this error because the docker container memory was exhausted. The dta file was upto 800MB but my docker container does not have enough memory to handle that.\nSolution was to load the file in chunks with Pandas, then create multiple parquet files for each dat file I was processing. This worked smoothly and the issue was resolved.",
        "question": "What error was encountered with the docker container?",
        "section": "Module 6: streaming with kafka",
        "document_id": 377,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Got this error because the docker container memory was exhausted. The dta file was upto 800MB but my docker container does not have enough memory to handle that.\nSolution was to load the file in chunks with Pandas, then create multiple parquet files for each dat file I was processing. This worked smoothly and the issue was resolved.",
        "question": "What was the size of the dta file mentioned?",
        "section": "Module 6: streaming with kafka",
        "document_id": 377,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Got this error because the docker container memory was exhausted. The dta file was upto 800MB but my docker container does not have enough memory to handle that.\nSolution was to load the file in chunks with Pandas, then create multiple parquet files for each dat file I was processing. This worked smoothly and the issue was resolved.",
        "question": "How was the issue with the docker container's memory resolved?",
        "section": "Module 6: streaming with kafka",
        "document_id": 377,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Got this error because the docker container memory was exhausted. The dta file was upto 800MB but my docker container does not have enough memory to handle that.\nSolution was to load the file in chunks with Pandas, then create multiple parquet files for each dat file I was processing. This worked smoothly and the issue was resolved.",
        "question": "What library was used to load the file in chunks?",
        "section": "Module 6: streaming with kafka",
        "document_id": 377,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Got this error because the docker container memory was exhausted. The dta file was upto 800MB but my docker container does not have enough memory to handle that.\nSolution was to load the file in chunks with Pandas, then create multiple parquet files for each dat file I was processing. This worked smoothly and the issue was resolved.",
        "question": "What format were the files converted to after processing?",
        "section": "Module 6: streaming with kafka",
        "document_id": 377,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv",
        "question": "What is the location of the rides.csv file in the Java example?",
        "section": "Module 6: streaming with kafka",
        "document_id": 378,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv",
        "question": "Which week is associated with the stream processing in the data engineering zoomcamp?",
        "section": "Module 6: streaming with kafka",
        "document_id": 378,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv",
        "question": "What is the filename mentioned in the text?",
        "section": "Module 6: streaming with kafka",
        "document_id": 378,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv",
        "question": "Where should the rides.csv file be copied from?",
        "section": "Module 6: streaming with kafka",
        "document_id": 378,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv",
        "question": "Is the file rides.csv found in a Java example?",
        "section": "Module 6: streaming with kafka",
        "document_id": 378,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\nKafka Python Videos - Rides.csv\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.",
        "question": "What is a suggested way to improve the audio quality of downloaded videos?",
        "section": "Module 6: streaming with kafka",
        "document_id": 379,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\nKafka Python Videos - Rides.csv\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.",
        "question": "Which media player did the author use to enhance the audio of the videos?",
        "section": "Module 6: streaming with kafka",
        "document_id": 379,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\nKafka Python Videos - Rides.csv\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.",
        "question": "What percentage increase in audio volume did the author apply using VLC media player?",
        "section": "Module 6: streaming with kafka",
        "document_id": 379,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\nKafka Python Videos - Rides.csv\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.",
        "question": "Where can one find the rides.csv data used by the producer.py Python programs?",
        "section": "Module 6: streaming with kafka",
        "document_id": 379,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\nKafka Python Videos - Rides.csv\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.",
        "question": "What resource does the author reference for accessing the rides.csv file?",
        "section": "Module 6: streaming with kafka",
        "document_id": 379,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you have this error, it most likely that your kafka broker docker container is not working.\nUse docker ps to confirm\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.",
        "question": "What does it indicate if you encounter this error related to Kafka?",
        "section": "Module 6: streaming with kafka",
        "document_id": 380,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you have this error, it most likely that your kafka broker docker container is not working.\nUse docker ps to confirm\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.",
        "question": "How can you confirm if your Kafka broker docker container is working?",
        "section": "Module 6: streaming with kafka",
        "document_id": 380,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you have this error, it most likely that your kafka broker docker container is not working.\nUse docker ps to confirm\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.",
        "question": "What command should you run to start all instances in the docker compose yaml file folder?",
        "section": "Module 6: streaming with kafka",
        "document_id": 380,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you have this error, it most likely that your kafka broker docker container is not working.\nUse docker ps to confirm\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.",
        "question": "What is the purpose of the 'docker compose up -d' command?",
        "section": "Module 6: streaming with kafka",
        "document_id": 380,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you have this error, it most likely that your kafka broker docker container is not working.\nUse docker ps to confirm\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.",
        "question": "What is the significance of using 'docker ps' in the troubleshooting process?",
        "section": "Module 6: streaming with kafka",
        "document_id": 380,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ankush said we can focus on horizontal scaling option.\n“think of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling”",
        "question": "What does Ankush suggest about scaling options?",
        "section": "Module 6: streaming with kafka",
        "document_id": 381,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ankush said we can focus on horizontal scaling option.\n“think of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling”",
        "question": "How does Ankush define horizontal scaling?",
        "section": "Module 6: streaming with kafka",
        "document_id": 381,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ankush said we can focus on horizontal scaling option.\n“think of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling”",
        "question": "What perspective does Ankush recommend for thinking about scaling?",
        "section": "Module 6: streaming with kafka",
        "document_id": 381,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ankush said we can focus on horizontal scaling option.\n“think of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling”",
        "question": "What is meant by consuming messages in the context of horizontal scaling?",
        "section": "Module 6: streaming with kafka",
        "document_id": 381,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ankush said we can focus on horizontal scaling option.\n“think of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling”",
        "question": "Why is horizontal scaling relevant to consumer end performance?",
        "section": "Module 6: streaming with kafka",
        "document_id": 381,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub.\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose",
        "question": "What should you do if you encounter an error related to sparks and juypter images?",
        "section": "Module 6: streaming with kafka",
        "document_id": 382,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub.\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose",
        "question": "Why are the sparks and juypter images not available on dockerHub?",
        "section": "Module 6: streaming with kafka",
        "document_id": 382,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub.\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose",
        "question": "What command should you run in the spark folder to build the images?",
        "section": "Module 6: streaming with kafka",
        "document_id": 382,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub.\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose",
        "question": "What environment is required to run the command ./build.sh?",
        "section": "Module 6: streaming with kafka",
        "document_id": 382,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub.\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose",
        "question": "What will happen if you do not build the images before running docker compose?",
        "section": "Module 6: streaming with kafka",
        "document_id": 382,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Run this command in terminal in the same directory (/docker/spark):\nchmod +x build.sh",
        "question": "What command should be run in the terminal?",
        "section": "Module 6: streaming with kafka",
        "document_id": 383,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Run this command in terminal in the same directory (/docker/spark):\nchmod +x build.sh",
        "question": "In which directory should the command be executed?",
        "section": "Module 6: streaming with kafka",
        "document_id": 383,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Run this command in terminal in the same directory (/docker/spark):\nchmod +x build.sh",
        "question": "What is the purpose of the command 'chmod +x build.sh'?",
        "section": "Module 6: streaming with kafka",
        "document_id": 383,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Run this command in terminal in the same directory (/docker/spark):\nchmod +x build.sh",
        "question": "What does the '+x' option do in the chmod command?",
        "section": "Module 6: streaming with kafka",
        "document_id": 383,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Run this command in terminal in the same directory (/docker/spark):\nchmod +x build.sh",
        "question": "What file is being modified by the command?",
        "section": "Module 6: streaming with kafka",
        "document_id": 383,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Restarting all services worked for me:\ndocker-compose down\ndocker-compose up",
        "question": "What commands are used to restart all services in the text?",
        "section": "Module 6: streaming with kafka",
        "document_id": 384,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Restarting all services worked for me:\ndocker-compose down\ndocker-compose up",
        "question": "What does the command 'docker-compose down' do?",
        "section": "Module 6: streaming with kafka",
        "document_id": 384,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Restarting all services worked for me:\ndocker-compose down\ndocker-compose up",
        "question": "What does the command 'docker-compose up' do?",
        "section": "Module 6: streaming with kafka",
        "document_id": 384,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Restarting all services worked for me:\ndocker-compose down\ndocker-compose up",
        "question": "What was the outcome of restarting all services according to the text?",
        "section": "Module 6: streaming with kafka",
        "document_id": 384,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Restarting all services worked for me:\ndocker-compose down\ndocker-compose up",
        "question": "Is there any specific context given for the need to restart services?",
        "section": "Module 6: streaming with kafka",
        "document_id": 384,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\n…\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n…\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n…\nSolution:\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\nSolution 2:\nCheck what Spark version your local machine has\npyspark –version\nspark-submit –version\nAdd your version to SPARK_VERSION in build.sh",
        "question": "What error was encountered when running ./spark-submit.sh streaming.py?",
        "section": "Module 6: streaming with kafka",
        "document_id": 385,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\n…\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n…\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n…\nSolution:\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\nSolution 2:\nCheck what Spark version your local machine has\npyspark –version\nspark-submit –version\nAdd your version to SPARK_VERSION in build.sh",
        "question": "What does the log entry about GarbageCollectionMetrics suggest users should configure?",
        "section": "Module 6: streaming with kafka",
        "document_id": 385,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\n…\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n…\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n…\nSolution:\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\nSolution 2:\nCheck what Spark version your local machine has\npyspark –version\nspark-submit –version\nAdd your version to SPARK_VERSION in build.sh",
        "question": "What is the recommended solution for resolving the mismatch of PySpark versions?",
        "section": "Module 6: streaming with kafka",
        "document_id": 385,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\n…\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n…\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n…\nSolution:\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\nSolution 2:\nCheck what Spark version your local machine has\npyspark –version\nspark-submit –version\nAdd your version to SPARK_VERSION in build.sh",
        "question": "Which command can you use to check the version of Spark installed on your local machine?",
        "section": "Module 6: streaming with kafka",
        "document_id": 385,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\n…\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n…\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n…\nSolution:\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\nSolution 2:\nCheck what Spark version your local machine has\npyspark –version\nspark-submit –version\nAdd your version to SPARK_VERSION in build.sh",
        "question": "What was the reason provided for the application being killed in the logs?",
        "section": "Module 6: streaming with kafka",
        "document_id": 385,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Start a new terminal\nRun: docker ps\nCopy the CONTAINER ID of the spark-master container\nRun: docker exec -it <spark_master_container_id> bash\nRun: cat logs/spark-master.out\nCheck for the log when the error happened\nGoogle the error message from there",
        "question": "What command is used to list running Docker containers?",
        "section": "Module 6: streaming with kafka",
        "document_id": 386,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Start a new terminal\nRun: docker ps\nCopy the CONTAINER ID of the spark-master container\nRun: docker exec -it <spark_master_container_id> bash\nRun: cat logs/spark-master.out\nCheck for the log when the error happened\nGoogle the error message from there",
        "question": "How do you access the bash shell of a specific Docker container?",
        "section": "Module 6: streaming with kafka",
        "document_id": 386,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Start a new terminal\nRun: docker ps\nCopy the CONTAINER ID of the spark-master container\nRun: docker exec -it <spark_master_container_id> bash\nRun: cat logs/spark-master.out\nCheck for the log when the error happened\nGoogle the error message from there",
        "question": "Where can you find the Spark master log file?",
        "section": "Module 6: streaming with kafka",
        "document_id": 386,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Start a new terminal\nRun: docker ps\nCopy the CONTAINER ID of the spark-master container\nRun: docker exec -it <spark_master_container_id> bash\nRun: cat logs/spark-master.out\nCheck for the log when the error happened\nGoogle the error message from there",
        "question": "What should you do after finding the error message in the logs?",
        "section": "Module 6: streaming with kafka",
        "document_id": 386,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Start a new terminal\nRun: docker ps\nCopy the CONTAINER ID of the spark-master container\nRun: docker exec -it <spark_master_container_id> bash\nRun: cat logs/spark-master.out\nCheck for the log when the error happened\nGoogle the error message from there",
        "question": "What information is needed to execute the command 'docker exec -it <spark_master_container_id> bash'?",
        "section": "Module 6: streaming with kafka",
        "document_id": 386,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure your java version is 11 or 8.\nCheck your version by:\njava --version\nCheck all your versions by:\n/usr/libexec/java_home -V\nIf you already have got java 11 but just not selected as default, select the specific version by:\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\n(or other version of 11)",
        "question": "What versions of Java are mentioned in the text?",
        "section": "Module 6: streaming with kafka",
        "document_id": 387,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure your java version is 11 or 8.\nCheck your version by:\njava --version\nCheck all your versions by:\n/usr/libexec/java_home -V\nIf you already have got java 11 but just not selected as default, select the specific version by:\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\n(or other version of 11)",
        "question": "How can you check your current Java version?",
        "section": "Module 6: streaming with kafka",
        "document_id": 387,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure your java version is 11 or 8.\nCheck your version by:\njava --version\nCheck all your versions by:\n/usr/libexec/java_home -V\nIf you already have got java 11 but just not selected as default, select the specific version by:\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\n(or other version of 11)",
        "question": "What command do you use to check all installed Java versions?",
        "section": "Module 6: streaming with kafka",
        "document_id": 387,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure your java version is 11 or 8.\nCheck your version by:\njava --version\nCheck all your versions by:\n/usr/libexec/java_home -V\nIf you already have got java 11 but just not selected as default, select the specific version by:\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\n(or other version of 11)",
        "question": "What command should you execute to set Java 11 as the default version?",
        "section": "Module 6: streaming with kafka",
        "document_id": 387,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Make sure your java version is 11 or 8.\nCheck your version by:\njava --version\nCheck all your versions by:\n/usr/libexec/java_home -V\nIf you already have got java 11 but just not selected as default, select the specific version by:\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\n(or other version of 11)",
        "question": "What should you do if you have Java 11 but it's not the default version?",
        "section": "Module 6: streaming with kafka",
        "document_id": 387,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\nSolution:\nIn build.gradle file, I added the following at the end:\nshadowJar {\narchiveBaseName = \"java-kafka-rides\"\narchiveClassifier = ''\n}\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar",
        "question": "What issue is described regarding the dependencies in the gradle.build?",
        "section": "Module 6: streaming with kafka",
        "document_id": 388,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\nSolution:\nIn build.gradle file, I added the following at the end:\nshadowJar {\narchiveBaseName = \"java-kafka-rides\"\narchiveClassifier = ''\n}\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar",
        "question": "What changes were made to the build.gradle file to address the issue?",
        "section": "Module 6: streaming with kafka",
        "document_id": 388,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\nSolution:\nIn build.gradle file, I added the following at the end:\nshadowJar {\narchiveBaseName = \"java-kafka-rides\"\narchiveClassifier = ''\n}\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar",
        "question": "What command was run in the command line after modifying the build.gradle file?",
        "section": "Module 6: streaming with kafka",
        "document_id": 388,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\nSolution:\nIn build.gradle file, I added the following at the end:\nshadowJar {\narchiveBaseName = \"java-kafka-rides\"\narchiveClassifier = ''\n}\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar",
        "question": "What is the resulting file created after running the shadowjar command?",
        "section": "Module 6: streaming with kafka",
        "document_id": 388,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\nSolution:\nIn build.gradle file, I added the following at the end:\nshadowJar {\narchiveBaseName = \"java-kafka-rides\"\narchiveClassifier = ''\n}\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar",
        "question": "What does the 'archiveBaseName' in the shadowJar configuration represent?",
        "section": "Module 6: streaming with kafka",
        "document_id": 388,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\nfastavro: pip install fastavro\nAbhirup Ghosh\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.",
        "question": "How can you install the confluent-kafka library?",
        "section": "Module 6: streaming with kafka",
        "document_id": 389,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\nfastavro: pip install fastavro\nAbhirup Ghosh\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.",
        "question": "What are the installation commands for fastavro?",
        "section": "Module 6: streaming with kafka",
        "document_id": 389,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\nfastavro: pip install fastavro\nAbhirup Ghosh\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.",
        "question": "Is the Faust library currently maintained?",
        "section": "Module 6: streaming with kafka",
        "document_id": 389,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\nfastavro: pip install fastavro\nAbhirup Ghosh\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.",
        "question": "What resources are available for learning about Python in relation to streaming?",
        "section": "Module 6: streaming with kafka",
        "document_id": 389,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\nfastavro: pip install fastavro\nAbhirup Ghosh\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.",
        "question": "Why is it recommended to watch the Java videos before the Python videos?",
        "section": "Module 6: streaming with kafka",
        "document_id": 389,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the project directory, run:\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java",
        "question": "What command is used to run the Java program in the project directory?",
        "section": "Module 6: streaming with kafka",
        "document_id": 390,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the project directory, run:\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java",
        "question": "Which Java class is being executed in the command?",
        "section": "Module 6: streaming with kafka",
        "document_id": 390,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the project directory, run:\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java",
        "question": "What does the '-cp' option specify in the Java command?",
        "section": "Module 6: streaming with kafka",
        "document_id": 390,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the project directory, run:\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java",
        "question": "What is the significance of '<jar_name>-1.0-SNAPSHOT.jar' in the command?",
        "section": "Module 6: streaming with kafka",
        "document_id": 390,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In the project directory, run:\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java",
        "question": "What is the expected file structure based on the provided command?",
        "section": "Module 6: streaming with kafka",
        "document_id": 390,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For example, when running JsonConsumer.java, got:\nConsuming form kafka started\nRESULTS:::0\nRESULTS:::0\nRESULTS:::0\nOr when running JsonProducer.java, got:\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\nSolution:\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)",
        "question": "What error is encountered when running JsonProducer.java?",
        "section": "Module 6: streaming with kafka",
        "document_id": 391,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For example, when running JsonConsumer.java, got:\nConsuming form kafka started\nRESULTS:::0\nRESULTS:::0\nRESULTS:::0\nOr when running JsonProducer.java, got:\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\nSolution:\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)",
        "question": "What should be verified in the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG?",
        "section": "Module 6: streaming with kafka",
        "document_id": 391,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For example, when running JsonConsumer.java, got:\nConsuming form kafka started\nRESULTS:::0\nRESULTS:::0\nRESULTS:::0\nOr when running JsonProducer.java, got:\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\nSolution:\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)",
        "question": "What file needs to be updated with the cluster key and secrets?",
        "section": "Module 6: streaming with kafka",
        "document_id": 391,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For example, when running JsonConsumer.java, got:\nConsuming form kafka started\nRESULTS:::0\nRESULTS:::0\nRESULTS:::0\nOr when running JsonProducer.java, got:\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\nSolution:\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)",
        "question": "What does the output 'RESULTS:::0' indicate when running JsonConsumer.java?",
        "section": "Module 6: streaming with kafka",
        "document_id": 391,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For example, when running JsonConsumer.java, got:\nConsuming form kafka started\nRESULTS:::0\nRESULTS:::0\nRESULTS:::0\nOr when running JsonProducer.java, got:\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\nSolution:\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)",
        "question": "What are the two examples of server URLs provided in the text?",
        "section": "Module 6: streaming with kafka",
        "document_id": 391,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\nSolution:\n(Source)\nVS Code\n→ Explorer (first icon on the left navigation bar)\n→ JAVA PROJECTS (bottom collapsable)\n→  icon next in the rightmost position to JAVA PROJECTS\n→  clean Workspace\n→ Confirm by clicking Reload and Delete\nNow you will be able to see the triangle icon next to each test like what you normally see in python tests.\nE.g.:\nYou can also add classes and packages in this window instead of creating files in the project directory",
        "question": "What icon is usually found next to each test in VS Code?",
        "section": "Module 6: streaming with kafka",
        "document_id": 392,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\nSolution:\n(Source)\nVS Code\n→ Explorer (first icon on the left navigation bar)\n→ JAVA PROJECTS (bottom collapsable)\n→  icon next in the rightmost position to JAVA PROJECTS\n→  clean Workspace\n→ Confirm by clicking Reload and Delete\nNow you will be able to see the triangle icon next to each test like what you normally see in python tests.\nE.g.:\nYou can also add classes and packages in this window instead of creating files in the project directory",
        "question": "What steps should be followed to see the triangle icon next to each test?",
        "section": "Module 6: streaming with kafka",
        "document_id": 392,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\nSolution:\n(Source)\nVS Code\n→ Explorer (first icon on the left navigation bar)\n→ JAVA PROJECTS (bottom collapsable)\n→  icon next in the rightmost position to JAVA PROJECTS\n→  clean Workspace\n→ Confirm by clicking Reload and Delete\nNow you will be able to see the triangle icon next to each test like what you normally see in python tests.\nE.g.:\nYou can also add classes and packages in this window instead of creating files in the project directory",
        "question": "Where can one find the 'JAVA PROJECTS' option in VS Code?",
        "section": "Module 6: streaming with kafka",
        "document_id": 392,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\nSolution:\n(Source)\nVS Code\n→ Explorer (first icon on the left navigation bar)\n→ JAVA PROJECTS (bottom collapsable)\n→  icon next in the rightmost position to JAVA PROJECTS\n→  clean Workspace\n→ Confirm by clicking Reload and Delete\nNow you will be able to see the triangle icon next to each test like what you normally see in python tests.\nE.g.:\nYou can also add classes and packages in this window instead of creating files in the project directory",
        "question": "What action must be taken after cleaning the workspace in VS Code?",
        "section": "Module 6: streaming with kafka",
        "document_id": 392,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\nSolution:\n(Source)\nVS Code\n→ Explorer (first icon on the left navigation bar)\n→ JAVA PROJECTS (bottom collapsable)\n→  icon next in the rightmost position to JAVA PROJECTS\n→  clean Workspace\n→ Confirm by clicking Reload and Delete\nNow you will be able to see the triangle icon next to each test like what you normally see in python tests.\nE.g.:\nYou can also add classes and packages in this window instead of creating files in the project directory",
        "question": "Can classes and packages be added in the same window as the JAVA PROJECTS, and if so, how?",
        "section": "Module 6: streaming with kafka",
        "document_id": 392,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In Confluent Cloud:\nEnvironment → default (or whatever you named your environment as) → The right navigation bar →  “Stream Governance API” →  The URL under “Endpoint”\nAnd create credentials from Credentials section below it",
        "question": "What steps are involved in accessing the Stream Governance API in Confluent Cloud?",
        "section": "Module 6: streaming with kafka",
        "document_id": 393,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In Confluent Cloud:\nEnvironment → default (or whatever you named your environment as) → The right navigation bar →  “Stream Governance API” →  The URL under “Endpoint”\nAnd create credentials from Credentials section below it",
        "question": "Where can the URL for the Stream Governance API be found?",
        "section": "Module 6: streaming with kafka",
        "document_id": 393,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In Confluent Cloud:\nEnvironment → default (or whatever you named your environment as) → The right navigation bar →  “Stream Governance API” →  The URL under “Endpoint”\nAnd create credentials from Credentials section below it",
        "question": "How can you create credentials for the Stream Governance API in Confluent Cloud?",
        "section": "Module 6: streaming with kafka",
        "document_id": 393,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In Confluent Cloud:\nEnvironment → default (or whatever you named your environment as) → The right navigation bar →  “Stream Governance API” →  The URL under “Endpoint”\nAnd create credentials from Credentials section below it",
        "question": "What is the first action you need to take to use the Stream Governance API?",
        "section": "Module 6: streaming with kafka",
        "document_id": 393,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In Confluent Cloud:\nEnvironment → default (or whatever you named your environment as) → The right navigation bar →  “Stream Governance API” →  The URL under “Endpoint”\nAnd create credentials from Credentials section below it",
        "question": "What does the right navigation bar in Confluent Cloud provide access to?",
        "section": "Module 6: streaming with kafka",
        "document_id": 393,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.",
        "question": "How can you check the version of your local Spark?",
        "section": "Module 6: streaming with kafka",
        "document_id": 394,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.",
        "question": "What command is used to check the Spark version?",
        "section": "Module 6: streaming with kafka",
        "document_id": 394,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.",
        "question": "Where do you need to ensure SPARK_VERSION matches your local version?",
        "section": "Module 6: streaming with kafka",
        "document_id": 394,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.",
        "question": "What should you verify about the PySpark installation?",
        "section": "Module 6: streaming with kafka",
        "document_id": 394,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.",
        "question": "What file contains the SPARK_VERSION that needs to match your local version?",
        "section": "Module 6: streaming with kafka",
        "document_id": 394,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "According to https://github.com/dpkp/kafka-python/\n“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING”\nUse pip install kafka-python-ng instead",
        "question": "What is the reason for suggesting the use of kafka-python-ng?",
        "section": "Project",
        "document_id": 395,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "According to https://github.com/dpkp/kafka-python/\n“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING”\nUse pip install kafka-python-ng instead",
        "question": "Where can one find the recommended kafka-python-ng package?",
        "section": "Project",
        "document_id": 395,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "According to https://github.com/dpkp/kafka-python/\n“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING”\nUse pip install kafka-python-ng instead",
        "question": "What command should be used to install kafka-python-ng?",
        "section": "Project",
        "document_id": 395,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "According to https://github.com/dpkp/kafka-python/\n“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING”\nUse pip install kafka-python-ng instead",
        "question": "What issues are associated with the current releases of kafka-python?",
        "section": "Project",
        "document_id": 395,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "According to https://github.com/dpkp/kafka-python/\n“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING”\nUse pip install kafka-python-ng instead",
        "question": "Is kafka-python-ng an alternative to the original kafka-python library?",
        "section": "Project",
        "document_id": 395,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Each submitted project will be evaluated by 3 (three) randomly assigned students that have also submitted the project.\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here.",
        "question": "How many students will evaluate each submitted project?",
        "section": "Project",
        "document_id": 396,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Each submitted project will be evaluated by 3 (three) randomly assigned students that have also submitted the project.\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here.",
        "question": "What is the consequence of not grading the projects of fellow students?",
        "section": "Project",
        "document_id": 396,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Each submitted project will be evaluated by 3 (three) randomly assigned students that have also submitted the project.\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here.",
        "question": "What determines the final grade a student receives for their project?",
        "section": "Project",
        "document_id": 396,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Each submitted project will be evaluated by 3 (three) randomly assigned students that have also submitted the project.\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here.",
        "question": "What must peer review criteria adhere to during the evaluation process?",
        "section": "Project",
        "document_id": 396,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Each submitted project will be evaluated by 3 (three) randomly assigned students that have also submitted the project.\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here.",
        "question": "Who will be responsible for grading the projects of fellow students?",
        "section": "Project",
        "document_id": 396,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There is only ONE project for this Zoomcamp. You do not need to submit or create two projects. There are simply TWO chances to pass the course. You can use the Second Attempt if you a) fail the first attempt b) do not have the time due to other engagements such as holiday or sickness etc. to enter your project into the first attempt.",
        "question": "How many projects do students need to submit for the Zoomcamp?",
        "section": "Project",
        "document_id": 397,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There is only ONE project for this Zoomcamp. You do not need to submit or create two projects. There are simply TWO chances to pass the course. You can use the Second Attempt if you a) fail the first attempt b) do not have the time due to other engagements such as holiday or sickness etc. to enter your project into the first attempt.",
        "question": "What are the two opportunities provided for passing the course?",
        "section": "Project",
        "document_id": 397,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There is only ONE project for this Zoomcamp. You do not need to submit or create two projects. There are simply TWO chances to pass the course. You can use the Second Attempt if you a) fail the first attempt b) do not have the time due to other engagements such as holiday or sickness etc. to enter your project into the first attempt.",
        "question": "Under what circumstances can a student use the Second Attempt?",
        "section": "Project",
        "document_id": 397,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There is only ONE project for this Zoomcamp. You do not need to submit or create two projects. There are simply TWO chances to pass the course. You can use the Second Attempt if you a) fail the first attempt b) do not have the time due to other engagements such as holiday or sickness etc. to enter your project into the first attempt.",
        "question": "Is it mandatory to create two separate projects for the Zoomcamp?",
        "section": "Project",
        "document_id": 397,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "There is only ONE project for this Zoomcamp. You do not need to submit or create two projects. There are simply TWO chances to pass the course. You can use the Second Attempt if you a) fail the first attempt b) do not have the time due to other engagements such as holiday or sickness etc. to enter your project into the first attempt.",
        "question": "What should a student do if they are unable to submit their project due to illness?",
        "section": "Project",
        "document_id": 397,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "See a list of datasets here: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_7_project/datasets.md",
        "question": "What is the URL to access the list of datasets?",
        "section": "Project",
        "document_id": 398,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "See a list of datasets here: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_7_project/datasets.md",
        "question": "In which week of the Data Engineering Zoomcamp project can the datasets be found?",
        "section": "Project",
        "document_id": 398,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "See a list of datasets here: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_7_project/datasets.md",
        "question": "What organization is hosting the Data Engineering Zoomcamp?",
        "section": "Project",
        "document_id": 398,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "See a list of datasets here: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_7_project/datasets.md",
        "question": "What type of resources are listed in the provided link?",
        "section": "Project",
        "document_id": 398,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "See a list of datasets here: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_7_project/datasets.md",
        "question": "How can someone access the datasets mentioned in the text?",
        "section": "Project",
        "document_id": 398,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to redefine the python environment variable to that of your user account",
        "question": "What is the purpose of redefining the Python environment variable?",
        "section": "Project",
        "document_id": 399,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to redefine the python environment variable to that of your user account",
        "question": "Whose user account should the Python environment variable be set to?",
        "section": "Project",
        "document_id": 399,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to redefine the python environment variable to that of your user account",
        "question": "What steps are needed to redefine the Python environment variable?",
        "section": "Project",
        "document_id": 399,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to redefine the python environment variable to that of your user account",
        "question": "Why might someone need to change their Python environment variable?",
        "section": "Project",
        "document_id": 399,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You need to redefine the python environment variable to that of your user account",
        "question": "What could happen if the Python environment variable is not set correctly?",
        "section": "Project",
        "document_id": 399,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Initiate a Spark Session\nspark = (SparkSession\n.builder\n.appName(app_name)\n.master(master=master)\n.getOrCreate())\nspark.streams.resetTerminated()\nquery1 = spark\n.readStream\n…\n…\n.load()\nquery2 = spark\n.readStream\n…\n…\n.load()\nquery3 = spark\n.readStream\n…\n…\n.load()\nquery1.start()\nquery2.start()\nquery3.start()\nspark.streams.awaitAnyTermination() #waits for any one of the query to receive kill signal or error failure. This is asynchronous\n# On the contrary query3.start().awaitTermination() is a blocking ex call. Works well when we are reading only from one topic.",
        "question": "What is the purpose of initiating a Spark Session?",
        "section": "Project",
        "document_id": 400,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Initiate a Spark Session\nspark = (SparkSession\n.builder\n.appName(app_name)\n.master(master=master)\n.getOrCreate())\nspark.streams.resetTerminated()\nquery1 = spark\n.readStream\n…\n…\n.load()\nquery2 = spark\n.readStream\n…\n…\n.load()\nquery3 = spark\n.readStream\n…\n…\n.load()\nquery1.start()\nquery2.start()\nquery3.start()\nspark.streams.awaitAnyTermination() #waits for any one of the query to receive kill signal or error failure. This is asynchronous\n# On the contrary query3.start().awaitTermination() is a blocking ex call. Works well when we are reading only from one topic.",
        "question": "How do you start multiple streaming queries in Spark?",
        "section": "Project",
        "document_id": 400,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Initiate a Spark Session\nspark = (SparkSession\n.builder\n.appName(app_name)\n.master(master=master)\n.getOrCreate())\nspark.streams.resetTerminated()\nquery1 = spark\n.readStream\n…\n…\n.load()\nquery2 = spark\n.readStream\n…\n…\n.load()\nquery3 = spark\n.readStream\n…\n…\n.load()\nquery1.start()\nquery2.start()\nquery3.start()\nspark.streams.awaitAnyTermination() #waits for any one of the query to receive kill signal or error failure. This is asynchronous\n# On the contrary query3.start().awaitTermination() is a blocking ex call. Works well when we are reading only from one topic.",
        "question": "What method is called to wait for any one of the streaming queries to receive a kill signal or error failure?",
        "section": "Project",
        "document_id": 400,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Initiate a Spark Session\nspark = (SparkSession\n.builder\n.appName(app_name)\n.master(master=master)\n.getOrCreate())\nspark.streams.resetTerminated()\nquery1 = spark\n.readStream\n…\n…\n.load()\nquery2 = spark\n.readStream\n…\n…\n.load()\nquery3 = spark\n.readStream\n…\n…\n.load()\nquery1.start()\nquery2.start()\nquery3.start()\nspark.streams.awaitAnyTermination() #waits for any one of the query to receive kill signal or error failure. This is asynchronous\n# On the contrary query3.start().awaitTermination() is a blocking ex call. Works well when we are reading only from one topic.",
        "question": "What is the difference between query3.start().awaitTermination() and spark.streams.awaitAnyTermination()?",
        "section": "Project",
        "document_id": 400,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Initiate a Spark Session\nspark = (SparkSession\n.builder\n.appName(app_name)\n.master(master=master)\n.getOrCreate())\nspark.streams.resetTerminated()\nquery1 = spark\n.readStream\n…\n…\n.load()\nquery2 = spark\n.readStream\n…\n…\n.load()\nquery3 = spark\n.readStream\n…\n…\n.load()\nquery1.start()\nquery2.start()\nquery3.start()\nspark.streams.awaitAnyTermination() #waits for any one of the query to receive kill signal or error failure. This is asynchronous\n# On the contrary query3.start().awaitTermination() is a blocking ex call. Works well when we are reading only from one topic.",
        "question": "What parameters are used when creating a SparkSession in the given code?",
        "section": "Project",
        "document_id": 400,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Transformed data can be moved in to azure blob storage and then it can be moved in to azure SQL DB, instead of moving directly from databricks to Azure SQL DB.",
        "question": "What is the purpose of moving transformed data to Azure Blob Storage?",
        "section": "Project",
        "document_id": 401,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Transformed data can be moved in to azure blob storage and then it can be moved in to azure SQL DB, instead of moving directly from databricks to Azure SQL DB.",
        "question": "Why might one choose to move data to Azure Blob Storage before Azure SQL DB?",
        "section": "Project",
        "document_id": 401,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Transformed data can be moved in to azure blob storage and then it can be moved in to azure SQL DB, instead of moving directly from databricks to Azure SQL DB.",
        "question": "Can data be moved directly from Databricks to Azure SQL DB?",
        "section": "Project",
        "document_id": 401,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Transformed data can be moved in to azure blob storage and then it can be moved in to azure SQL DB, instead of moving directly from databricks to Azure SQL DB.",
        "question": "What are the advantages of using Azure Blob Storage in the data pipeline?",
        "section": "Project",
        "document_id": 401,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Transformed data can be moved in to azure blob storage and then it can be moved in to azure SQL DB, instead of moving directly from databricks to Azure SQL DB.",
        "question": "How does the process of moving data from Azure Blob Storage to Azure SQL DB work?",
        "section": "Project",
        "document_id": 401,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\nDetailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\nSource code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py",
        "question": "What access does the trial dbt account provide?",
        "section": "Project",
        "document_id": 402,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\nDetailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\nSource code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py",
        "question": "What is required to run a job in Airflow using the dbt API?",
        "section": "Project",
        "document_id": 402,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\nDetailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\nSource code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py",
        "question": "Why is it important to be careful not to commit the API key to Github?",
        "section": "Project",
        "document_id": 402,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\nDetailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\nSource code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py",
        "question": "Where can detailed information about dbt and Airflow be found?",
        "section": "Project",
        "document_id": 402,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\nDetailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\nSource code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py",
        "question": "What is the purpose of the provided source code example?",
        "section": "Project",
        "document_id": 402,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html\nhttps://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html\nGive the following roles to you service account:\nDataProc Administrator\nService Account User (explanation here)\nUse DataprocSubmitPySparkJobOperator, DataprocDeleteClusterOperator and  DataprocCreateClusterOperator.\nWhen using  DataprocSubmitPySparkJobOperator, do not forget to add:\ndataproc_jars = [\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar\"]\nBecause DataProc does not already have the BigQuery Connector.",
        "question": "What roles should be assigned to the service account for using Google Cloud Dataproc?",
        "section": "Project",
        "document_id": 403,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html\nhttps://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html\nGive the following roles to you service account:\nDataProc Administrator\nService Account User (explanation here)\nUse DataprocSubmitPySparkJobOperator, DataprocDeleteClusterOperator and  DataprocCreateClusterOperator.\nWhen using  DataprocSubmitPySparkJobOperator, do not forget to add:\ndataproc_jars = [\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar\"]\nBecause DataProc does not already have the BigQuery Connector.",
        "question": "Which operators are recommended for use with Google Cloud Dataproc in Apache Airflow?",
        "section": "Project",
        "document_id": 403,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html\nhttps://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html\nGive the following roles to you service account:\nDataProc Administrator\nService Account User (explanation here)\nUse DataprocSubmitPySparkJobOperator, DataprocDeleteClusterOperator and  DataprocCreateClusterOperator.\nWhen using  DataprocSubmitPySparkJobOperator, do not forget to add:\ndataproc_jars = [\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar\"]\nBecause DataProc does not already have the BigQuery Connector.",
        "question": "What is the purpose of the dataproc_jars parameter in the DataprocSubmitPySparkJobOperator?",
        "section": "Project",
        "document_id": 403,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html\nhttps://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html\nGive the following roles to you service account:\nDataProc Administrator\nService Account User (explanation here)\nUse DataprocSubmitPySparkJobOperator, DataprocDeleteClusterOperator and  DataprocCreateClusterOperator.\nWhen using  DataprocSubmitPySparkJobOperator, do not forget to add:\ndataproc_jars = [\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar\"]\nBecause DataProc does not already have the BigQuery Connector.",
        "question": "Why is the BigQuery Connector not included with Google Cloud Dataproc by default?",
        "section": "Project",
        "document_id": 403,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html\nhttps://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html\nGive the following roles to you service account:\nDataProc Administrator\nService Account User (explanation here)\nUse DataprocSubmitPySparkJobOperator, DataprocDeleteClusterOperator and  DataprocCreateClusterOperator.\nWhen using  DataprocSubmitPySparkJobOperator, do not forget to add:\ndataproc_jars = [\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar\"]\nBecause DataProc does not already have the BigQuery Connector.",
        "question": "What is the importance of the Dataproc Administrator and Service Account User roles in relation to Apache Airflow?",
        "section": "Project",
        "document_id": 403,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can trigger your dbt job in Mage pipeline. For this get your dbt cloud api key under settings/Api tokens/personal tokens. Add it safely to  your .env\nFor example\ndbt_api_trigger=dbt_**\nNavigate to job page and find api trigger  link\nThen create a custom mage Python block with a simple http request like here\nfrom dotenv import load_dotenv\nfrom pathlib import Path\ndotenv_path = Path('/home/src/.env')\nload_dotenv(dotenv_path=dotenv_path)\ndbt_api_trigger= os.getenv(dbt_api_trigger)\nurl = f\"https://cloud.getdbt.com/api/v2/accounts/{dbt_account_id}/jobs/<job_id>/run/\"\nheaders = {\n        \"Authorization\": f\"Token {dbt_api_trigger}\",\n        \"Content-Type\": \"application/json\" }\nbody = {\n        \"cause\": \"Triggered via API\"\n    }\n    response = requests.post(url, headers=headers, json=body)\nvoila! You triggered dbt job form your mage pipeline.",
        "question": "What is the first step to trigger a dbt job in a Mage pipeline?",
        "section": "Project",
        "document_id": 404,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can trigger your dbt job in Mage pipeline. For this get your dbt cloud api key under settings/Api tokens/personal tokens. Add it safely to  your .env\nFor example\ndbt_api_trigger=dbt_**\nNavigate to job page and find api trigger  link\nThen create a custom mage Python block with a simple http request like here\nfrom dotenv import load_dotenv\nfrom pathlib import Path\ndotenv_path = Path('/home/src/.env')\nload_dotenv(dotenv_path=dotenv_path)\ndbt_api_trigger= os.getenv(dbt_api_trigger)\nurl = f\"https://cloud.getdbt.com/api/v2/accounts/{dbt_account_id}/jobs/<job_id>/run/\"\nheaders = {\n        \"Authorization\": f\"Token {dbt_api_trigger}\",\n        \"Content-Type\": \"application/json\" }\nbody = {\n        \"cause\": \"Triggered via API\"\n    }\n    response = requests.post(url, headers=headers, json=body)\nvoila! You triggered dbt job form your mage pipeline.",
        "question": "Where can you find your dbt cloud API key?",
        "section": "Project",
        "document_id": 404,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can trigger your dbt job in Mage pipeline. For this get your dbt cloud api key under settings/Api tokens/personal tokens. Add it safely to  your .env\nFor example\ndbt_api_trigger=dbt_**\nNavigate to job page and find api trigger  link\nThen create a custom mage Python block with a simple http request like here\nfrom dotenv import load_dotenv\nfrom pathlib import Path\ndotenv_path = Path('/home/src/.env')\nload_dotenv(dotenv_path=dotenv_path)\ndbt_api_trigger= os.getenv(dbt_api_trigger)\nurl = f\"https://cloud.getdbt.com/api/v2/accounts/{dbt_account_id}/jobs/<job_id>/run/\"\nheaders = {\n        \"Authorization\": f\"Token {dbt_api_trigger}\",\n        \"Content-Type\": \"application/json\" }\nbody = {\n        \"cause\": \"Triggered via API\"\n    }\n    response = requests.post(url, headers=headers, json=body)\nvoila! You triggered dbt job form your mage pipeline.",
        "question": "What Python library is used to load environment variables in the provided code?",
        "section": "Project",
        "document_id": 404,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can trigger your dbt job in Mage pipeline. For this get your dbt cloud api key under settings/Api tokens/personal tokens. Add it safely to  your .env\nFor example\ndbt_api_trigger=dbt_**\nNavigate to job page and find api trigger  link\nThen create a custom mage Python block with a simple http request like here\nfrom dotenv import load_dotenv\nfrom pathlib import Path\ndotenv_path = Path('/home/src/.env')\nload_dotenv(dotenv_path=dotenv_path)\ndbt_api_trigger= os.getenv(dbt_api_trigger)\nurl = f\"https://cloud.getdbt.com/api/v2/accounts/{dbt_account_id}/jobs/<job_id>/run/\"\nheaders = {\n        \"Authorization\": f\"Token {dbt_api_trigger}\",\n        \"Content-Type\": \"application/json\" }\nbody = {\n        \"cause\": \"Triggered via API\"\n    }\n    response = requests.post(url, headers=headers, json=body)\nvoila! You triggered dbt job form your mage pipeline.",
        "question": "What is the purpose of the 'body' dictionary in the HTTP request?",
        "section": "Project",
        "document_id": 404,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can trigger your dbt job in Mage pipeline. For this get your dbt cloud api key under settings/Api tokens/personal tokens. Add it safely to  your .env\nFor example\ndbt_api_trigger=dbt_**\nNavigate to job page and find api trigger  link\nThen create a custom mage Python block with a simple http request like here\nfrom dotenv import load_dotenv\nfrom pathlib import Path\ndotenv_path = Path('/home/src/.env')\nload_dotenv(dotenv_path=dotenv_path)\ndbt_api_trigger= os.getenv(dbt_api_trigger)\nurl = f\"https://cloud.getdbt.com/api/v2/accounts/{dbt_account_id}/jobs/<job_id>/run/\"\nheaders = {\n        \"Authorization\": f\"Token {dbt_api_trigger}\",\n        \"Content-Type\": \"application/json\" }\nbody = {\n        \"cause\": \"Triggered via API\"\n    }\n    response = requests.post(url, headers=headers, json=body)\nvoila! You triggered dbt job form your mage pipeline.",
        "question": "How do you include the API token in the HTTP headers for the request?",
        "section": "Project",
        "document_id": 404,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The slack thread : thttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1677678161866999\nThe question is that sometimes even if you take plenty of effort to document every single step, and we can't even sure if the person doing the peer review will be able to follow-up, so how this criteria will be evaluated?\nAlex clarifies: “Ideally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions and so on - then it's already great”",
        "question": "What is the main concern expressed in the slack thread regarding documentation?",
        "section": "Project",
        "document_id": 405,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The slack thread : thttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1677678161866999\nThe question is that sometimes even if you take plenty of effort to document every single step, and we can't even sure if the person doing the peer review will be able to follow-up, so how this criteria will be evaluated?\nAlex clarifies: “Ideally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions and so on - then it's already great”",
        "question": "How does Alex suggest handling peer reviews when time is limited?",
        "section": "Project",
        "document_id": 405,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The slack thread : thttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1677678161866999\nThe question is that sometimes even if you take plenty of effort to document every single step, and we can't even sure if the person doing the peer review will be able to follow-up, so how this criteria will be evaluated?\nAlex clarifies: “Ideally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions and so on - then it's already great”",
        "question": "What criteria is being evaluated during the peer review process?",
        "section": "Project",
        "document_id": 405,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The slack thread : thttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1677678161866999\nThe question is that sometimes even if you take plenty of effort to document every single step, and we can't even sure if the person doing the peer review will be able to follow-up, so how this criteria will be evaluated?\nAlex clarifies: “Ideally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions and so on - then it's already great”",
        "question": "What should reviewers look for when they check the code without re-running everything?",
        "section": "Project",
        "document_id": 405,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The slack thread : thttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1677678161866999\nThe question is that sometimes even if you take plenty of effort to document every single step, and we can't even sure if the person doing the peer review will be able to follow-up, so how this criteria will be evaluated?\nAlex clarifies: “Ideally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions and so on - then it's already great”",
        "question": "What does Alex imply about the ideal process for code peer reviews?",
        "section": "Project",
        "document_id": 405,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The key valut in Azure cloud is used to store credentials or passwords or secrets of different tech stack used in Azure. For example if u do not want to expose the password in SQL database, then we can save the password under a given name and use them in other Azure stack.",
        "question": "What is the purpose of the key vault in Azure cloud?",
        "section": "Project",
        "document_id": 406,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The key valut in Azure cloud is used to store credentials or passwords or secrets of different tech stack used in Azure. For example if u do not want to expose the password in SQL database, then we can save the password under a given name and use them in other Azure stack.",
        "question": "How does the key vault help in managing credentials?",
        "section": "Project",
        "document_id": 406,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The key valut in Azure cloud is used to store credentials or passwords or secrets of different tech stack used in Azure. For example if u do not want to expose the password in SQL database, then we can save the password under a given name and use them in other Azure stack.",
        "question": "Can you give an example of how to use the key vault with a SQL database?",
        "section": "Project",
        "document_id": 406,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The key valut in Azure cloud is used to store credentials or passwords or secrets of different tech stack used in Azure. For example if u do not want to expose the password in SQL database, then we can save the password under a given name and use them in other Azure stack.",
        "question": "What types of secrets can be stored in Azure's key vault?",
        "section": "Project",
        "document_id": 406,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The key valut in Azure cloud is used to store credentials or passwords or secrets of different tech stack used in Azure. For example if u do not want to expose the password in SQL database, then we can save the password under a given name and use them in other Azure stack.",
        "question": "Why is it important to avoid exposing passwords in the Azure stack?",
        "section": "Project",
        "document_id": 406,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can get the version of py4j from inside docker using this command\ndocker exec -it --user airflow airflow-airflow-scheduler-1 bash -c \"ls /opt/spark/python/lib\"",
        "question": "What command can be used to get the version of py4j inside Docker?",
        "section": "Project",
        "document_id": 407,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can get the version of py4j from inside docker using this command\ndocker exec -it --user airflow airflow-airflow-scheduler-1 bash -c \"ls /opt/spark/python/lib\"",
        "question": "Which user is specified in the Docker command to execute the command?",
        "section": "Project",
        "document_id": 407,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can get the version of py4j from inside docker using this command\ndocker exec -it --user airflow airflow-airflow-scheduler-1 bash -c \"ls /opt/spark/python/lib\"",
        "question": "What is the name of the Docker container used in the command?",
        "section": "Project",
        "document_id": 407,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can get the version of py4j from inside docker using this command\ndocker exec -it --user airflow airflow-airflow-scheduler-1 bash -c \"ls /opt/spark/python/lib\"",
        "question": "What is the directory being listed to find the py4j version?",
        "section": "Project",
        "document_id": 407,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "You can get the version of py4j from inside docker using this command\ndocker exec -it --user airflow airflow-airflow-scheduler-1 bash -c \"ls /opt/spark/python/lib\"",
        "question": "What does the 'bash -c' option do in the Docker command?",
        "section": "Project",
        "document_id": 407,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Either use conda or pip for managing venv, using both of them together will cause incompatibility.\nIf you’re using conda, install psycopg2 using the conda-forge channel, which may handle the architecture compatibility automatically\nconda install -c conda-forge psycopg2\nIf pip, do the normal install\npip install psycopg2",
        "question": "What is the consequence of using both conda and pip together for managing venv?",
        "section": "Project",
        "document_id": 408,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Either use conda or pip for managing venv, using both of them together will cause incompatibility.\nIf you’re using conda, install psycopg2 using the conda-forge channel, which may handle the architecture compatibility automatically\nconda install -c conda-forge psycopg2\nIf pip, do the normal install\npip install psycopg2",
        "question": "How can you install psycopg2 using conda?",
        "section": "Project",
        "document_id": 408,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Either use conda or pip for managing venv, using both of them together will cause incompatibility.\nIf you’re using conda, install psycopg2 using the conda-forge channel, which may handle the architecture compatibility automatically\nconda install -c conda-forge psycopg2\nIf pip, do the normal install\npip install psycopg2",
        "question": "What channel should be used with conda to install psycopg2?",
        "section": "Project",
        "document_id": 408,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Either use conda or pip for managing venv, using both of them together will cause incompatibility.\nIf you’re using conda, install psycopg2 using the conda-forge channel, which may handle the architecture compatibility automatically\nconda install -c conda-forge psycopg2\nIf pip, do the normal install\npip install psycopg2",
        "question": "What is the command to install psycopg2 using pip?",
        "section": "Project",
        "document_id": 408,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Either use conda or pip for managing venv, using both of them together will cause incompatibility.\nIf you’re using conda, install psycopg2 using the conda-forge channel, which may handle the architecture compatibility automatically\nconda install -c conda-forge psycopg2\nIf pip, do the normal install\npip install psycopg2",
        "question": "Why might conda-forge be preferred for installing psycopg2 with conda?",
        "section": "Project",
        "document_id": 408,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is not a FAQ but more of an advice if you want to set up dbt locally, I did it in the following way:\nI had the postgres instance from week 2 (year 2024) up (the docker-compose)\nmkdir dbt\nvi dbt/profiles.yml\nAnd here I attached this content (only the required fields) and replaced them with the proper values (for instance mine where in the .env file of the folder of week 2 docker stuff)\ncd dbt && git clone https://github.com/dbt-labs/dbt-starter-project\nmkdir project && cd project && mv dbt-starter-project/* .\nMake sure that you align the profile name in profiles.yml with the dbt_project.yml file\nAdd this line anywhere on the dbt_project.yml file:\nconfig-version: 2\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres ls\nIf you have trouble run\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres debug",
        "question": "What is the purpose of the profiles.yml file in setting up dbt locally?",
        "section": "Project",
        "document_id": 409,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is not a FAQ but more of an advice if you want to set up dbt locally, I did it in the following way:\nI had the postgres instance from week 2 (year 2024) up (the docker-compose)\nmkdir dbt\nvi dbt/profiles.yml\nAnd here I attached this content (only the required fields) and replaced them with the proper values (for instance mine where in the .env file of the folder of week 2 docker stuff)\ncd dbt && git clone https://github.com/dbt-labs/dbt-starter-project\nmkdir project && cd project && mv dbt-starter-project/* .\nMake sure that you align the profile name in profiles.yml with the dbt_project.yml file\nAdd this line anywhere on the dbt_project.yml file:\nconfig-version: 2\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres ls\nIf you have trouble run\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres debug",
        "question": "What command is used to clone the dbt starter project?",
        "section": "Project",
        "document_id": 409,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is not a FAQ but more of an advice if you want to set up dbt locally, I did it in the following way:\nI had the postgres instance from week 2 (year 2024) up (the docker-compose)\nmkdir dbt\nvi dbt/profiles.yml\nAnd here I attached this content (only the required fields) and replaced them with the proper values (for instance mine where in the .env file of the folder of week 2 docker stuff)\ncd dbt && git clone https://github.com/dbt-labs/dbt-starter-project\nmkdir project && cd project && mv dbt-starter-project/* .\nMake sure that you align the profile name in profiles.yml with the dbt_project.yml file\nAdd this line anywhere on the dbt_project.yml file:\nconfig-version: 2\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres ls\nIf you have trouble run\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres debug",
        "question": "How do you configure the dbt_project.yml file to align with profiles.yml?",
        "section": "Project",
        "document_id": 409,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is not a FAQ but more of an advice if you want to set up dbt locally, I did it in the following way:\nI had the postgres instance from week 2 (year 2024) up (the docker-compose)\nmkdir dbt\nvi dbt/profiles.yml\nAnd here I attached this content (only the required fields) and replaced them with the proper values (for instance mine where in the .env file of the folder of week 2 docker stuff)\ncd dbt && git clone https://github.com/dbt-labs/dbt-starter-project\nmkdir project && cd project && mv dbt-starter-project/* .\nMake sure that you align the profile name in profiles.yml with the dbt_project.yml file\nAdd this line anywhere on the dbt_project.yml file:\nconfig-version: 2\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres ls\nIf you have trouble run\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres debug",
        "question": "What Docker command is provided for testing the dbt setup?",
        "section": "Project",
        "document_id": 409,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "This is not a FAQ but more of an advice if you want to set up dbt locally, I did it in the following way:\nI had the postgres instance from week 2 (year 2024) up (the docker-compose)\nmkdir dbt\nvi dbt/profiles.yml\nAnd here I attached this content (only the required fields) and replaced them with the proper values (for instance mine where in the .env file of the folder of week 2 docker stuff)\ncd dbt && git clone https://github.com/dbt-labs/dbt-starter-project\nmkdir project && cd project && mv dbt-starter-project/* .\nMake sure that you align the profile name in profiles.yml with the dbt_project.yml file\nAdd this line anywhere on the dbt_project.yml file:\nconfig-version: 2\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres ls\nIf you have trouble run\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres debug",
        "question": "What should be done if there are troubles running the Docker command for dbt?",
        "section": "Project",
        "document_id": 409,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The following line should be included in pyspark configuration\n# Example initialization of SparkSession variable\nspark = (SparkSession.builder\n.master(...)\n.appName(...)\n# Add the following configuration\n.config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-3.5-bigquery:0.37.0\")\n)",
        "question": "What line should be included in the PySpark configuration?",
        "section": "Project",
        "document_id": 410,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The following line should be included in pyspark configuration\n# Example initialization of SparkSession variable\nspark = (SparkSession.builder\n.master(...)\n.appName(...)\n# Add the following configuration\n.config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-3.5-bigquery:0.37.0\")\n)",
        "question": "How is the SparkSession variable initialized in the provided code?",
        "section": "Project",
        "document_id": 410,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The following line should be included in pyspark configuration\n# Example initialization of SparkSession variable\nspark = (SparkSession.builder\n.master(...)\n.appName(...)\n# Add the following configuration\n.config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-3.5-bigquery:0.37.0\")\n)",
        "question": "What is the purpose of the '.config' method in the SparkSession initialization?",
        "section": "Project",
        "document_id": 410,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The following line should be included in pyspark configuration\n# Example initialization of SparkSession variable\nspark = (SparkSession.builder\n.master(...)\n.appName(...)\n# Add the following configuration\n.config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-3.5-bigquery:0.37.0\")\n)",
        "question": "Which package is specified in the configuration for Spark to use BigQuery?",
        "section": "Project",
        "document_id": 410,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The following line should be included in pyspark configuration\n# Example initialization of SparkSession variable\nspark = (SparkSession.builder\n.master(...)\n.appName(...)\n# Add the following configuration\n.config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-3.5-bigquery:0.37.0\")\n)",
        "question": "What version of the BigQuery connector is mentioned in the configuration?",
        "section": "Project",
        "document_id": 410,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Install the astronomer-cosmos package as a dependency. (see Terraform example).\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\nYour dbt lineage graph should now appear as tasks inside a task group like this:",
        "question": "What package must be installed as a dependency for the project?",
        "section": "Course Management Form for Homeworks",
        "document_id": 411,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Install the astronomer-cosmos package as a dependency. (see Terraform example).\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\nYour dbt lineage graph should now appear as tasks inside a task group like this:",
        "question": "Where should the dbt-core project be copied within the Composer GCP bucket?",
        "section": "Course Management Form for Homeworks",
        "document_id": 411,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Install the astronomer-cosmos package as a dependency. (see Terraform example).\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\nYour dbt lineage graph should now appear as tasks inside a task group like this:",
        "question": "How should the profiles.yml file be configured for authentication?",
        "section": "Course Management Form for Homeworks",
        "document_id": 411,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Install the astronomer-cosmos package as a dependency. (see Terraform example).\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\nYour dbt lineage graph should now appear as tasks inside a task group like this:",
        "question": "What class should be used to create a new DAG in this context?",
        "section": "Course Management Form for Homeworks",
        "document_id": 411,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Install the astronomer-cosmos package as a dependency. (see Terraform example).\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\nYour dbt lineage graph should now appear as tasks inside a task group like this:",
        "question": "What should the profiles_yml_filepath point to in the ProfileConfig?",
        "section": "Course Management Form for Homeworks",
        "document_id": 411,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname, or your real name, if you prefer. Your entry on the Leaderboard is the one highlighted in teal(?) / light green (?).\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.\nQuestion: Is it possible to create external tables in BigQuery using URLs, such as those from the NY Taxi data website?\nAnswer: Not really, only Bigtable, Cloud Storage, and Google Drive are supported data stores.",
        "question": "What does the display name on the leaderboard represent?",
        "section": "Workshop 1 - dlthub",
        "document_id": 412,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname, or your real name, if you prefer. Your entry on the Leaderboard is the one highlighted in teal(?) / light green (?).\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.\nQuestion: Is it possible to create external tables in BigQuery using URLs, such as those from the NY Taxi data website?\nAnswer: Not really, only Bigtable, Cloud Storage, and Google Drive are supported data stores.",
        "question": "Can users customize their display names on the leaderboard?",
        "section": "Workshop 1 - dlthub",
        "document_id": 412,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname, or your real name, if you prefer. Your entry on the Leaderboard is the one highlighted in teal(?) / light green (?).\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.\nQuestion: Is it possible to create external tables in BigQuery using URLs, such as those from the NY Taxi data website?\nAnswer: Not really, only Bigtable, Cloud Storage, and Google Drive are supported data stores.",
        "question": "What should the Certificate name reflect?",
        "section": "Workshop 1 - dlthub",
        "document_id": 412,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname, or your real name, if you prefer. Your entry on the Leaderboard is the one highlighted in teal(?) / light green (?).\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.\nQuestion: Is it possible to create external tables in BigQuery using URLs, such as those from the NY Taxi data website?\nAnswer: Not really, only Bigtable, Cloud Storage, and Google Drive are supported data stores.",
        "question": "What does the 'Display on Leaderboard' option control?",
        "section": "Workshop 1 - dlthub",
        "document_id": 412,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname, or your real name, if you prefer. Your entry on the Leaderboard is the one highlighted in teal(?) / light green (?).\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.\nQuestion: Is it possible to create external tables in BigQuery using URLs, such as those from the NY Taxi data website?\nAnswer: Not really, only Bigtable, Cloud Storage, and Google Drive are supported data stores.",
        "question": "What color highlights an entry on the Leaderboard?",
        "section": "Workshop 1 - dlthub",
        "document_id": 412,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).",
        "question": "What package needs to be installed to run the provided code?",
        "section": "Workshop 1 - dlthub",
        "document_id": 413,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).",
        "question": "What command should be executed to install the 'dlt[duckdb]' package?",
        "section": "Workshop 1 - dlthub",
        "document_id": 413,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).",
        "question": "Is it necessary to install the duckdb pip package separately?",
        "section": "Workshop 1 - dlthub",
        "document_id": 413,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).",
        "question": "What should you ensure before loading the duckdb package?",
        "section": "Workshop 1 - dlthub",
        "document_id": 413,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).",
        "question": "What environment is suggested for running the installation command?",
        "section": "Workshop 1 - dlthub",
        "document_id": 413,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are running Jupyter Notebook on a fresh new Codespace or in local machine with a new Virtual Environment, you will need this package to run the starter Jupyter Notebook offered by the teacher. Execute this:\npip install jupyter",
        "question": "What command do you need to execute to install Jupyter on a fresh Codespace?",
        "section": "Workshop 1 - dlthub",
        "document_id": 414,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are running Jupyter Notebook on a fresh new Codespace or in local machine with a new Virtual Environment, you will need this package to run the starter Jupyter Notebook offered by the teacher. Execute this:\npip install jupyter",
        "question": "Why might you need to install the Jupyter package?",
        "section": "Workshop 1 - dlthub",
        "document_id": 414,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are running Jupyter Notebook on a fresh new Codespace or in local machine with a new Virtual Environment, you will need this package to run the starter Jupyter Notebook offered by the teacher. Execute this:\npip install jupyter",
        "question": "On what environments might you need to install Jupyter according to the text?",
        "section": "Workshop 1 - dlthub",
        "document_id": 414,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are running Jupyter Notebook on a fresh new Codespace or in local machine with a new Virtual Environment, you will need this package to run the starter Jupyter Notebook offered by the teacher. Execute this:\npip install jupyter",
        "question": "Who offers the starter Jupyter Notebook mentioned in the text?",
        "section": "Workshop 1 - dlthub",
        "document_id": 414,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you are running Jupyter Notebook on a fresh new Codespace or in local machine with a new Virtual Environment, you will need this package to run the starter Jupyter Notebook offered by the teacher. Execute this:\npip install jupyter",
        "question": "What is the purpose of the pip install command in the context of Jupyter Notebook?",
        "section": "Workshop 1 - dlthub",
        "document_id": 414,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Alternatively, you can switch to in-file storage with:",
        "question": "What is the alternative storage method mentioned?",
        "section": "Workshop 1 - dlthub",
        "document_id": 415,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Alternatively, you can switch to in-file storage with:",
        "question": "How can you switch to in-file storage?",
        "section": "Workshop 1 - dlthub",
        "document_id": 415,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Alternatively, you can switch to in-file storage with:",
        "question": "What does the term 'in-file storage' refer to?",
        "section": "Workshop 1 - dlthub",
        "document_id": 415,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Alternatively, you can switch to in-file storage with:",
        "question": "Are there any prerequisites for switching to in-file storage?",
        "section": "Workshop 1 - dlthub",
        "document_id": 415,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Alternatively, you can switch to in-file storage with:",
        "question": "What are the potential benefits of using in-file storage?",
        "section": "Workshop 1 - dlthub",
        "document_id": 415,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check the contents of the repository with ls - the command.sh file should be in the root folder\nIf it is not, verify that you had cloned the correct repository - https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04",
        "question": "What command should you use to check the contents of the repository?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 417,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check the contents of the repository with ls - the command.sh file should be in the root folder\nIf it is not, verify that you had cloned the correct repository - https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04",
        "question": "Where should the command.sh file be located?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 417,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check the contents of the repository with ls - the command.sh file should be in the root folder\nIf it is not, verify that you had cloned the correct repository - https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04",
        "question": "What should you do if the command.sh file is not in the root folder?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 417,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check the contents of the repository with ls - the command.sh file should be in the root folder\nIf it is not, verify that you had cloned the correct repository - https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04",
        "question": "What is the URL of the repository that needs to be cloned?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 417,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Check the contents of the repository with ls - the command.sh file should be in the root folder\nIf it is not, verify that you had cloned the correct repository - https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04",
        "question": "What might be a reason for not finding the command.sh file in the correct location?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 417,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "psql is a command line tool that is installed alongside PostgreSQL DB, but since we've always been running PostgreSQL in a container, you've only got `pgcli`, which lacks the feature to run a sql script into the DB. Besides, having a command line for each database flavor you'll have to deal with as a Data Professional is far from ideal.\nSo, instead, you can use usql. Check the docs for details on how to install for your OS. On macOS, it supports `homebrew`, and on Windows, it supports scoop.\nSo, to run the taxi_trips.sql script with usql:",
        "question": "What is psql and how is it typically used?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 418,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "psql is a command line tool that is installed alongside PostgreSQL DB, but since we've always been running PostgreSQL in a container, you've only got `pgcli`, which lacks the feature to run a sql script into the DB. Besides, having a command line for each database flavor you'll have to deal with as a Data Professional is far from ideal.\nSo, instead, you can use usql. Check the docs for details on how to install for your OS. On macOS, it supports `homebrew`, and on Windows, it supports scoop.\nSo, to run the taxi_trips.sql script with usql:",
        "question": "Why might someone prefer usql over pgcli?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 418,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "psql is a command line tool that is installed alongside PostgreSQL DB, but since we've always been running PostgreSQL in a container, you've only got `pgcli`, which lacks the feature to run a sql script into the DB. Besides, having a command line for each database flavor you'll have to deal with as a Data Professional is far from ideal.\nSo, instead, you can use usql. Check the docs for details on how to install for your OS. On macOS, it supports `homebrew`, and on Windows, it supports scoop.\nSo, to run the taxi_trips.sql script with usql:",
        "question": "What installation methods are mentioned for usql on macOS and Windows?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 418,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "psql is a command line tool that is installed alongside PostgreSQL DB, but since we've always been running PostgreSQL in a container, you've only got `pgcli`, which lacks the feature to run a sql script into the DB. Besides, having a command line for each database flavor you'll have to deal with as a Data Professional is far from ideal.\nSo, instead, you can use usql. Check the docs for details on how to install for your OS. On macOS, it supports `homebrew`, and on Windows, it supports scoop.\nSo, to run the taxi_trips.sql script with usql:",
        "question": "What is the main limitation of pgcli mentioned in the text?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 418,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "psql is a command line tool that is installed alongside PostgreSQL DB, but since we've always been running PostgreSQL in a container, you've only got `pgcli`, which lacks the feature to run a sql script into the DB. Besides, having a command line for each database flavor you'll have to deal with as a Data Professional is far from ideal.\nSo, instead, you can use usql. Check the docs for details on how to install for your OS. On macOS, it supports `homebrew`, and on Windows, it supports scoop.\nSo, to run the taxi_trips.sql script with usql:",
        "question": "How do you run a SQL script using usql?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 418,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you encounter this error and are certain that you have docker compose installed, but typically run it as docker compose without the hyphen, then consider editing command.sh file by removing the hyphen from ‘docker-compose’. Example:\nstart-cluster() {\ndocker compose -f docker/docker-compose.yml up -d\n}",
        "question": "What should you do if you encounter an error while using docker compose?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 419,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you encounter this error and are certain that you have docker compose installed, but typically run it as docker compose without the hyphen, then consider editing command.sh file by removing the hyphen from ‘docker-compose’. Example:\nstart-cluster() {\ndocker compose -f docker/docker-compose.yml up -d\n}",
        "question": "How is docker compose typically run according to the text?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 419,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you encounter this error and are certain that you have docker compose installed, but typically run it as docker compose without the hyphen, then consider editing command.sh file by removing the hyphen from ‘docker-compose’. Example:\nstart-cluster() {\ndocker compose -f docker/docker-compose.yml up -d\n}",
        "question": "What specific change is suggested for the command.sh file?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 419,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you encounter this error and are certain that you have docker compose installed, but typically run it as docker compose without the hyphen, then consider editing command.sh file by removing the hyphen from ‘docker-compose’. Example:\nstart-cluster() {\ndocker compose -f docker/docker-compose.yml up -d\n}",
        "question": "In the example provided, what command is used to start the cluster?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 419,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you encounter this error and are certain that you have docker compose installed, but typically run it as docker compose without the hyphen, then consider editing command.sh file by removing the hyphen from ‘docker-compose’. Example:\nstart-cluster() {\ndocker compose -f docker/docker-compose.yml up -d\n}",
        "question": "What file format is mentioned in the docker compose command?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 419,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ERROR: The Compose file './docker/docker-compose.yml' is invalid because:\nInvalid top-level property \"x-image\". Valid top-level sections for this Compose file are: version, services, networks, volumes, secrets, configs, and extensions starting with \"x-\".\nYou might be seeing this error because you're using the wrong Compose file version. Either specify a supported version (e.g \"2.2\" or \"3.3\") and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.\nFor more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/\nIf you encounter the above error and have docker-compose installed, try updating your version of docker-compose. At the time of reporting this issue (March 17 2024), Ubuntu does not seem to support a docker-compose version high enough to run the required docker images. If you have this error and are on a Ubuntu machine, consider starting a VM with a Debian machine or look for an alternative way to download docker-compose at the latest version on your machine.",
        "question": "What is the main reason for the error in the Docker Compose file?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 420,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ERROR: The Compose file './docker/docker-compose.yml' is invalid because:\nInvalid top-level property \"x-image\". Valid top-level sections for this Compose file are: version, services, networks, volumes, secrets, configs, and extensions starting with \"x-\".\nYou might be seeing this error because you're using the wrong Compose file version. Either specify a supported version (e.g \"2.2\" or \"3.3\") and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.\nFor more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/\nIf you encounter the above error and have docker-compose installed, try updating your version of docker-compose. At the time of reporting this issue (March 17 2024), Ubuntu does not seem to support a docker-compose version high enough to run the required docker images. If you have this error and are on a Ubuntu machine, consider starting a VM with a Debian machine or look for an alternative way to download docker-compose at the latest version on your machine.",
        "question": "Which top-level properties are valid in a Docker Compose file?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 420,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ERROR: The Compose file './docker/docker-compose.yml' is invalid because:\nInvalid top-level property \"x-image\". Valid top-level sections for this Compose file are: version, services, networks, volumes, secrets, configs, and extensions starting with \"x-\".\nYou might be seeing this error because you're using the wrong Compose file version. Either specify a supported version (e.g \"2.2\" or \"3.3\") and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.\nFor more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/\nIf you encounter the above error and have docker-compose installed, try updating your version of docker-compose. At the time of reporting this issue (March 17 2024), Ubuntu does not seem to support a docker-compose version high enough to run the required docker images. If you have this error and are on a Ubuntu machine, consider starting a VM with a Debian machine or look for an alternative way to download docker-compose at the latest version on your machine.",
        "question": "What should you do if you're using the wrong Compose file version?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 420,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ERROR: The Compose file './docker/docker-compose.yml' is invalid because:\nInvalid top-level property \"x-image\". Valid top-level sections for this Compose file are: version, services, networks, volumes, secrets, configs, and extensions starting with \"x-\".\nYou might be seeing this error because you're using the wrong Compose file version. Either specify a supported version (e.g \"2.2\" or \"3.3\") and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.\nFor more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/\nIf you encounter the above error and have docker-compose installed, try updating your version of docker-compose. At the time of reporting this issue (March 17 2024), Ubuntu does not seem to support a docker-compose version high enough to run the required docker images. If you have this error and are on a Ubuntu machine, consider starting a VM with a Debian machine or look for an alternative way to download docker-compose at the latest version on your machine.",
        "question": "What is recommended for Ubuntu users encountering this error with docker-compose?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 420,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "ERROR: The Compose file './docker/docker-compose.yml' is invalid because:\nInvalid top-level property \"x-image\". Valid top-level sections for this Compose file are: version, services, networks, volumes, secrets, configs, and extensions starting with \"x-\".\nYou might be seeing this error because you're using the wrong Compose file version. Either specify a supported version (e.g \"2.2\" or \"3.3\") and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.\nFor more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/\nIf you encounter the above error and have docker-compose installed, try updating your version of docker-compose. At the time of reporting this issue (March 17 2024), Ubuntu does not seem to support a docker-compose version high enough to run the required docker images. If you have this error and are on a Ubuntu machine, consider starting a VM with a Debian machine or look for an alternative way to download docker-compose at the latest version on your machine.",
        "question": "Where can you find more information about the Compose file format versions?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 420,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: [source] Yes, it is so that we can observe the changes as we’re working on the queries in real-time. The script is changing the date timestamp to the current time, so our queries with the now()filter would work. Open another terminal tab to copy+paste the queries while the stream-kafka script is running in the background.\nNoel: I have recently increased this up to 100 at a time, you may pull the latest changes from the repository.",
        "question": "What is the purpose of changing the date timestamp in the script?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 421,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: [source] Yes, it is so that we can observe the changes as we’re working on the queries in real-time. The script is changing the date timestamp to the current time, so our queries with the now()filter would work. Open another terminal tab to copy+paste the queries while the stream-kafka script is running in the background.\nNoel: I have recently increased this up to 100 at a time, you may pull the latest changes from the repository.",
        "question": "Why is it suggested to open another terminal tab while the stream-kafka script is running?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 421,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: [source] Yes, it is so that we can observe the changes as we’re working on the queries in real-time. The script is changing the date timestamp to the current time, so our queries with the now()filter would work. Open another terminal tab to copy+paste the queries while the stream-kafka script is running in the background.\nNoel: I have recently increased this up to 100 at a time, you may pull the latest changes from the repository.",
        "question": "How many queries does Noel mention can be processed at once?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 421,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: [source] Yes, it is so that we can observe the changes as we’re working on the queries in real-time. The script is changing the date timestamp to the current time, so our queries with the now()filter would work. Open another terminal tab to copy+paste the queries while the stream-kafka script is running in the background.\nNoel: I have recently increased this up to 100 at a time, you may pull the latest changes from the repository.",
        "question": "What should you do to observe changes while working on queries?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 421,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: [source] Yes, it is so that we can observe the changes as we’re working on the queries in real-time. The script is changing the date timestamp to the current time, so our queries with the now()filter would work. Open another terminal tab to copy+paste the queries while the stream-kafka script is running in the background.\nNoel: I have recently increased this up to 100 at a time, you may pull the latest changes from the repository.",
        "question": "Where can the latest changes be pulled from according to Noel?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 421,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: No, it is not.",
        "question": "Is it correct to say that the answer is affirmative?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 422,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: No, it is not.",
        "question": "What kind of answer is provided?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 422,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: No, it is not.",
        "question": "Does the text indicate agreement or disagreement?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 422,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: No, it is not.",
        "question": "What is the subject of the answer given?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 422,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: No, it is not.",
        "question": "Can the answer be interpreted as a 'yes' or 'no'?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 422,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: about 7GB free for all the containers to be provisioned and then the psql still needs to run and ingest the taxi data, so maybe 10gb in total?",
        "question": "How much free space is available for the containers?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 423,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: about 7GB free for all the containers to be provisioned and then the psql still needs to run and ingest the taxi data, so maybe 10gb in total?",
        "question": "What is the estimated total space required including psql and taxi data ingestion?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 423,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: about 7GB free for all the containers to be provisioned and then the psql still needs to run and ingest the taxi data, so maybe 10gb in total?",
        "question": "What is the specific role of psql in this context?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 423,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: about 7GB free for all the containers to be provisioned and then the psql still needs to run and ingest the taxi data, so maybe 10gb in total?",
        "question": "How does the free space impact the provisioning of containers?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 423,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans: about 7GB free for all the containers to be provisioned and then the psql still needs to run and ingest the taxi data, so maybe 10gb in total?",
        "question": "What type of data is being ingested by psql?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 423,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Replace psycopg2==2.9.9 with psycopg2-binary in the requirements.txt file [source] [another]\nWhen you open another terminal to run the psql, remember to do the source command.sh step for each terminal session\n---------------------------------------------------------------------------------------------",
        "question": "What should you replace in the requirements.txt file?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 424,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Replace psycopg2==2.9.9 with psycopg2-binary in the requirements.txt file [source] [another]\nWhen you open another terminal to run the psql, remember to do the source command.sh step for each terminal session\n---------------------------------------------------------------------------------------------",
        "question": "What is the version of psycopg2 that needs to be replaced?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 424,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Replace psycopg2==2.9.9 with psycopg2-binary in the requirements.txt file [source] [another]\nWhen you open another terminal to run the psql, remember to do the source command.sh step for each terminal session\n---------------------------------------------------------------------------------------------",
        "question": "What command should be run in each terminal session before starting psql?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 424,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Replace psycopg2==2.9.9 with psycopg2-binary in the requirements.txt file [source] [another]\nWhen you open another terminal to run the psql, remember to do the source command.sh step for each terminal session\n---------------------------------------------------------------------------------------------",
        "question": "Is the source command required for multiple terminal sessions?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 424,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Replace psycopg2==2.9.9 with psycopg2-binary in the requirements.txt file [source] [another]\nWhen you open another terminal to run the psql, remember to do the source command.sh step for each terminal session\n---------------------------------------------------------------------------------------------",
        "question": "What file is being modified to change the database dependency?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 424,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re using an Anaconda installation:\nCd home/\nConda install gcc\nSource back to your RisingWave Venv - source .venv/bin/activate\nPip install psycopg2-binary\nPip install -r requirements.txt\nFor some reason this worked - the Conda base doesn’t have the GCC installed - (GNU Compiler Collection) a compiler system that supports various programming languages. Without this the it fails to install pyproject.toml-based projects\n“It's possible that in your specific environment, the gcc installation was required at the system level rather than within the virtual environment. This can happen if the build process for psycopg2 tries to access system-level dependencies during installation.\nInstalling gcc in your main Python installation (Conda) would make it available system-wide, allowing any Python environment to access it when necessary for building packages.”\ngcc stands for GNU Compiler Collection. It is a compiler system developed by the GNU Project that supports various programming languages, including C, C++, Objective-C, and Fortran.\nGCC is widely used for compiling source code written in these languages into executable programs or libraries. It's a key tool in the software development process, particularly in the compilation stage where source code is translated into machine code that can be executed by a computer's processor.\nIn addition to compiling source code, GCC also provides various optimization options, debugging support, and extensive documentation, making it a powerful and versatile tool for developers across different platforms and architectures.\n—-----------------------------------------------------------------------------------",
        "question": "What is the purpose of installing GCC in an Anaconda installation?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 425,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re using an Anaconda installation:\nCd home/\nConda install gcc\nSource back to your RisingWave Venv - source .venv/bin/activate\nPip install psycopg2-binary\nPip install -r requirements.txt\nFor some reason this worked - the Conda base doesn’t have the GCC installed - (GNU Compiler Collection) a compiler system that supports various programming languages. Without this the it fails to install pyproject.toml-based projects\n“It's possible that in your specific environment, the gcc installation was required at the system level rather than within the virtual environment. This can happen if the build process for psycopg2 tries to access system-level dependencies during installation.\nInstalling gcc in your main Python installation (Conda) would make it available system-wide, allowing any Python environment to access it when necessary for building packages.”\ngcc stands for GNU Compiler Collection. It is a compiler system developed by the GNU Project that supports various programming languages, including C, C++, Objective-C, and Fortran.\nGCC is widely used for compiling source code written in these languages into executable programs or libraries. It's a key tool in the software development process, particularly in the compilation stage where source code is translated into machine code that can be executed by a computer's processor.\nIn addition to compiling source code, GCC also provides various optimization options, debugging support, and extensive documentation, making it a powerful and versatile tool for developers across different platforms and architectures.\n—-----------------------------------------------------------------------------------",
        "question": "Why is it necessary to install GCC at the system level rather than just within the virtual environment?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 425,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re using an Anaconda installation:\nCd home/\nConda install gcc\nSource back to your RisingWave Venv - source .venv/bin/activate\nPip install psycopg2-binary\nPip install -r requirements.txt\nFor some reason this worked - the Conda base doesn’t have the GCC installed - (GNU Compiler Collection) a compiler system that supports various programming languages. Without this the it fails to install pyproject.toml-based projects\n“It's possible that in your specific environment, the gcc installation was required at the system level rather than within the virtual environment. This can happen if the build process for psycopg2 tries to access system-level dependencies during installation.\nInstalling gcc in your main Python installation (Conda) would make it available system-wide, allowing any Python environment to access it when necessary for building packages.”\ngcc stands for GNU Compiler Collection. It is a compiler system developed by the GNU Project that supports various programming languages, including C, C++, Objective-C, and Fortran.\nGCC is widely used for compiling source code written in these languages into executable programs or libraries. It's a key tool in the software development process, particularly in the compilation stage where source code is translated into machine code that can be executed by a computer's processor.\nIn addition to compiling source code, GCC also provides various optimization options, debugging support, and extensive documentation, making it a powerful and versatile tool for developers across different platforms and architectures.\n—-----------------------------------------------------------------------------------",
        "question": "What programming languages does the GNU Compiler Collection (GCC) support?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 425,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re using an Anaconda installation:\nCd home/\nConda install gcc\nSource back to your RisingWave Venv - source .venv/bin/activate\nPip install psycopg2-binary\nPip install -r requirements.txt\nFor some reason this worked - the Conda base doesn’t have the GCC installed - (GNU Compiler Collection) a compiler system that supports various programming languages. Without this the it fails to install pyproject.toml-based projects\n“It's possible that in your specific environment, the gcc installation was required at the system level rather than within the virtual environment. This can happen if the build process for psycopg2 tries to access system-level dependencies during installation.\nInstalling gcc in your main Python installation (Conda) would make it available system-wide, allowing any Python environment to access it when necessary for building packages.”\ngcc stands for GNU Compiler Collection. It is a compiler system developed by the GNU Project that supports various programming languages, including C, C++, Objective-C, and Fortran.\nGCC is widely used for compiling source code written in these languages into executable programs or libraries. It's a key tool in the software development process, particularly in the compilation stage where source code is translated into machine code that can be executed by a computer's processor.\nIn addition to compiling source code, GCC also provides various optimization options, debugging support, and extensive documentation, making it a powerful and versatile tool for developers across different platforms and architectures.\n—-----------------------------------------------------------------------------------",
        "question": "How does GCC contribute to the software development process?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 425,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "If you’re using an Anaconda installation:\nCd home/\nConda install gcc\nSource back to your RisingWave Venv - source .venv/bin/activate\nPip install psycopg2-binary\nPip install -r requirements.txt\nFor some reason this worked - the Conda base doesn’t have the GCC installed - (GNU Compiler Collection) a compiler system that supports various programming languages. Without this the it fails to install pyproject.toml-based projects\n“It's possible that in your specific environment, the gcc installation was required at the system level rather than within the virtual environment. This can happen if the build process for psycopg2 tries to access system-level dependencies during installation.\nInstalling gcc in your main Python installation (Conda) would make it available system-wide, allowing any Python environment to access it when necessary for building packages.”\ngcc stands for GNU Compiler Collection. It is a compiler system developed by the GNU Project that supports various programming languages, including C, C++, Objective-C, and Fortran.\nGCC is widely used for compiling source code written in these languages into executable programs or libraries. It's a key tool in the software development process, particularly in the compilation stage where source code is translated into machine code that can be executed by a computer's processor.\nIn addition to compiling source code, GCC also provides various optimization options, debugging support, and extensive documentation, making it a powerful and versatile tool for developers across different platforms and architectures.\n—-----------------------------------------------------------------------------------",
        "question": "What are some features of GCC that make it a versatile tool for developers?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 425,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\nUse the git bash terminal in windows.\nActivate python venv from git bash: source .venv/Scripts/activate\nModify the seed_kafka.py file: in the first line, replace python3 with python.\nNow from git bash, run the seed-kafka cmd. It should work now.\nAdditional Notes:\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\nThe equivalent of source commands.sh  in Powershell is . .\\commands.sh from the workshop directory.\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\n—--------------------------------------------------------------------------------------",
        "question": "What terminal is recommended for use in Windows when rectifying minor errors?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 426,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\nUse the git bash terminal in windows.\nActivate python venv from git bash: source .venv/Scripts/activate\nModify the seed_kafka.py file: in the first line, replace python3 with python.\nNow from git bash, run the seed-kafka cmd. It should work now.\nAdditional Notes:\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\nThe equivalent of source commands.sh  in Powershell is . .\\commands.sh from the workshop directory.\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\n—--------------------------------------------------------------------------------------",
        "question": "How do you activate a Python virtual environment from the git bash terminal?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 426,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\nUse the git bash terminal in windows.\nActivate python venv from git bash: source .venv/Scripts/activate\nModify the seed_kafka.py file: in the first line, replace python3 with python.\nNow from git bash, run the seed-kafka cmd. It should work now.\nAdditional Notes:\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\nThe equivalent of source commands.sh  in Powershell is . .\\commands.sh from the workshop directory.\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\n—--------------------------------------------------------------------------------------",
        "question": "What modification should be made to the first line of the seed_kafka.py file?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 426,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\nUse the git bash terminal in windows.\nActivate python venv from git bash: source .venv/Scripts/activate\nModify the seed_kafka.py file: in the first line, replace python3 with python.\nNow from git bash, run the seed-kafka cmd. It should work now.\nAdditional Notes:\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\nThe equivalent of source commands.sh  in Powershell is . .\\commands.sh from the workshop directory.\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\n—--------------------------------------------------------------------------------------",
        "question": "What command should be run from git bash to execute the seed-kafka process after making changes?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 426,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\nUse the git bash terminal in windows.\nActivate python venv from git bash: source .venv/Scripts/activate\nModify the seed_kafka.py file: in the first line, replace python3 with python.\nNow from git bash, run the seed-kafka cmd. It should work now.\nAdditional Notes:\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\nThe equivalent of source commands.sh  in Powershell is . .\\commands.sh from the workshop directory.\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\n—--------------------------------------------------------------------------------------",
        "question": "What is the command to connect to the RisingWave cluster from Powershell?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 426,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In case the script gets stuck on\n%3|1709652240.100|FAIL|rdkafka#producer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)gre\nafter trying to load the trip data, check the logs of the message_queue container in docker. If it keeps restarting with Could not initialize seastar: std::runtime_error (insufficient physical memory: needed 4294967296 available 4067422208)  as the last message, then go to the docker-compose file in the docker folder of the project and change the ‘memory’ command for the message_queue service for some lower value.\nSolution: lower the memory allocation of the service “message_queue” in your docker-compose file from 4GB. If you have the “insufficient physical memory” error message (try 3GB)\nIssue: Running psql -f risingwave-sql/table/trip_data.sql after starting services with ‘default’ values using docker-compose up gives the error  “psql:risingwave-sql/table/trip_data.sql:61: ERROR:  syntax error at or near \".\" LINE 60:       properties.bootstrap.server='message_queue:29092'”\nSolution: Make sure you have run source commands.sh in each terminal window",
        "question": "What does the error message 'Connection refused' indicate when connecting to localhost:9092?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 427,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In case the script gets stuck on\n%3|1709652240.100|FAIL|rdkafka#producer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)gre\nafter trying to load the trip data, check the logs of the message_queue container in docker. If it keeps restarting with Could not initialize seastar: std::runtime_error (insufficient physical memory: needed 4294967296 available 4067422208)  as the last message, then go to the docker-compose file in the docker folder of the project and change the ‘memory’ command for the message_queue service for some lower value.\nSolution: lower the memory allocation of the service “message_queue” in your docker-compose file from 4GB. If you have the “insufficient physical memory” error message (try 3GB)\nIssue: Running psql -f risingwave-sql/table/trip_data.sql after starting services with ‘default’ values using docker-compose up gives the error  “psql:risingwave-sql/table/trip_data.sql:61: ERROR:  syntax error at or near \".\" LINE 60:       properties.bootstrap.server='message_queue:29092'”\nSolution: Make sure you have run source commands.sh in each terminal window",
        "question": "How can you resolve the 'insufficient physical memory' error when initializing the message_queue service?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 427,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In case the script gets stuck on\n%3|1709652240.100|FAIL|rdkafka#producer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)gre\nafter trying to load the trip data, check the logs of the message_queue container in docker. If it keeps restarting with Could not initialize seastar: std::runtime_error (insufficient physical memory: needed 4294967296 available 4067422208)  as the last message, then go to the docker-compose file in the docker folder of the project and change the ‘memory’ command for the message_queue service for some lower value.\nSolution: lower the memory allocation of the service “message_queue” in your docker-compose file from 4GB. If you have the “insufficient physical memory” error message (try 3GB)\nIssue: Running psql -f risingwave-sql/table/trip_data.sql after starting services with ‘default’ values using docker-compose up gives the error  “psql:risingwave-sql/table/trip_data.sql:61: ERROR:  syntax error at or near \".\" LINE 60:       properties.bootstrap.server='message_queue:29092'”\nSolution: Make sure you have run source commands.sh in each terminal window",
        "question": "What value should you change the memory allocation to in the docker-compose file if you encounter an insufficient memory error?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 427,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In case the script gets stuck on\n%3|1709652240.100|FAIL|rdkafka#producer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)gre\nafter trying to load the trip data, check the logs of the message_queue container in docker. If it keeps restarting with Could not initialize seastar: std::runtime_error (insufficient physical memory: needed 4294967296 available 4067422208)  as the last message, then go to the docker-compose file in the docker folder of the project and change the ‘memory’ command for the message_queue service for some lower value.\nSolution: lower the memory allocation of the service “message_queue” in your docker-compose file from 4GB. If you have the “insufficient physical memory” error message (try 3GB)\nIssue: Running psql -f risingwave-sql/table/trip_data.sql after starting services with ‘default’ values using docker-compose up gives the error  “psql:risingwave-sql/table/trip_data.sql:61: ERROR:  syntax error at or near \".\" LINE 60:       properties.bootstrap.server='message_queue:29092'”\nSolution: Make sure you have run source commands.sh in each terminal window",
        "question": "What command should be run in each terminal window to avoid the syntax error when running psql with trip_data.sql?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 427,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "In case the script gets stuck on\n%3|1709652240.100|FAIL|rdkafka#producer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)gre\nafter trying to load the trip data, check the logs of the message_queue container in docker. If it keeps restarting with Could not initialize seastar: std::runtime_error (insufficient physical memory: needed 4294967296 available 4067422208)  as the last message, then go to the docker-compose file in the docker folder of the project and change the ‘memory’ command for the message_queue service for some lower value.\nSolution: lower the memory allocation of the service “message_queue” in your docker-compose file from 4GB. If you have the “insufficient physical memory” error message (try 3GB)\nIssue: Running psql -f risingwave-sql/table/trip_data.sql after starting services with ‘default’ values using docker-compose up gives the error  “psql:risingwave-sql/table/trip_data.sql:61: ERROR:  syntax error at or near \".\" LINE 60:       properties.bootstrap.server='message_queue:29092'”\nSolution: Make sure you have run source commands.sh in each terminal window",
        "question": "What is the significance of the error message related to properties.bootstrap.server in the context of psql?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 427,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use seed-kafka instead of stream-kafka to get a static set of results.",
        "question": "What is the main advantage of using seed-kafka over stream-kafka?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 428,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use seed-kafka instead of stream-kafka to get a static set of results.",
        "question": "What will users achieve by using seed-kafka?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 428,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use seed-kafka instead of stream-kafka to get a static set of results.",
        "question": "Can seed-kafka be used for dynamic results?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 428,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use seed-kafka instead of stream-kafka to get a static set of results.",
        "question": "What type of results does seed-kafka provide?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 428,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Use seed-kafka instead of stream-kafka to get a static set of results.",
        "question": "Is stream-kafka suitable for obtaining a static set of results?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 428,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is best to use the order by and limit clause on the query to the materialized view instead of the materialized view creation in order to guarantee consistent results\nHomework - The answers in the homework do not match the provided options: You must follow the following steps: 1. clean-cluster 2. docker volume prune and use seed-kafka instead of stream-kafka. Ensure that the number of records is 100K.",
        "question": "What is the recommended method for ensuring consistent results when querying a materialized view?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 429,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is best to use the order by and limit clause on the query to the materialized view instead of the materialized view creation in order to guarantee consistent results\nHomework - The answers in the homework do not match the provided options: You must follow the following steps: 1. clean-cluster 2. docker volume prune and use seed-kafka instead of stream-kafka. Ensure that the number of records is 100K.",
        "question": "What steps should be followed if the homework answers do not match the provided options?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 429,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is best to use the order by and limit clause on the query to the materialized view instead of the materialized view creation in order to guarantee consistent results\nHomework - The answers in the homework do not match the provided options: You must follow the following steps: 1. clean-cluster 2. docker volume prune and use seed-kafka instead of stream-kafka. Ensure that the number of records is 100K.",
        "question": "What command is suggested to clean the cluster in the homework instructions?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 429,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is best to use the order by and limit clause on the query to the materialized view instead of the materialized view creation in order to guarantee consistent results\nHomework - The answers in the homework do not match the provided options: You must follow the following steps: 1. clean-cluster 2. docker volume prune and use seed-kafka instead of stream-kafka. Ensure that the number of records is 100K.",
        "question": "How many records should be used when executing the homework tasks?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 429,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "It is best to use the order by and limit clause on the query to the materialized view instead of the materialized view creation in order to guarantee consistent results\nHomework - The answers in the homework do not match the provided options: You must follow the following steps: 1. clean-cluster 2. docker volume prune and use seed-kafka instead of stream-kafka. Ensure that the number of records is 100K.",
        "question": "What should be used instead of stream-kafka according to the homework guidelines?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 429,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For this workshop, and if you are following the view from Noel (2024) this requires you to install postgres to use it on your terminal. Found this steps (commands) to get it done [source]:\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list'\nsudo apt update\napt install postgresql postgresql-contrib\n(comment): now let’s check the service for postgresql\nservice postgresql status\n(comment) If down: use the next command\nservice postgresql start\n(comment) And your are done",
        "question": "What is the first command needed to install Postgres according to Noel (2024)?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 430,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For this workshop, and if you are following the view from Noel (2024) this requires you to install postgres to use it on your terminal. Found this steps (commands) to get it done [source]:\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list'\nsudo apt update\napt install postgresql postgresql-contrib\n(comment): now let’s check the service for postgresql\nservice postgresql status\n(comment) If down: use the next command\nservice postgresql start\n(comment) And your are done",
        "question": "How can you check the status of the PostgreSQL service after installation?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 430,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For this workshop, and if you are following the view from Noel (2024) this requires you to install postgres to use it on your terminal. Found this steps (commands) to get it done [source]:\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list'\nsudo apt update\napt install postgresql postgresql-contrib\n(comment): now let’s check the service for postgresql\nservice postgresql status\n(comment) If down: use the next command\nservice postgresql start\n(comment) And your are done",
        "question": "What should you do if the PostgreSQL service is down?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 430,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For this workshop, and if you are following the view from Noel (2024) this requires you to install postgres to use it on your terminal. Found this steps (commands) to get it done [source]:\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list'\nsudo apt update\napt install postgresql postgresql-contrib\n(comment): now let’s check the service for postgresql\nservice postgresql status\n(comment) If down: use the next command\nservice postgresql start\n(comment) And your are done",
        "question": "Which command is used to update the package list before installing PostgreSQL?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 430,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "For this workshop, and if you are following the view from Noel (2024) this requires you to install postgres to use it on your terminal. Found this steps (commands) to get it done [source]:\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list'\nsudo apt update\napt install postgresql postgresql-contrib\n(comment): now let’s check the service for postgresql\nservice postgresql status\n(comment) If down: use the next command\nservice postgresql start\n(comment) And your are done",
        "question": "What command is used to install PostgreSQL and its additional components?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 430,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Refer to the solution given in the first solution here:\nhttps://stackoverflow.com/questions/24683221/xdg-open-no-method-available-even-after-installing-xdg-utils\nInstead of w3m use any other browser of your choice.\nIt is just trying to open the index.html file. Which you can do from your File Explorer/Finder. If you’re on wsl try using explorer.exe index.html",
        "question": "What is the main issue addressed in the linked Stack Overflow solution?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 431,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Refer to the solution given in the first solution here:\nhttps://stackoverflow.com/questions/24683221/xdg-open-no-method-available-even-after-installing-xdg-utils\nInstead of w3m use any other browser of your choice.\nIt is just trying to open the index.html file. Which you can do from your File Explorer/Finder. If you’re on wsl try using explorer.exe index.html",
        "question": "What alternative to w3m is suggested for opening the index.html file?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 431,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Refer to the solution given in the first solution here:\nhttps://stackoverflow.com/questions/24683221/xdg-open-no-method-available-even-after-installing-xdg-utils\nInstead of w3m use any other browser of your choice.\nIt is just trying to open the index.html file. Which you can do from your File Explorer/Finder. If you’re on wsl try using explorer.exe index.html",
        "question": "How can you open the index.html file using File Explorer or Finder?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 431,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Refer to the solution given in the first solution here:\nhttps://stackoverflow.com/questions/24683221/xdg-open-no-method-available-even-after-installing-xdg-utils\nInstead of w3m use any other browser of your choice.\nIt is just trying to open the index.html file. Which you can do from your File Explorer/Finder. If you’re on wsl try using explorer.exe index.html",
        "question": "What command is recommended for users on WSL to open index.html?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 431,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Refer to the solution given in the first solution here:\nhttps://stackoverflow.com/questions/24683221/xdg-open-no-method-available-even-after-installing-xdg-utils\nInstead of w3m use any other browser of your choice.\nIt is just trying to open the index.html file. Which you can do from your File Explorer/Finder. If you’re on wsl try using explorer.exe index.html",
        "question": "Where can you find the specific solution referenced in the text?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 431,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Example Error:\nWhen attempting to execute a Python script named seed-kafka.py or server.py with the following shebang line specifying Python 3 as the interpreter:\nUsers may encounter the following error in a Unix-like environment:\nThis error indicates that there is a problem with the Python interpreter path specified in the shebang line. The presence of the \\r character suggests that the script was edited or created in a Windows environment, causing the interpreter path to be incorrect when executed in Unix-like environments.\n2 Solutions:\nEither one or the other\nUpdate Shebang Line:\nVerify Python Interpreter Path: Use the which python3 command to determine the path to the Python 3 interpreter available in the current environment.\nUpdate Shebang Line: Open the script file in a text editor. Modify the shebang line to point to the correct Python interpreter path found in the previous step. Ensure that the shebang line is consistent with the Python interpreter path in the execution environment.\nExample Shebang Line:\nReplace /usr/bin/env python3 with the correct Python interpreter path found using which python3.\nConvert Line Endings:\nUse the dos2unix command-line tool to convert the line endings of the script from Windows-style to Unix-style.\nThis removes the extraneous carriage return characters (\\r), resolving issues related to unexpected tokens and ensuring compatibility with Unix-like environments.\nExample Command:",
        "question": "What error may occur when executing a Python script with a specific shebang line in a Unix-like environment?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 432,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Example Error:\nWhen attempting to execute a Python script named seed-kafka.py or server.py with the following shebang line specifying Python 3 as the interpreter:\nUsers may encounter the following error in a Unix-like environment:\nThis error indicates that there is a problem with the Python interpreter path specified in the shebang line. The presence of the \\r character suggests that the script was edited or created in a Windows environment, causing the interpreter path to be incorrect when executed in Unix-like environments.\n2 Solutions:\nEither one or the other\nUpdate Shebang Line:\nVerify Python Interpreter Path: Use the which python3 command to determine the path to the Python 3 interpreter available in the current environment.\nUpdate Shebang Line: Open the script file in a text editor. Modify the shebang line to point to the correct Python interpreter path found in the previous step. Ensure that the shebang line is consistent with the Python interpreter path in the execution environment.\nExample Shebang Line:\nReplace /usr/bin/env python3 with the correct Python interpreter path found using which python3.\nConvert Line Endings:\nUse the dos2unix command-line tool to convert the line endings of the script from Windows-style to Unix-style.\nThis removes the extraneous carriage return characters (\\r), resolving issues related to unexpected tokens and ensuring compatibility with Unix-like environments.\nExample Command:",
        "question": "What does the presence of the \r character in a script indicate?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 432,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Example Error:\nWhen attempting to execute a Python script named seed-kafka.py or server.py with the following shebang line specifying Python 3 as the interpreter:\nUsers may encounter the following error in a Unix-like environment:\nThis error indicates that there is a problem with the Python interpreter path specified in the shebang line. The presence of the \\r character suggests that the script was edited or created in a Windows environment, causing the interpreter path to be incorrect when executed in Unix-like environments.\n2 Solutions:\nEither one or the other\nUpdate Shebang Line:\nVerify Python Interpreter Path: Use the which python3 command to determine the path to the Python 3 interpreter available in the current environment.\nUpdate Shebang Line: Open the script file in a text editor. Modify the shebang line to point to the correct Python interpreter path found in the previous step. Ensure that the shebang line is consistent with the Python interpreter path in the execution environment.\nExample Shebang Line:\nReplace /usr/bin/env python3 with the correct Python interpreter path found using which python3.\nConvert Line Endings:\nUse the dos2unix command-line tool to convert the line endings of the script from Windows-style to Unix-style.\nThis removes the extraneous carriage return characters (\\r), resolving issues related to unexpected tokens and ensuring compatibility with Unix-like environments.\nExample Command:",
        "question": "Which command can be used to verify the Python interpreter path in the current environment?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 432,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Example Error:\nWhen attempting to execute a Python script named seed-kafka.py or server.py with the following shebang line specifying Python 3 as the interpreter:\nUsers may encounter the following error in a Unix-like environment:\nThis error indicates that there is a problem with the Python interpreter path specified in the shebang line. The presence of the \\r character suggests that the script was edited or created in a Windows environment, causing the interpreter path to be incorrect when executed in Unix-like environments.\n2 Solutions:\nEither one or the other\nUpdate Shebang Line:\nVerify Python Interpreter Path: Use the which python3 command to determine the path to the Python 3 interpreter available in the current environment.\nUpdate Shebang Line: Open the script file in a text editor. Modify the shebang line to point to the correct Python interpreter path found in the previous step. Ensure that the shebang line is consistent with the Python interpreter path in the execution environment.\nExample Shebang Line:\nReplace /usr/bin/env python3 with the correct Python interpreter path found using which python3.\nConvert Line Endings:\nUse the dos2unix command-line tool to convert the line endings of the script from Windows-style to Unix-style.\nThis removes the extraneous carriage return characters (\\r), resolving issues related to unexpected tokens and ensuring compatibility with Unix-like environments.\nExample Command:",
        "question": "How can you update the shebang line in a Python script?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 432,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Example Error:\nWhen attempting to execute a Python script named seed-kafka.py or server.py with the following shebang line specifying Python 3 as the interpreter:\nUsers may encounter the following error in a Unix-like environment:\nThis error indicates that there is a problem with the Python interpreter path specified in the shebang line. The presence of the \\r character suggests that the script was edited or created in a Windows environment, causing the interpreter path to be incorrect when executed in Unix-like environments.\n2 Solutions:\nEither one or the other\nUpdate Shebang Line:\nVerify Python Interpreter Path: Use the which python3 command to determine the path to the Python 3 interpreter available in the current environment.\nUpdate Shebang Line: Open the script file in a text editor. Modify the shebang line to point to the correct Python interpreter path found in the previous step. Ensure that the shebang line is consistent with the Python interpreter path in the execution environment.\nExample Shebang Line:\nReplace /usr/bin/env python3 with the correct Python interpreter path found using which python3.\nConvert Line Endings:\nUse the dos2unix command-line tool to convert the line endings of the script from Windows-style to Unix-style.\nThis removes the extraneous carriage return characters (\\r), resolving issues related to unexpected tokens and ensuring compatibility with Unix-like environments.\nExample Command:",
        "question": "What command-line tool can be used to convert Windows-style line endings to Unix-style?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 432,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans : Windowing in streaming SQL involves defining a time-based or row-based boundary for data processing. It allows you to analyze and aggregate data over specific time intervals or based on the number of events received, providing a way to manage and organize streaming data for analysis.",
        "question": "What is windowing in streaming SQL?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 433,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans : Windowing in streaming SQL involves defining a time-based or row-based boundary for data processing. It allows you to analyze and aggregate data over specific time intervals or based on the number of events received, providing a way to manage and organize streaming data for analysis.",
        "question": "What types of boundaries can be defined in windowing?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 433,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans : Windowing in streaming SQL involves defining a time-based or row-based boundary for data processing. It allows you to analyze and aggregate data over specific time intervals or based on the number of events received, providing a way to manage and organize streaming data for analysis.",
        "question": "How does windowing help in analyzing streaming data?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 433,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans : Windowing in streaming SQL involves defining a time-based or row-based boundary for data processing. It allows you to analyze and aggregate data over specific time intervals or based on the number of events received, providing a way to manage and organize streaming data for analysis.",
        "question": "What are the two main criteria for windowing mentioned in the text?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 433,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Ans : Windowing in streaming SQL involves defining a time-based or row-based boundary for data processing. It allows you to analyze and aggregate data over specific time intervals or based on the number of events received, providing a way to manage and organize streaming data for analysis.",
        "question": "Why is windowing important for managing streaming data?",
        "section": "Workshop 2 - RisingWave",
        "document_id": 433,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Docker Commands\n# Create a Docker Image from a base image\nDocker run -it ubuntu bash\n#List docker images\nDocker images list\n#List  Running containers\nDocker ps -a\n#List with full container ids\nDocker ps -a --no-trunc\n#Add onto existing image to create new image\nDocker commit -a <User_Name> -m \"Message\" container_id New_Image_Name\n# Create a Docker Image with an entrypoint from a base image\nDocker run -it --entry_point=bash python:3.11\n#Attach to a stopped container\nDocker start -ai <Container_Name>\n#Attach to a running container\ndocker exec -it <Container_ID> bash\n#copying from host to container\nDocker cp <SRC_PATH/file> <containerid>:<dest_path>\n#copying from container to host\nDocker cp <containerid>:<Srct_path> <Dest Path on host/file>\n#Create an image from a docker file\nDocker build -t <Image_Name> <Location of Dockerfile>\n#DockerFile Options and best practices\nhttps://devopscube.com/build-docker-image/\n#Docker delete all images forcefully\ndocker rmi -f $(docker images -aq)\n#Docker delete all containers forcefully\ndocker rm -f $(docker ps -qa)\n#docker compose creation\nhttps://www.composerize.com/\nGCP Commands\n1.     Create SSH Keys\n2.     Added to the Settings of Compute Engine VM Instance\n3.     SSH-ed into the VM Instance with a config similar to following\nHost my-website.com\nHostName my-website.com\nUser my-user\nIdentityFile ~/.ssh/id_rsa\n4.     Installed Anaconda by installing the sh file through bash <Anaconda.sh>\n5.     Install Docker after\na.     Sudo apt-get update\nb.     Sudo apt-get docker\n6.     To run Docker without SUDO permissions\na.     https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md\n7.     Google cloud remote copy\na.     gcloud compute scp LOCAL_FILE_PATHVM_NAME:REMOTE_DIR\nInstall GCP Cloud SDK on Docker Machine\nhttps://stackoverflow.com/questions/23247943/trouble-installing-google-cloud-sdk-in-ubuntu\nsudo apt-get install apt-transport-https ca-certificates gnupg && echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\"| sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list&& curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && sudo apt-get update && sudo apt-get install google-cloud-sdk && sudo apt-get install google-cloud-sdk-app-engine-java && sudo apt-get install google-cloud-sdk-app-engine-python && gcloud init\nAnaconda Commands\n#Activate environment\nConda Activate <environment_name>\n#DeActivate environment\nConda DeActivate <environment_name>\n#Start iterm without conda environment\nconda config --set auto_activate_base false\n# Using Conda forge as default (Community driven packaging recipes and solutions)\nhttps://conda-forge.org/docs/user/introduction.html\nconda --version\nconda update conda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n#Using Libmamba as Solver\nconda install pgcli  --solver=libmamba\nLinux/MAC Commands\nStarting and Stopping Services on Linux\n●  \tsudo systemctl start postgresql\n●  \tsudo systemctl stop postgresql\nStarting and Stopping Services on MAC\n●      launchctl start postgresql\n●      launchctl stop postgresql\nIdentifying processes listening to a Port across MAC/Linux\nsudo lsof -i -P -n | grep LISTEN\n$ sudo netstat -tulpn | grep LISTEN\n$ sudo ss -tulpn | grep LISTEN\n$ sudo lsof -i:22 ## see a specific port such as 22 ##\n$ sudo nmap -sTU -O IP-address-Here\nInstalling a package on Debian\nsudo apt install <packagename>\nListing all package on Debian\nDpkg -l | grep <packagename>\nUnInstalling a package on Debian\nSudo apt remove <packagename>\nSudo apt autoclean  && sudo apt autoremove\nList all Processes on Debian/Ubuntu\nPs -aux\napt-get update && apt-get install procps\napt-get install iproute2 for ss -tulpn\n#Postgres Install\nsudo sh -c 'echo \"deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list'\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo apt-get update\nsudo apt-get -y install postgresql\n#Changing Postgresql port to 5432\n- sudo service postgresql stop - sed -e 's/^port.*/port = 5432/' /etc/postgresql/10/main/postgresql.conf > postgresql.conf\n- sudo chown postgres postgresql.conf\n- sudo mv postgresql.conf /etc/postgresql/10/main\n- sudo systemctl restart postgresql",
        "question": "What command is used to create a Docker image from a base image?",
        "section": "Triggers in Mage via CLI",
        "document_id": 435,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Docker Commands\n# Create a Docker Image from a base image\nDocker run -it ubuntu bash\n#List docker images\nDocker images list\n#List  Running containers\nDocker ps -a\n#List with full container ids\nDocker ps -a --no-trunc\n#Add onto existing image to create new image\nDocker commit -a <User_Name> -m \"Message\" container_id New_Image_Name\n# Create a Docker Image with an entrypoint from a base image\nDocker run -it --entry_point=bash python:3.11\n#Attach to a stopped container\nDocker start -ai <Container_Name>\n#Attach to a running container\ndocker exec -it <Container_ID> bash\n#copying from host to container\nDocker cp <SRC_PATH/file> <containerid>:<dest_path>\n#copying from container to host\nDocker cp <containerid>:<Srct_path> <Dest Path on host/file>\n#Create an image from a docker file\nDocker build -t <Image_Name> <Location of Dockerfile>\n#DockerFile Options and best practices\nhttps://devopscube.com/build-docker-image/\n#Docker delete all images forcefully\ndocker rmi -f $(docker images -aq)\n#Docker delete all containers forcefully\ndocker rm -f $(docker ps -qa)\n#docker compose creation\nhttps://www.composerize.com/\nGCP Commands\n1.     Create SSH Keys\n2.     Added to the Settings of Compute Engine VM Instance\n3.     SSH-ed into the VM Instance with a config similar to following\nHost my-website.com\nHostName my-website.com\nUser my-user\nIdentityFile ~/.ssh/id_rsa\n4.     Installed Anaconda by installing the sh file through bash <Anaconda.sh>\n5.     Install Docker after\na.     Sudo apt-get update\nb.     Sudo apt-get docker\n6.     To run Docker without SUDO permissions\na.     https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md\n7.     Google cloud remote copy\na.     gcloud compute scp LOCAL_FILE_PATHVM_NAME:REMOTE_DIR\nInstall GCP Cloud SDK on Docker Machine\nhttps://stackoverflow.com/questions/23247943/trouble-installing-google-cloud-sdk-in-ubuntu\nsudo apt-get install apt-transport-https ca-certificates gnupg && echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\"| sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list&& curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && sudo apt-get update && sudo apt-get install google-cloud-sdk && sudo apt-get install google-cloud-sdk-app-engine-java && sudo apt-get install google-cloud-sdk-app-engine-python && gcloud init\nAnaconda Commands\n#Activate environment\nConda Activate <environment_name>\n#DeActivate environment\nConda DeActivate <environment_name>\n#Start iterm without conda environment\nconda config --set auto_activate_base false\n# Using Conda forge as default (Community driven packaging recipes and solutions)\nhttps://conda-forge.org/docs/user/introduction.html\nconda --version\nconda update conda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n#Using Libmamba as Solver\nconda install pgcli  --solver=libmamba\nLinux/MAC Commands\nStarting and Stopping Services on Linux\n●  \tsudo systemctl start postgresql\n●  \tsudo systemctl stop postgresql\nStarting and Stopping Services on MAC\n●      launchctl start postgresql\n●      launchctl stop postgresql\nIdentifying processes listening to a Port across MAC/Linux\nsudo lsof -i -P -n | grep LISTEN\n$ sudo netstat -tulpn | grep LISTEN\n$ sudo ss -tulpn | grep LISTEN\n$ sudo lsof -i:22 ## see a specific port such as 22 ##\n$ sudo nmap -sTU -O IP-address-Here\nInstalling a package on Debian\nsudo apt install <packagename>\nListing all package on Debian\nDpkg -l | grep <packagename>\nUnInstalling a package on Debian\nSudo apt remove <packagename>\nSudo apt autoclean  && sudo apt autoremove\nList all Processes on Debian/Ubuntu\nPs -aux\napt-get update && apt-get install procps\napt-get install iproute2 for ss -tulpn\n#Postgres Install\nsudo sh -c 'echo \"deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list'\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo apt-get update\nsudo apt-get -y install postgresql\n#Changing Postgresql port to 5432\n- sudo service postgresql stop - sed -e 's/^port.*/port = 5432/' /etc/postgresql/10/main/postgresql.conf > postgresql.conf\n- sudo chown postgres postgresql.conf\n- sudo mv postgresql.conf /etc/postgresql/10/main\n- sudo systemctl restart postgresql",
        "question": "How can you list all running and stopped Docker containers?",
        "section": "Triggers in Mage via CLI",
        "document_id": 435,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Docker Commands\n# Create a Docker Image from a base image\nDocker run -it ubuntu bash\n#List docker images\nDocker images list\n#List  Running containers\nDocker ps -a\n#List with full container ids\nDocker ps -a --no-trunc\n#Add onto existing image to create new image\nDocker commit -a <User_Name> -m \"Message\" container_id New_Image_Name\n# Create a Docker Image with an entrypoint from a base image\nDocker run -it --entry_point=bash python:3.11\n#Attach to a stopped container\nDocker start -ai <Container_Name>\n#Attach to a running container\ndocker exec -it <Container_ID> bash\n#copying from host to container\nDocker cp <SRC_PATH/file> <containerid>:<dest_path>\n#copying from container to host\nDocker cp <containerid>:<Srct_path> <Dest Path on host/file>\n#Create an image from a docker file\nDocker build -t <Image_Name> <Location of Dockerfile>\n#DockerFile Options and best practices\nhttps://devopscube.com/build-docker-image/\n#Docker delete all images forcefully\ndocker rmi -f $(docker images -aq)\n#Docker delete all containers forcefully\ndocker rm -f $(docker ps -qa)\n#docker compose creation\nhttps://www.composerize.com/\nGCP Commands\n1.     Create SSH Keys\n2.     Added to the Settings of Compute Engine VM Instance\n3.     SSH-ed into the VM Instance with a config similar to following\nHost my-website.com\nHostName my-website.com\nUser my-user\nIdentityFile ~/.ssh/id_rsa\n4.     Installed Anaconda by installing the sh file through bash <Anaconda.sh>\n5.     Install Docker after\na.     Sudo apt-get update\nb.     Sudo apt-get docker\n6.     To run Docker without SUDO permissions\na.     https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md\n7.     Google cloud remote copy\na.     gcloud compute scp LOCAL_FILE_PATHVM_NAME:REMOTE_DIR\nInstall GCP Cloud SDK on Docker Machine\nhttps://stackoverflow.com/questions/23247943/trouble-installing-google-cloud-sdk-in-ubuntu\nsudo apt-get install apt-transport-https ca-certificates gnupg && echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\"| sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list&& curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && sudo apt-get update && sudo apt-get install google-cloud-sdk && sudo apt-get install google-cloud-sdk-app-engine-java && sudo apt-get install google-cloud-sdk-app-engine-python && gcloud init\nAnaconda Commands\n#Activate environment\nConda Activate <environment_name>\n#DeActivate environment\nConda DeActivate <environment_name>\n#Start iterm without conda environment\nconda config --set auto_activate_base false\n# Using Conda forge as default (Community driven packaging recipes and solutions)\nhttps://conda-forge.org/docs/user/introduction.html\nconda --version\nconda update conda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n#Using Libmamba as Solver\nconda install pgcli  --solver=libmamba\nLinux/MAC Commands\nStarting and Stopping Services on Linux\n●  \tsudo systemctl start postgresql\n●  \tsudo systemctl stop postgresql\nStarting and Stopping Services on MAC\n●      launchctl start postgresql\n●      launchctl stop postgresql\nIdentifying processes listening to a Port across MAC/Linux\nsudo lsof -i -P -n | grep LISTEN\n$ sudo netstat -tulpn | grep LISTEN\n$ sudo ss -tulpn | grep LISTEN\n$ sudo lsof -i:22 ## see a specific port such as 22 ##\n$ sudo nmap -sTU -O IP-address-Here\nInstalling a package on Debian\nsudo apt install <packagename>\nListing all package on Debian\nDpkg -l | grep <packagename>\nUnInstalling a package on Debian\nSudo apt remove <packagename>\nSudo apt autoclean  && sudo apt autoremove\nList all Processes on Debian/Ubuntu\nPs -aux\napt-get update && apt-get install procps\napt-get install iproute2 for ss -tulpn\n#Postgres Install\nsudo sh -c 'echo \"deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list'\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo apt-get update\nsudo apt-get -y install postgresql\n#Changing Postgresql port to 5432\n- sudo service postgresql stop - sed -e 's/^port.*/port = 5432/' /etc/postgresql/10/main/postgresql.conf > postgresql.conf\n- sudo chown postgres postgresql.conf\n- sudo mv postgresql.conf /etc/postgresql/10/main\n- sudo systemctl restart postgresql",
        "question": "What is the purpose of the Docker commit command?",
        "section": "Triggers in Mage via CLI",
        "document_id": 435,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Docker Commands\n# Create a Docker Image from a base image\nDocker run -it ubuntu bash\n#List docker images\nDocker images list\n#List  Running containers\nDocker ps -a\n#List with full container ids\nDocker ps -a --no-trunc\n#Add onto existing image to create new image\nDocker commit -a <User_Name> -m \"Message\" container_id New_Image_Name\n# Create a Docker Image with an entrypoint from a base image\nDocker run -it --entry_point=bash python:3.11\n#Attach to a stopped container\nDocker start -ai <Container_Name>\n#Attach to a running container\ndocker exec -it <Container_ID> bash\n#copying from host to container\nDocker cp <SRC_PATH/file> <containerid>:<dest_path>\n#copying from container to host\nDocker cp <containerid>:<Srct_path> <Dest Path on host/file>\n#Create an image from a docker file\nDocker build -t <Image_Name> <Location of Dockerfile>\n#DockerFile Options and best practices\nhttps://devopscube.com/build-docker-image/\n#Docker delete all images forcefully\ndocker rmi -f $(docker images -aq)\n#Docker delete all containers forcefully\ndocker rm -f $(docker ps -qa)\n#docker compose creation\nhttps://www.composerize.com/\nGCP Commands\n1.     Create SSH Keys\n2.     Added to the Settings of Compute Engine VM Instance\n3.     SSH-ed into the VM Instance with a config similar to following\nHost my-website.com\nHostName my-website.com\nUser my-user\nIdentityFile ~/.ssh/id_rsa\n4.     Installed Anaconda by installing the sh file through bash <Anaconda.sh>\n5.     Install Docker after\na.     Sudo apt-get update\nb.     Sudo apt-get docker\n6.     To run Docker without SUDO permissions\na.     https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md\n7.     Google cloud remote copy\na.     gcloud compute scp LOCAL_FILE_PATHVM_NAME:REMOTE_DIR\nInstall GCP Cloud SDK on Docker Machine\nhttps://stackoverflow.com/questions/23247943/trouble-installing-google-cloud-sdk-in-ubuntu\nsudo apt-get install apt-transport-https ca-certificates gnupg && echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\"| sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list&& curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && sudo apt-get update && sudo apt-get install google-cloud-sdk && sudo apt-get install google-cloud-sdk-app-engine-java && sudo apt-get install google-cloud-sdk-app-engine-python && gcloud init\nAnaconda Commands\n#Activate environment\nConda Activate <environment_name>\n#DeActivate environment\nConda DeActivate <environment_name>\n#Start iterm without conda environment\nconda config --set auto_activate_base false\n# Using Conda forge as default (Community driven packaging recipes and solutions)\nhttps://conda-forge.org/docs/user/introduction.html\nconda --version\nconda update conda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n#Using Libmamba as Solver\nconda install pgcli  --solver=libmamba\nLinux/MAC Commands\nStarting and Stopping Services on Linux\n●  \tsudo systemctl start postgresql\n●  \tsudo systemctl stop postgresql\nStarting and Stopping Services on MAC\n●      launchctl start postgresql\n●      launchctl stop postgresql\nIdentifying processes listening to a Port across MAC/Linux\nsudo lsof -i -P -n | grep LISTEN\n$ sudo netstat -tulpn | grep LISTEN\n$ sudo ss -tulpn | grep LISTEN\n$ sudo lsof -i:22 ## see a specific port such as 22 ##\n$ sudo nmap -sTU -O IP-address-Here\nInstalling a package on Debian\nsudo apt install <packagename>\nListing all package on Debian\nDpkg -l | grep <packagename>\nUnInstalling a package on Debian\nSudo apt remove <packagename>\nSudo apt autoclean  && sudo apt autoremove\nList all Processes on Debian/Ubuntu\nPs -aux\napt-get update && apt-get install procps\napt-get install iproute2 for ss -tulpn\n#Postgres Install\nsudo sh -c 'echo \"deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list'\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo apt-get update\nsudo apt-get -y install postgresql\n#Changing Postgresql port to 5432\n- sudo service postgresql stop - sed -e 's/^port.*/port = 5432/' /etc/postgresql/10/main/postgresql.conf > postgresql.conf\n- sudo chown postgres postgresql.conf\n- sudo mv postgresql.conf /etc/postgresql/10/main\n- sudo systemctl restart postgresql",
        "question": "What command installs the Google Cloud SDK on a Docker machine?",
        "section": "Triggers in Mage via CLI",
        "document_id": 435,
        "course": "data-engineering-zoomcamp"
    },
    {
        "text": "Docker Commands\n# Create a Docker Image from a base image\nDocker run -it ubuntu bash\n#List docker images\nDocker images list\n#List  Running containers\nDocker ps -a\n#List with full container ids\nDocker ps -a --no-trunc\n#Add onto existing image to create new image\nDocker commit -a <User_Name> -m \"Message\" container_id New_Image_Name\n# Create a Docker Image with an entrypoint from a base image\nDocker run -it --entry_point=bash python:3.11\n#Attach to a stopped container\nDocker start -ai <Container_Name>\n#Attach to a running container\ndocker exec -it <Container_ID> bash\n#copying from host to container\nDocker cp <SRC_PATH/file> <containerid>:<dest_path>\n#copying from container to host\nDocker cp <containerid>:<Srct_path> <Dest Path on host/file>\n#Create an image from a docker file\nDocker build -t <Image_Name> <Location of Dockerfile>\n#DockerFile Options and best practices\nhttps://devopscube.com/build-docker-image/\n#Docker delete all images forcefully\ndocker rmi -f $(docker images -aq)\n#Docker delete all containers forcefully\ndocker rm -f $(docker ps -qa)\n#docker compose creation\nhttps://www.composerize.com/\nGCP Commands\n1.     Create SSH Keys\n2.     Added to the Settings of Compute Engine VM Instance\n3.     SSH-ed into the VM Instance with a config similar to following\nHost my-website.com\nHostName my-website.com\nUser my-user\nIdentityFile ~/.ssh/id_rsa\n4.     Installed Anaconda by installing the sh file through bash <Anaconda.sh>\n5.     Install Docker after\na.     Sudo apt-get update\nb.     Sudo apt-get docker\n6.     To run Docker without SUDO permissions\na.     https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md\n7.     Google cloud remote copy\na.     gcloud compute scp LOCAL_FILE_PATHVM_NAME:REMOTE_DIR\nInstall GCP Cloud SDK on Docker Machine\nhttps://stackoverflow.com/questions/23247943/trouble-installing-google-cloud-sdk-in-ubuntu\nsudo apt-get install apt-transport-https ca-certificates gnupg && echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\"| sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list&& curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && sudo apt-get update && sudo apt-get install google-cloud-sdk && sudo apt-get install google-cloud-sdk-app-engine-java && sudo apt-get install google-cloud-sdk-app-engine-python && gcloud init\nAnaconda Commands\n#Activate environment\nConda Activate <environment_name>\n#DeActivate environment\nConda DeActivate <environment_name>\n#Start iterm without conda environment\nconda config --set auto_activate_base false\n# Using Conda forge as default (Community driven packaging recipes and solutions)\nhttps://conda-forge.org/docs/user/introduction.html\nconda --version\nconda update conda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n#Using Libmamba as Solver\nconda install pgcli  --solver=libmamba\nLinux/MAC Commands\nStarting and Stopping Services on Linux\n●  \tsudo systemctl start postgresql\n●  \tsudo systemctl stop postgresql\nStarting and Stopping Services on MAC\n●      launchctl start postgresql\n●      launchctl stop postgresql\nIdentifying processes listening to a Port across MAC/Linux\nsudo lsof -i -P -n | grep LISTEN\n$ sudo netstat -tulpn | grep LISTEN\n$ sudo ss -tulpn | grep LISTEN\n$ sudo lsof -i:22 ## see a specific port such as 22 ##\n$ sudo nmap -sTU -O IP-address-Here\nInstalling a package on Debian\nsudo apt install <packagename>\nListing all package on Debian\nDpkg -l | grep <packagename>\nUnInstalling a package on Debian\nSudo apt remove <packagename>\nSudo apt autoclean  && sudo apt autoremove\nList all Processes on Debian/Ubuntu\nPs -aux\napt-get update && apt-get install procps\napt-get install iproute2 for ss -tulpn\n#Postgres Install\nsudo sh -c 'echo \"deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list'\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo apt-get update\nsudo apt-get -y install postgresql\n#Changing Postgresql port to 5432\n- sudo service postgresql stop - sed -e 's/^port.*/port = 5432/' /etc/postgresql/10/main/postgresql.conf > postgresql.conf\n- sudo chown postgres postgresql.conf\n- sudo mv postgresql.conf /etc/postgresql/10/main\n- sudo systemctl restart postgresql",
        "question": "How do you activate and deactivate a Conda environment?",
        "section": "Triggers in Mage via CLI",
        "document_id": 435,
        "course": "data-engineering-zoomcamp"
    }
]